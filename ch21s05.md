<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Stratégiakeresés"><div class="titlepage"><div><div><h1 class="title"><a id="id754893"/>Stratégiakeresés</h1></div></div></div><p>A megerősítéses tanulással kapcsolatos, utolsó általunk tárgyalt megközelítés <span class="strong"><strong>stratégiakeresés</strong></span> (<span class="strong"><strong>policy search</strong></span>) néven ismert. Bizonyos értelemben a stratégiakeresés a legegyszerűbb a fejezet összes módszere közt. Az alapötlet annyi, hogy engedjük változni a stratégiát, amíg javul, és utána állítsuk le a változást.</p><p>A vizsgálatot kezdjük magával a stratégiával. Emlékezzünk vissza, hogy a <span xml:lang="hu" class="emphasis"><em>π</em></span> stratégia egy olyan függvény, amely állapotokat képez le cselekvésekre. Mi elsősorban olyan <span class="emphasis"><em>paraméterezet π</em></span> stratégiák vizsgálatában vagyunk érdekeltek, amelyeknek sokkal kevesebb paramétere van, mint ahány állapot található az állapottérben (éppúgy, mint az előző alfejezetben). Például paraméterezett <span class="emphasis"><em>Q</em></span>-függvényekkel reprezentálhatjuk a <span xml:lang="hu" class="emphasis"><em>π</em></span> stratégiát, egy-egy <span class="emphasis"><em>Q</em></span>-függvényt rendelve minden cselekvéshez, és azt a cselekvést választjuk, amely a legnagyobb jósolt értéket adja:</p><p><span class="inlinemediaobject"><img src="math/mi-21-0021.gif" alt="Stratégiakeresés"/></span></p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Minden egyes <span class="emphasis"><em>Q</em></span>-függvény lehet a <span xml:lang="hu" class="emphasis"><em>θ</em></span> paraméterek lineáris függvénye, mint a (21.9) egyenletben, vagy lehet nemlineáris függvény, mint például egy neurális háló. A stratégiakeresés ezek után a <span xml:lang="hu" class="emphasis"><em>θ</em></span> paramétereket változtatja oly módon, hogy javítsa a stratégiát. Vegyük észre, hogy ha a stratégiát <span class="emphasis"><em>Q</em></span>-függvényekkel reprezentáljuk, akkor a stratégiakeresés egy <span class="emphasis"><em>Q</em></span>-függvény tanulási eljárást eredményez. <span class="emphasis"><em>Ez a folyamat azonban nem azonos a Q-tanulással!</em></span> A függvényapproximációt használó <span class="emphasis"><em>Q</em></span>-tanulásban az algoritmus egy olyan <span xml:lang="hu" class="emphasis"><em>θ</em></span>-t keres, amelyre a <span class="inlinemediaobject"><img src="math/mi-21-0022.gif" alt="Stratégiakeresés"/></span> „közel” van <span class="emphasis"><em>Q*</em></span>-hoz, az optimális <span class="emphasis"><em>Q</em></span>-függvényhez. A stratégiakeresés ezzel szemben olyan <span xml:lang="hu" class="emphasis"><em>θ</em></span>-t keres, amely jó működést eredményez, az eredményül kapott értékek alapvetően eltérhetnek.<sup>[<a id="id754997" href="#ftn.id754997" class="footnote">217</a>]</sup> Egy másik nyilvánvaló példa a két dolog különbségére az az eset, amikor a <span xml:lang="hu" class="emphasis"><em>π</em></span>(<span class="emphasis"><em>s</em></span>) stratégiát úgy számítjuk, hogy 10 lépés mélységben előretekintő keresést végzünk egy közelítő <span class="inlinemediaobject"><img src="math/mi-21-0023.gif" alt="Stratégiakeresés"/></span> hasznosságfüggvény alapján. A jó eredményt adó <span xml:lang="hu" class="emphasis"><em>θ</em></span> messze lehet attól, amely az <span class="inlinemediaobject"><img src="math/mi-21-0024.gif" alt="Stratégiakeresés"/></span>-t az igazi hasznosságfüggvényhez hasonlóvá teszi.</p></div><p>A (21.13) egyenletben bemutatott stratégiareprezentáció egyik problémája, hogy diszkrét cselekvések esetén a stratégia a paraméterek <span class="emphasis"><em>nemfolytonos</em></span> függvénye.<sup>[<a id="id755058" href="#ftn.id755058" class="footnote">218</a>]</sup> Azaz lesznek olyan <span xml:lang="hu" class="emphasis"><em>θ</em></span> értékek, amelyeknél végtelen kis változás a <span xml:lang="hu" class="emphasis"><em>θ</em></span>-ban azt eredményezi, hogy a stratégia egyik cselekvésről a másikra vált. Ez azt is jelenti, hogy a stratégia értéke is változhat nemfolytonos módon, ami nehézzé teszi a gradiensalapú eljárások alkalmazását. Emiatt a stratégiakeresési eljárások gyakran használják a <span xml:lang="hu" class="emphasis"><em>π<sub>θ</sub></em></span>(<span class="emphasis"><em>s, a</em></span>) <span class="strong"><strong>sztochasz-tikus stratégia</strong></span> (<span class="strong"><strong>stochastic policy</strong></span>) reprezentációt, amely az <span class="emphasis"><em>s</em></span> állapotban az <span class="emphasis"><em>a</em></span> cselekvés választásának valószínűségét specifikálja. Egy népszerű reprezentáció a <span class="strong"><strong>szoftmax függvény</strong></span> (<span class="strong"><strong>softmax function</strong></span>):</p><p><span class="inlinemediaobject"><img src="math/mi-21-0025.gif" alt="Stratégiakeresés"/></span></p><p>A szoftmax függvény közel determinisztikussá válik, ha az egyik cselekvés sokkal jobb, mint a többi, de mindig differenciálható <span xml:lang="hu" class="emphasis"><em>θ</em></span> szerint; így a stratégia értéke (amely folytonos módon függ a cselekvésválasztási valószínűségtől) differenciálható függvénye lesz <span xml:lang="hu" class="emphasis"><em>θ</em></span>-nak.</p><p>Nézzünk most módszereket a stratégia javítására. Kezdjük a legegyszerűbb esettel: determinisztikus stratégia, determinisztikus környezetben. Ebben az esetben a stratégia értékelése triviális: egyszerűen végrehajtjuk a stratégiát, és megfigyeljük az összegyűjtött jutalmat, ez adja számunkra a <span class="strong"><strong>stratégia értéké</strong></span>t (<span class="strong"><strong>policy value</strong></span>), <span xml:lang="hu" class="emphasis"><em>ρ</em></span>(<span xml:lang="hu" class="emphasis"><em>θ</em></span>)-t. A stratégia javítása ezek után egy standard optimalizációs probléma, ilyeneket a 4. fejezetben tárgyaltunk. Követhetjük a <span class="strong"><strong>stratégiagradiens</strong></span> (<span class="strong"><strong>policy gradient</strong></span>) vektort, ∇<span class="emphasis"><em><sub>θ</sub> ρ</em></span>(<span xml:lang="hu" class="emphasis"><em>θ</em></span>)-t, feltéve, hogy <span xml:lang="hu" class="emphasis"><em>ρ</em></span>(<span xml:lang="hu" class="emphasis"><em>θ</em></span>) differenciálható. Másik lehetőség, ha az <span class="strong"><strong>empirikus gradiens</strong></span>t (<span class="strong"><strong>empirical gradient</strong></span>) követjük hegymászó módszerrel – azaz kiértékeljük a stratégiát minden egyes paraméter kis megváltozása esetére. A szokásos feltételek esetén ez a stratégiatér egy lokális optimumához fog konvergálni.</p><p>Ha a környezet (vagy a stratégia) sztochasztikus, akkor a helyzet nehezebbé válik. Tegyük fel, hogy hegymászó módszert próbálunk alkalmazni, ami azt kívánja, hogy összehasonlítsuk <span xml:lang="hu" class="emphasis"><em>ρ</em></span>(<span xml:lang="hu" class="emphasis"><em>θ</em></span>)-t <span xml:lang="hu" class="emphasis"><em>ρ</em></span>(<span class="emphasis"><em>θ </em></span>+<span class="emphasis"><em> </em></span>Δ<span class="emphasis"><em>θ</em></span>)-val valamilyen kis Δ<span class="emphasis"><em>θ</em></span> esetén. Az a probléma, hogy a teljes jutalom nagyon nagyokat változhat kísérletről kísérletre, így a kisszámú kísérletből számított stratégiaérték nagyon megbízhatatlan lesz – két ilyen becslés összehasonlítása pedig még megbízhatatlanabb. Egy lehetséges megoldás, ha egyszerűen sok kísérletet futtatunk le, és a minta variancájával mérjük, hogy elegendő kísérletet futtattunk-e már ahhoz, hogy megbízhatóan jelezni tudjuk a <span xml:lang="hu" class="emphasis"><em>ρ</em></span>(<span xml:lang="hu" class="emphasis"><em>θ</em></span>) javításának irányát. Sajnálatos módon ez sok valós probléma esetén nem praktikus, mert az egyes kísérletek drágák, időigényesek és esetleg veszélyesek lehetnek.</p><p>A <span xml:lang="hu" class="emphasis"><em>π<sub>θ</sub></em></span>(<span class="emphasis"><em>s, a</em></span>) sztochasztikus stratégia esetén lehetőségünk van arra, hogy a <span xml:lang="hu" class="emphasis"><em>θ</em></span>-ban végrehajtott kísérletek eredményei alapján a ∇<span class="emphasis"><em><sub>θ</sub>  ρ</em></span>(<span xml:lang="hu" class="emphasis"><em>θ</em></span>) gradiens torzítatlan becslését állítsuk elő <span xml:lang="hu" class="emphasis"><em>θ</em></span>-ban. Az egyszerűség kedvéért a becslést arra az egyszerű esetre vezetjük le, amikor nem szekvenciális környezetben közvetlenül az <span class="emphasis"><em>s</em></span><sub>0</sub> startállapotbeli cselekvés után megkapjuk a jutalmat. Ebben az esetben a stratégiaérték egyszerűen a jutalom várható értéke, tehát:</p><p><span class="inlinemediaobject"><img src="math/mi-21-0026.gif" alt="Stratégiakeresés"/></span></p><p>Most egy egyszerű trükköt alkalmazunk, hogy ezt az összegzést approximálni tudjuk a <span xml:lang="hu" class="emphasis"><em>π<sub>θ</sub></em></span>(<span class="emphasis"><em>s, a</em></span>) által meghatározott valószínűség-eloszlásból generált mintákkal. Tegyük fel, hogy összesen <span class="emphasis"><em>N</em></span> kísérletünk van, és a <span class="emphasis"><em>j</em></span>-edik kísérletben az <span class="emphasis"><em>a<sub>j</sub></em></span> cselekvést választottuk. Ekkor </p><p><span class="inlinemediaobject"><img src="math/mi-21-0027.gif" alt="Stratégiakeresés"/></span></p><p>Tehát a stratégia igazi gradiensét egy szummával közelítettük, ahol a tagok az egyes kísérletek cselekvésválasztási valószínűségeinek gradiensét tartalmazzák. A szekvenciális esetre ez a következőképp általánosítható:</p><p><span class="inlinemediaobject"><img src="math/mi-21-0028.gif" alt="Stratégiakeresés"/></span></p><p>minden egyes bejárt <span class="emphasis"><em>s</em></span> állapotra. Itt <span class="emphasis"><em>a<sub>j</sub> </em></span>a<span class="emphasis"><em> j</em></span>-edik kísérlet során az <span class="emphasis"><em>s</em></span> állapotban választott cselekvés, és <span class="emphasis"><em>R<sub>j</sub></em></span>(<span class="emphasis"><em>s</em></span>) a <span class="emphasis"><em>j</em></span>-edik kísérlet során az <span class="emphasis"><em>s</em></span> állapotból kiindulva gyűjtött összes jutalom. Az eredményül kapott algoritmust <code class="code">REINFORCE</code>-nak nevezték el (Williams, 1992). Rendszerint sokkal hatékonyabb, mint a minden egyes <span xml:lang="hu" class="emphasis"><em>θ</em></span>-ban sok kísérletet használó hegymászó eljárás. Viszont sajnos még mindig sokkal lassabb annál, mint amire szükségünk lenne.</p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Vizsgáljuk a következő feladatot: adott két blackjack<sup>[<a id="id755377" href="#ftn.id755377" class="footnote">219</a>]</sup> program, döntsük el, hogy melyik a jobb! Egyik lehetőség, ha egy közös „bank” ellen játszatjuk őket egy adott számú kártyaleosztásban, és megnézzük, hogy melyik nyert többet. Ezzel az a probléma – mint láttuk –, hogy mindegyik program nyereségei nagy fluktuációt fognak mutatni annak függvényében, hogy milyen lapokat kapott. Egy kézenfekvő megoldás erre, ha előre generálunk egy sor leosztást, egy <span class="emphasis"><em>leosztáshalmazt</em></span>. Ezzel elkerüljük a különböző kártyaleosztások okozta mérési hibát. Ez a <code class="code">PEGASUS</code> algoritmus ötletének alapja (Ng és Jordan, 2000). Az algoritmus olyan területeken alkalmazható, ahol rendelkezésünkre áll egy szimulátor, ezáltal a „véletlen” kísérlet kimenetelek megismételhetővé válnak. Az algoritmus <span class="emphasis"><em>N</em></span> véletlen számsorozatot generál előre, mindegyik felhasználható arra, hogy egy tetszőleges stratégia alapján kísérletet futtassunk vele. A stratégiakeresést úgy hajtjuk végre, hogy mindegyik stratégiajelöltet az alapján értékelünk, hogy <span class="emphasis"><em>ugyanazt</em></span> a véletlen sorozat halmazt használja a kísérletek kimeneti értékeinek meghatározásához. Megmutatható, hogy az <span class="emphasis"><em>összes</em></span> stratégia értékének jó becsléséhez szükséges véletlen sorozatok száma csak a stratégiatér komplexitásától függ, és egyáltalán nem függ a mögöttes terület komplexitásától. A <code class="code">PEGASUS</code> algoritmussal számos területen (például az autonóm helikoptervezetés területén) több hatékony stratégiát fejlesztettek ki (lásd 21.10. ábra).</p></div><div class="figure"><a id="id755406"/><p class="title"><strong>21.10. ábra - Időben eltolt képek egymásra másolása útján kapott eredő kép, amelyen egy nagyon bonyolult „körberepülés orral a kör középpontja fele” manővert hajtanak végre. A helikoptert egy Pegasus stratégiakereső algoritmussal fejlesztett stratégiával vezérlik. Egy szimulátormodellt fejlesztettek a valós helikopter egyes vezérlési beavatkozásokra adott válaszainak vizsgálatára. Ezek után egész éjszaka futtatták az algoritmust a szimulátoron. Egy sor vezérlőt fejlesztettek ki különböző manőverekre. Minden esetben, amikor távirányítást használtak, messze jobb volt az eredmény, mint a képzett humán pilótáké. (A képet Andrew Ng engedélyével közöljük.)</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/21-10.png" alt="Időben eltolt képek egymásra másolása útján kapott eredő kép, amelyen egy nagyon bonyolult „körberepülés orral a kör középpontja fele” manővert hajtanak végre. A helikoptert egy Pegasus stratégiakereső algoritmussal fejlesztett stratégiával vezérlik. Egy szimulátormodellt fejlesztettek a valós helikopter egyes vezérlési beavatkozásokra adott válaszainak vizsgálatára. Ezek után egész éjszaka futtatták az algoritmust a szimulátoron. Egy sor vezérlőt fejlesztettek ki különböző manőverekre. Minden esetben, amikor távirányítást használtak, messze jobb volt az eredmény, mint a képzett humán pilótáké. (A képet Andrew Ng engedélyével közöljük.)"/></div></div></div><div class="footnotes"><br/><hr/><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id754997" href="#id754997" class="para">217</a>] </sup> Triviális, hogy a <span class="inlinemediaobject"><img src="math/mi-21-0032.gif" alt="Stratégiakeresés"/></span> közelítő <span class="emphasis"><em>Q</em></span>-függvény optimális teljesítményt ad, bár egyáltalán nincs közel <span class="emphasis"><em>Q</em></span><sup>*</sup>-hoz.</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id755058" href="#id755058" class="para">218</a>] </sup> Folytonos cselekvési terekre a stratégia a paraméterek sima függvénye lehet.</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id755377" href="#id755377" class="para">219</a>] </sup> 21-es játékként is ismert.</p></div></div></div></body></html>
