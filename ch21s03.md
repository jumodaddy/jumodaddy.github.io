<?xml version="1.0" encoding="UTF-8" standalone="no"?>

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Aktív megerősítéses tanulás"><div class="titlepage"><div><div><h1 class="title"><a id="id752639"/>Aktív megerősítéses tanulás</h1></div></div></div><p>A passzív tanuló ágensnek rögzített stratégiája van, ez határozza meg a viselkedését.
Az aktív ágensnek viszont el kell döntenie, hogy melyik cselekvést válassza. Vegyük először az adaptív dinamikus programozás alapján tanuló ágenst, és nézzük meg azt, hogyan kell módosítani, hogy ezt az új lehetőséget kezelni tudja.</p><p>Először is az ágensnek nem csupán egy rögzített stratégia modelljét kell megtanulnia, hanem egy teljes modellt, amibe az összes cselekvésének lehetséges eredményeihez tartozó valószínűségek is beletartoznak. Erre a <code class="code">PASSZÍV-ADP-ÁGENS</code> esetén alkalmazott egyszerű tanulási eljárás is kiválóan alkalmas. Ezek után figyelembe kell vennünk, hogy az ágens választhat a cselekvések között. A megtanulandó hasznosságok azok, amelyeket az <span class="emphasis"><em>optimális</em></span> stratégia definiál. Ezek a hasznosságok megfelelnek a <a class="xref" href="ch17s02.md#ID_714_oldal">„Az állapotok hasznossága”</a> részben közölt Bellman-egyenletnek, amit itt megismétlünk:</p><p><span class="inlinemediaobject"><img src="math/mi-21-0003.gif" alt="Aktív megerősítéses tanulás"/></span></p><p>A 17. fejezetben közölt értékiterációs vagy stratégiaiterációs algoritmusok segítségével ezek az egyenletek megoldhatók az <span class="emphasis"><em>U</em></span> hasznosságfüggvényre nézve. A végső kérdés az, hogy mit csináljunk az egyes lépésekben. Az egyik lehetőség, hogy a megtanult modell alapján nyert optimális <span class="emphasis"><em>U</em></span> hasznosságfüggvény alapján az ágens meghatároz egy egy lépésre előretekintő optimális cselekvést annak érdekében, hogy maximálja a várható hasznosságot. A másik lehetőség, hogy ha stratégiaiterációt használt, akkor az optimális stratégia már rendelkezésére áll, így „gondolkodás nélkül” csak végre kell hajtania az optimális stratégia által javasolt cselekvést. A kérdés az, hogy valóban végre kell-e hajtania?</p><div class="section" title="Felfedezés"><div class="titlepage"><div><div><h2 class="title"><a id="id752676"/>Felfedezés</h2></div></div></div><p>A 21.6. ábra bemutatja annak a kísérletsorozatnak az eredményét, amely során az ADP-ágens az egyes lépések során tanult optimális stratégia által javasolt cselekvést követi. Látható, hogy az ágens <span class="emphasis"><em>nem tanulja meg</em></span> sem a helyes hasznosságértékeket, sem az igazi optimális stratégiát! Ehelyett a 39-edik kísérletnél megtalál egy stratégiát, amely a +1 jutalomhoz vezet az alsó – (2,1), (3,1), (3,2), (3,3) – úton (lásd 21.6. ábra). Ezek után kisebb változtatásokkal gyűjti a tapasztalatokat, és a 276-ik kísérlettől kezdve mereven ragaszkodik ehhez a stratégiához, így soha nem tanulja meg a többi állapot hasznosságát, és soha nem talál rá a valóban optimális útra a (1,2), (1,3), (2,3) állapotokon keresztül. Ezt <span class="strong"><strong>mohó ágens</strong></span>nek (<span class="strong"><strong>greedy agent</strong></span>) nevezzük. Ismételt kísérletek azt mutatták, hogy a mohó ágens ebben a környezetben <span class="emphasis"><em>nagyon</em></span> <span class="emphasis"><em>ritkán</em></span> találja meg az optimális stratégiát, és időnként borzalmas stratégiákhoz konvergál.</p><p>Hogyan történhet meg, hogy optimális cselekvéseket választva szuboptimális eredményhez jutunk? A válasz abban rejlik, hogy a megtanult modell nem azonos az igazi környezettel, tehát az, ami optimális a megtanult modellben, szuboptimális lehet az igazi környezetben. Sajnálatos módon az ágens nem tudja, hogy milyen a valódi környezet, így nem képes az igazi környezetre nézve optimális cselekvést meghatározni. Mit kell akkor tennünk?</p><div class="figure"><a id="id752705"/><p class="title"><strong>21.6. ábra - Annak a mohó ADP-ágensnek a felmutatott teljesítménye, amely végrehajtja a tanult modell alapján optimálisnak tűnő cselekvést. (a) A kilenc nem végállapot alapján számolt átlagos RMS-hiba. (b) Az a szuboptimális stratégia, amely felé ebben a kísérletsorozatban a mohó ágens konvergál.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/21-06.png" alt="Annak a mohó ADP-ágensnek a felmutatott teljesítménye, amely végrehajtja a tanult modell alapján optimálisnak tűnő cselekvést. (a) A kilenc nem végállapot alapján számolt átlagos RMS-hiba. (b) Az a szuboptimális stratégia, amely felé ebben a kísérletsorozatban a mohó ágens konvergál."/></div></div></div><p>Amit a mohó ágens nem vett figyelembe, az az, hogy a cselekvésnek több funkciója is van annál, mint hogy a jelenleg megtanult modell alapján jutalmat hozzon, ezenfelül hozzájárul az igazi modell megismeréséhez is a nyert észleléseken keresztül. A modell javítása viszont azt eredményezheti, hogy az ágens több jutalomhoz jut a jövőben.<sup>[<a id="id752717" href="#ftn.id752717" class="footnote">214</a>]</sup> Ennek megfelelően az ágensnek kompromisszumot kell kötnie a <span class="strong"><strong>kihasználás</strong></span> (<span class="strong"><strong>exploitation</strong></span>) – a jelenlegi hasznosságbecslésében tükrözött modell alapján történő jutalom maximalizálás – és a <span class="strong"><strong>felfedezés</strong></span> (<span class="strong"><strong>exploration</strong></span>) között, ez utóbbi a hosszú távú eredményességet szolgálja. Ha csupán a kihasználásra tör, akkor azt kockáztatja, hogy beragad valamilyen megszokott kerékvágásba. Ha pedig csak a felfedezéssel törődik, hogy tudását bővítse, de soha nem ülteti át a tudást a gyakorlatba – nem hoz semmilyen hasznot. A való életben állandóan döntenünk kell, hogy folytassuk-e kényelmes életünket, vagy vágjunk neki az ismeretlennek, hátha egy új és jobb életet fedezünk fel. Minél több mindent értünk, annál kevesebb felfedezésre van szükségünk.</p><p>Tudunk-e ennél kicsit precízebbek lenni? Van egyáltalán <span class="emphasis"><em>optimális</em></span> felfedezési stratégia? Kiderült, hogy ezt a kérdést a statisztikus döntéselmélet egy részterülete az ún. <span class="strong"><strong>rabló probléma</strong></span> (<span class="strong"><strong>bandit problem</strong></span>) kapcsán nagy mélységben tárgyalta (lásd a bekeretezett részt).</p><p>Bár a rablók problémáját rendkívül nehéz pontosan megoldani úgy, hogy egy <span class="emphasis"><em>optimális</em></span> felfedezési stratégiát nyerjünk, azonban készíthető egy <span class="emphasis"><em>ésszerű</em></span> terv, ami végül az ágens optimális viselkedéséhez vezet. Minden ilyen eljárás a végtelen felfedezés határán lehet mohó (<span class="strong"><strong>VFHM</strong></span>) (greedy in the limit of infinite exploration, <span class="strong"><strong>GLIE</strong></span>). Egy VFHM-séma alapján működő ágensnek korlátlan számban ki kell próbálnia minden cselekvést az összes állapotban. Ezáltal kerüli el annak veszélyét, hogy egy szokatlanul rossz kimenetelű sorozat miatt véges (nem nulla) valószínűséggel nem talál meg egy optimális cselekvést. Egy ilyen terv alapján működő ADP-ágens végül megtanulja az igazi környezeti modellt. Egy VFHM-tervnek végül mohóvá kell válnia, így az ágens cselekvései optimálissá válnak a megtanult (azaz itt az igazi) modellre nézve.</p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p><span class="strong"><strong>Rablók és felfedezés</strong></span></p><p>Las Vegasban a <span class="emphasis"><em>félkarú rabló</em></span> egy pénzbedobós játékautomata. A játékos bedob egy érmét, meghúzza a kart, és elveszi a nyereményt (ha nyer). Egy <span class="bold"><strong><span class="emphasis"><em>n </em></span>karú rabló</strong></span>nak (<span class="bold"><strong><span class="emphasis"><em>n</em></span>-armed bandit</strong></span>) <span class="emphasis"><em>n</em></span> karja van. A játékosnak minden érme bedobásakor választania kell, hogy melyik kart húzza meg – azt, amelyik eddig a legjobban fizetett, vagy talán egy olyat, amelyet még nem próbált?</p><p>Az <span class="emphasis"><em>n </em></span>karú rabló probléma a világ sok, létfontosságú területén számos valós feladat formális modelljeként szolgálhat, mint például az MI kutatás-fejlesztés éves költségvetésének kialakítása. Mindegyik kar egy cselekvésnek felel meg (mint például 400 millió forint elkülönítése MI-tankönyvek fejlesztésére), és a kar meghúzásával járó nyeremény a cselekvés választása esetén várható haszonnak felel meg (esetünkben óriási haszon várható). Az ismeretlen terület feltárása – akár egy új kutatásról, akár egy új üzletközpontról van szó – kockázatos, drága, bizonytalan haszonnal jár; másrészt ha egyáltalán nem kutatunk, akkor soha nem fogunk <span class="emphasis"><em>semmilyen</em></span> értékes cselekvést felfedezni.</p><p>Ahhoz, hogy a rabló problémát megfelelően formalizáljuk, pontosan meg kell határoznunk, hogy mit értünk optimális viselkedésen. Az irodalomban található legtöbb definíció azt feltételezi, hogy a cél az ágens élettartama alatt elérhető összes jutalom várható értékének maximalizálása. Ezek a definíciók megkívánják, hogy várható értéket képezzünk egyrészt az összes lehetséges világ felett, amelybe csak kerülhet az ágens, másrészt bármelyik adott világ minden egyes lehetséges cselekvési szekvenciája fölött. Itt a „világot” a <span class="emphasis"><em>T</em></span>(<span class="emphasis"><em>s, a, s'</em></span>) állapotátmenet-modell definiálja. Tehát ahhoz, hogy az ágens optimálisan cselekedjék, szüksége van a lehetséges modellek a priori eloszlására. Ezen az úton rendszerint teljesen kezelhetetlen optimalizációs problémákhoz jutunk.</p><p>Néhány esetben azonban – például amikor az egyes gépek nyereményei függetlenek, és leértékelt jutalmakat használunk – minden egyes játékgépre kiszámítható az úgynevezett <span class="strong"><strong>Gittins-index</strong></span> (Gittins, 1989). Az index csak annak függvénye, hogy hányszor választottuk a gépet, és eddig mennyit fizetett ki. Az index azt jelzi minden egyes gépre, hogy mennyire éri meg további pénzt bedobnunk, és a várható kifizetésen és az információ várható értékén alapul. A legnagyobb indexű gép választása az optimális felfedezési stratégia. Sajnálatos módon eddig még nem sikerült megtalálni annak a módját, hogy hogyan kell a Gittins-indexet szekvenciális döntési problémákra kiterjeszteni.</p><p>Az <span class="emphasis"><em>n </em></span>karú rabló elmélet felhasználható arra, hogy bizonyítsuk a genetikus algoritmusok szelekciós stratégiájának ésszerűségét (lásd 4. fejezet). Tekintsünk az <span class="emphasis"><em>n </em></span>karú rabló problémában minden egyes kart egy lehetséges génfüzérnek, továbbá a pénz bedobása ennél a karnál feleljen meg ezen gének reprodukciójának. Ebben az esetben a genetikus algoritmus optimálisan osztja szét a pénzt akkor, ha adott a függetlenségi feltételezések egy megfelelő halmaza.</p></div><p>Számos VFHM-séma ismert; az egyik legegyszerűbb, ha az ágens az idő 1/<span class="emphasis"><em>t</em></span> részében véletlen cselekvést választ, egyébként pedig mohó stratégiát követ. Tény, hogy ez végül optimális stratégiához vezet, de időnként rendkívül lassan konvergál. Egy értelmesebb megközelítés, ha az ágens némi súlyt ad azoknak a cselekvéseknek, amelyeket nem használt még gyakran, miközben igyekszik elkerülni azokat, amelyeknek a hasznosságát kicsinek hiszi. Ezt meg tudjuk valósítani úgy, ha úgy módosítjuk a (21.4) kényszeregyenletet, hogy nagyobb hasznosságot tulajdonítson a relatíve kipróbálatlan állapot-cselekvés pároknak. Lényegében egy optimista priort hoz létre, az ágens kezdetben úgy viselkedik, mintha hatalmas jutalmak lennének szétszórva a helyszínen. Jelölje <span class="emphasis"><em>U<sup>+</sup></em></span>(<span class="emphasis"><em>s</em></span>) az <span class="emphasis"><em>s</em></span> állapot hasznosságának (tehát a hátralevő jutalom várható értékének) optimista becslését, <span class="emphasis"><em>N</em></span>(<span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>s</em></span>) pedig azt a számot, ahányszor az <span class="emphasis"><em>s</em></span> állapotban az <span class="emphasis"><em>a</em></span> cselekvést választottuk. Tegyük fel, hogy az ADP-tanulást végző ágens értékiterációt végez, ez esetben a frissítési egyenletet [(17.6) egyenlet] át kell írnunk úgy, hogy az az optimista becslést tartalmazza. A következő összefüggés ezt megvalósítja:</p><p><span class="inlinemediaobject"><img src="math/mi-21-0004.gif" alt="Annak a mohó ADP-ágensnek a felmutatott teljesítménye, amely végrehajtja a tanult modell alapján optimálisnak tűnő cselekvést. (a) A kilenc nem végállapot alapján számolt átlagos RMS-hiba. (b) Az a szuboptimális stratégia, amely felé ebben a kísérletsorozatban a mohó ágens konvergál."/></span></p><p>Itt <span class="emphasis"><em>f </em></span>(<span class="emphasis"><em>u</em></span>, <span class="emphasis"><em>n</em></span>) az úgynevezett <span class="strong"><strong>felfedezési függvény</strong></span> (<span class="strong"><strong>exploration function</strong></span>). Ez határozza meg a mohóság (a magas <span class="emphasis"><em>u</em></span> értékek preferenciája) és a kíváncsiság (az alacsony <span class="emphasis"><em>n</em></span> értékek preferenciája – cselekvések, amelyeket még nem alkalmaztunk nagyon gyakran) közötti kompromisszumot. Az <span class="emphasis"><em>f </em></span>(<span class="emphasis"><em>u</em></span>, <span class="emphasis"><em>n</em></span>) függvénynek <span class="emphasis"><em>u</em></span>-ban növekvőnek, <span class="emphasis"><em>n</em></span>-ben csökkenőnek kell lennie. Nyilvánvalóan sok lehetséges függvény megfelel ennek a feltételnek. Egy különösen egyszerű függvénydefiníció:</p><p><span class="inlinemediaobject"><img src="math/mi-21-0005.gif" alt="Annak a mohó ADP-ágensnek a felmutatott teljesítménye, amely végrehajtja a tanult modell alapján optimálisnak tűnő cselekvést. (a) A kilenc nem végállapot alapján számolt átlagos RMS-hiba. (b) Az a szuboptimális stratégia, amely felé ebben a kísérletsorozatban a mohó ágens konvergál."/></span></p><p>ahol <span class="emphasis"><em>R</em></span><sup>+</sup> a tetszőleges állapotban kapható legnagyobb jutalom optimista becslése, míg <span class="emphasis"><em>N<sub>e</sub></em></span> egy rögzített paraméter. Ennek alkalmazásával az ágens minden állapot-cselekvés párt legalább <span class="emphasis"><em>N<sub>e</sub></em></span> alkalommal kipróbál.</p><p>Az, hogy a (21.5) egyenlet jobb oldalán nem <span class="emphasis"><em>U</em></span>, hanem <span class="emphasis"><em>U<sup>+</sup></em></span> szerepel, nagyon fontos. Ahogy a felfedezés folyik, a startállapot környéki állapotok és cselekvések jó néhányszor kipróbálásra kerülhetnek. Ha <span class="emphasis"><em>U</em></span>-t – a pesszimistább becslést – használtuk volna, akkor az ágens hamar elveszthetné a szabad területek felfedezése iránti vonzalmát.
Az <span class="emphasis"><em>U<sup>+</sup></em></span> szerepeltetése azt jelenti, hogy a felfedezéshez kapcsolt jutalom visszaterjed a felfedezetlen terület határáról. Így nem csupán a szokatlan cselekvéseket preferáljuk, hanem a felértékeljük azokat a cselekvéseket, amelyek a felfedezetlen területek <span class="emphasis"><em>felé</em></span> fogják vinni az ágenst. Ennek a felfedezési stratégiának a hatása jól látható a 21.7. ábrán, gyors konvergenciát látunk az optimális teljesítményhez, ellentétben a mohó ágensnél látottakkal. Az optimálishoz nagyon közeli stratégiát talál mindössze 18 kísérlet után. Vegyük észre, hogy maguk a hasznosságérték-becslések nem konvergálnak ilyen gyorsan. Ennek az az oka, hogy az ágens meglehetősen hamar felhagy az állapottér jutalmat nem eredményező részeinek felfedezésével, oda már csak „véletlenszerűen” téved. Mindamellett teljesen ésszerű az ágens részéről, hogy nem sokat törődik azon állapotok hasznosságának pontos értékével, amelyekről tudja, hogy nemkívánatosak és elkerülhetők.</p><div class="figure"><a id="id754116"/><p class="title"><strong>21.7. ábra - A felfedező ADP-ágens teljesítménye, <span class="emphasis"><em>R<sup>+</sup> = </em></span>2 és <span class="emphasis"><em>N<sub>e</sub> = </em></span>5 esetén. (a) Néhány kiválasztott állapot hasznosságának becslése az idő függvényében. (b) A hasznosságértékek RMS-hibája, és az ehhez kapcsolható, a stratégiából származó veszteség.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/21-07.png" alt="A felfedező ADP-ágens teljesítménye, R+ = 2 és Ne = 5 esetén. (a) Néhány kiválasztott állapot hasznosságának becslése az idő függvényében. (b) A hasznosságértékek RMS-hibája, és az ehhez kapcsolható, a stratégiából származó veszteség."/></div></div></div></div><div class="section" title="Egy cselekvésérték-függvény tanulása"><div class="titlepage"><div><div><h2 class="title"><a id="id754140"/>Egy cselekvésérték-függvény tanulása</h2></div></div></div><p>Most, hogy már van egy aktív ADP-ágensünk, nézzük meg, hogy mi módon tudunk egy aktív időbeli különbség tanuló ágenst létrehozni. A legszembetűnőbb eltérés a passzív esethez képest, hogy az ágensnek többé nem áll rendelkezésére egy rögzített stratégia, tehát ha megtanul valamilyen <span class="emphasis"><em>U</em></span> hasznosságfüggvényt, akkor meg kell tanulnia egy modellt is, hogy egy <span class="emphasis"><em>U</em></span>-n alapuló egylépéses előretekintő keretben cselekvést tudjon választani. A modellkialakítás problémája ugyanaz az IK-ágensnél, mint az ADP-ágensnél. Mi a helyzet magával az IK frissítési szabállyal? Talán meglepő, de a (21.3) IK frissítési szabály változatlan marad. Ez furcsának tűnhet a következő okból: tegyük fel, hogy az ágens olyan lépést választ, ami normál esetben jó célhoz vezet, de a környezet nemdeterminisztikus volta miatt az ágens valamilyen katasztrofális állapotban végzi. Az IK frissítési szabály ezt éppoly komolyan veszi, mintha ez a kimenetel a cselekvés normál következménye lenne, pedig csak egy pech volt, és nem kellene sokat aggódnia miatta. Valójában az ilyen valószínűtlen kimenetel csak ritkán fordul elő egy nagy tanítósorozat-halmazban, így reményeink szerint hosszú távon a valószínűségével arányosan kap súlyt a hatása. Ismét azt mondhatjuk, hogy az IK-ágens ugyanazokhoz az értékekhez fog tartani, mint az ADP-ágens, ahogy a tanítósorozatok száma tart a végtelenhez.</p><p>Van egy alternatív IK-módszer, amit <span class="bold"><strong><span class="emphasis"><em>Q</em></span>-tanulás</strong></span>nak (<span class="bold"><strong><span class="emphasis"><em>Q</em></span>-learning</strong></span>) hívunk, ez cselekvésérték-reprezentációt tanul, nem hasznosságot. <span class="emphasis"><em>Q</em></span>(<span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>s</em></span>)-sel fogjuk jelölni annak az értékét, ha az <span class="emphasis"><em>s</em></span> állapotban az <span class="emphasis"><em>a</em></span> cselekvést választjuk. A <span class="emphasis"><em>Q</em></span>-értékek közvetlenül összekapcsolhatók a hasznosságértékekkel a következő összefüggés alapján:</p><p><span class="inlinemediaobject"><img src="math/mi-21-0006.gif" alt="Egy cselekvésérték-függvény tanulása"/></span></p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>A <span class="emphasis"><em>Q</em></span>-függvény úgy tűnhet, mintha csupán a hasznosságinformáció tárolásának egy eltérő módja lenne, de van egy nagyon fontos tulajdonsága: <span class="emphasis"><em>egy Q-függvényt tanuló IK- ágensnek nincs szüksége sem a tanulás, sem a cselekvés kiválasztás modelljére.</em></span> Ezért a <span class="emphasis"><em>Q</em></span>-tanulást <span class="strong"><strong>modellmentes</strong></span> (<span class="strong"><strong>model-free</strong></span>) módszernek is nevezik. Éppúgy, mint a hasznosságoknál, felírhatunk egy egyensúlyi egyenletet, amelyet a korrekt <span class="emphasis"><em>Q</em></span>-értékeknek ki kell elégíteniük:</p><p><span class="inlinemediaobject"><img src="math/mi-21-0007.gif" alt="Egy cselekvésérték-függvény tanulása"/></span></p><p>Az ADP-tanulást végző ágenssel azonos módon itt is felhasználhatjuk ezt az egyenletet egy iterációs eljárásban arra, hogy egy becsült modell alapján pontos <span class="emphasis"><em>Q</em></span>-értékeket számoljunk. Ez viszont azt kívánná meg, hogy a modellt is megtanuljuk, hiszen az egyenlet használja a <span class="emphasis"><em>T</em></span>(<span class="emphasis"><em>s</em></span>, <span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>s'</em></span>)-t. Másrészről viszont az IK-megközelítés nem igényel modellt. Így a <span class="emphasis"><em>Q</em></span>-értékekre vonatkozó IK frissítési összefüggés:</p><p><span class="inlinemediaobject"><img src="math/mi-21-0008.gif" alt="Egy cselekvésérték-függvény tanulása"/></span></p><p>amely mindig kiszámításra kerül, amikor az <span class="emphasis"><em>s</em></span> állapotban az <span class="emphasis"><em>a</em></span> cselekvést választottuk, amely az <span class="emphasis"><em>s'</em></span> állapotba vitt.</p></div><p>A 21.8. ábra mutatja be az IK-módszert használó, felfedező <span class="emphasis"><em>Q</em></span>-tanuló ágens teljes programját. Vegyük észre, hogy ugyanazt az <span class="emphasis"><em>f</em></span> felfedezési függvényt használja, mint amit a felfedező ADP-ágens is használt – ezért van szükség a végrehajtott cselekvések gyakorisági statisztikáira (az <span class="emphasis"><em>N</em></span> táblára). Ha egy egyszerűbb felfedezési stratégiát használnánk – mondjuk a lépések egy részében véletlenszerű működést iktatva be, ahol ez részarány az idővel csökken –, akkor mellőzhetnénk ezt a statisztikát.</p><p>A <span class="emphasis"><em>Q</em></span>-tanuló ágens megtanulja a 4 × 3-as világra az optimális stratégiát, de ezt sokkal lassabban teszi, mint az ADP-ágens. Ennek oka, hogy az IK nem kényszerít modellje alapján konzisztenciát az értékekre. Ez az összehasonlítás felvet egy általános kérdést: mi a jobb: egy modellt és egy hasznosságfüggvényt tanulni, vagy egy cselekvésértékfüggvényt? Más szavakkal, mi a legjobb módja egy ágensfüggvény reprezentálásának? Ez a mesterséges intelligencia alapjait érintő kérdés. Mint az 1. fejezetben kijelentettük, az MI legnagyobb részében kulcsfontosságú történeti örökség a (gyakran ki nem mondott) ragaszkodás a <span class="strong"><strong>tudásalapú</strong></span> (<span class="strong"><strong>knowledge-based</strong></span>) megközelítéshez. Ez vezet ahhoz a feltevéshez, hogy az ágensfüggvény reprezentációjának legjobb módja, ha valamilyen szempontból az ágenst körülvevő környezet modelljét építjük meg.</p><div class="figure"><a id="id754311"/><p class="title"><strong>21.8. ábra - Egy felfedező <span class="emphasis"><em>Q</em></span>-tanuló ágens. Egy aktív tanuló, amely megtanulja minden cselekvésnek minden egyes állapotban a <span class="emphasis"><em>Q</em></span>(<span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>s</em></span>) értékét. Ugyanazt az <span class="emphasis"><em>f</em></span> felfedezési függvényt használja, amelyet a felfedező ADP-ágens is használt, de elkerüli az állapotátmenet-modell tanulását, mivel az állapot <span class="emphasis"><em>Q</em></span>-értékét közvetlenül a szomszédos állapotok <span class="emphasis"><em>Q</em></span>-értékeihez tudja kapcsolni.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/21-08.png" alt="Egy felfedező Q-tanuló ágens. Egy aktív tanuló, amely megtanulja minden cselekvésnek minden egyes állapotban a Q(a, s) értékét. Ugyanazt az f felfedezési függvényt használja, amelyet a felfedező ADP-ágens is használt, de elkerüli az állapotátmenet-modell tanulását, mivel az állapot Q-értékét közvetlenül a szomszédos állapotok Q-értékeihez tudja kapcsolni."/></div></div></div><p>Néhány kutató ugyanakkor – mind az MI-közösségen belül, mind azon kívül – azt állítja, hogy a modellmentes módszerek, mint például a <span class="emphasis"><em>Q</em></span>-tanulás léte azt mutatja, hogy a tudásalapú megközelítés nem szükséges. Mindamellett nem sok egyéb van e kérdés eldöntésére, mint az intuíciónk. A mi intuíciónk szerint, ahogy a környezet komplexebbé válik, a tudásalapú megközelítés előnyei egyre nyilvánvalóbbá válnak. Ez még a játékokban – mint a sakk, a dámajáték és az ostáblajáték (lásd következő alfejezet) – is így jelentkezik, a modellalapú kiértékelő függvények tanulására fordított erőfeszítések több sikert hoztak, mint a <span class="emphasis"><em>Q</em></span>-tanuló módszerek.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id752717" href="#id752717" class="para">214</a>] </sup> Vegyük észre a 16. fejezetben tárgyalt információérték-elmélettel fennálló közvetlen analógiát.</p></div></div></div></body></html>
