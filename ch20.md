<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="chapter" title="20. fejezet - Statisztikai tanulási módszerek"><div class="titlepage"><div><div><h1 class="title"><a id="id738312"/>20. fejezet - Statisztikai tanulási módszerek</h1></div></div></div><p><span class="emphasis"><em>Ebben a fejezetben megfigyelések alapján történő bizonytalan következtetésnek tekintjük a tanulást.</em></span></p><p>Az V. részben rámutattunk, hogy a valós életre jellemző környezetben gyakran előfordul, hogy a tudás bizonytalan. Az ágensek a bizonytalanságot valószínűség- és döntéselméleti módszerekkel tudják kezelni, de ehhez először tapasztalataik alapján fel kell állítaniuk a világra vonatkozó valószínűségi modelljüket. Ez a fejezet bemutatja, hogy milyen módon tudják ezt megtenni. Látni fogjuk, hogyan kell úgy formalizálni a tanulási feladatot, mint egy valószínűségi következtetési folyamatot (lásd 20.1. alfejezet). Bemutatjuk, hogy a tanulás bayesi megközelítése rendkívül hatékony: általános megoldást ad a zaj, a túlilleszkedés és az optimális predikció problémáira. Továbbá figyelembe veszi azt a tényt, hogy a nem-egészen-mindentudó ágens soha nem tudhatja biztosan, hogy a világról alkotott melyik elmélet helyes, mégis döntéseket kell hoznia.</p><p>A 20.2. és 20.3. alfejezetben valószínűségi modellek – elsősorban a Bayes-hálók – tanulási módszereit ismertetjük. A 20.4. alfejezet olyan módszereket tárgyal, amelyek speciális minták tárolását és előhívását végzik. A 20.5. alfejezet a <span class="strong"><strong>neurális háló</strong></span> (<span class="strong"><strong>neural network</strong></span>) tanítással foglalkozik, míg a 20.6. alfejezet a <span class="strong"><strong>kernelgépek</strong></span>kel (<span class="strong"><strong>kernel machine</strong></span>). Ennek a fejezetnek egy része erősebben matematikai megközelítésű (feltételezi a többváltozós analízis alapjainak ismeretét), bár az általános tanulságok megérthetők anélkül is, hogy elmerülnénk a matematikai részletekben. Hasznos lehet, ha az olvasó ezen a ponton először átnézi a 13. és a 14. fejezeteket, valamint az A) függeléket.</p><div class="section" title="Statisztikai tanulás"><div class="titlepage"><div><div><h1 class="title"><a id="id738343"/>Statisztikai tanulás</h1></div></div></div><p>Az ebben a fejezetben használt alapvető koncepciók ismét az<span class="strong"><strong> adat</strong></span> (<span class="strong"><strong>data</strong></span>) és a <span class="strong"><strong>hipotézis</strong></span> (<span class="strong"><strong>hypothesis</strong></span>), <span class="strong"><strong>éppúgy</strong></span>, mint ahogy a 18. fejezetben volt. Itt az adatok a <span class="strong"><strong>tény</strong></span>ek (<span class="strong"><strong>evidence</strong></span>) – ezek a területet leíró valószínűségi változók egy részének vagy mindegyikének egy konkrét megvalósulását jelentik. A hipotézis valamilyen valószínűségi elmélet arról, hogy a világ adott területe hogyan is működik, ennek speciális esetei a logikai elmélettel leírt területek.</p><p>Vegyünk egy <span class="emphasis"><em>nagyon</em></span> egyszerű példát. Kedvenc „meglepetés” cukorkánk kétféle, meggy- (nyam-nyam) és citrom- (brrrr) ízben kapható. A cukorkagyártónak sajátos humora van, és mindegyik cukorkát – ízétől függetlenül – ugyanolyan átlátszatlan papírba csomagolja. Az édességet nagyon nagy zsákokban árulják, amelyekből ötféle van – kívülről megint csak megkülönböztethetetlenek egymástól:</p><p><code class="code"><em><span class="remark">h</span></em><sub>1</sub>: 100% meggy</code></p><p><code class="code"><em><span class="remark">h</span></em><sub>2</sub>: 75% meggy + 25% citrom</code></p><p><code class="code"><em><span class="remark">h</span></em><sub>3</sub>: 50% meggy + 50% citrom</code></p><p><code class="code"><em><span class="remark">h</span></em><sub>4</sub>: 25% meggy + 75% citrom</code></p><p><code class="code"><em><span class="remark">h</span></em><sub>5</sub>: 100% citrom</code></p><p>Egy adott új zsák cukorka esetén a <span class="emphasis"><em>H</em></span> (<span class="emphasis"><em>hipotézis</em></span>t jelölő) véletlen változó a zsák típusát jelenti, lehetséges értékei <span class="emphasis"><em>h</em></span><sub>1</sub>-től <span class="emphasis"><em>h</em></span><sub>5</sub>-ig terjednek. Természetesen <span class="emphasis"><em>H</em></span> közvetlenül nem figyelhető meg. Ahogy a cukorkákat felbontjuk és megvizsgáljuk, adatokat gyűjtünk – amelyeket <span class="emphasis"><em>D</em></span><sub>1</sub>, <span class="emphasis"><em>D</em></span><sub>2</sub>, …, <span class="emphasis"><em>D<sub>N</sub></em></span> jelöl. Mindegyik <span class="emphasis"><em>D<sub>i</sub></em></span> egy olyan véletlen változó, amelynek lehetséges értékei a <span class="emphasis"><em>meggy</em></span>, illetve a <span class="emphasis"><em>citrom</em></span>. Az ágens feladata, hogy jóslást adjon a következő cukorka ízére.<sup>[<a id="id738968" href="#ftn.id738968" class="footnote">194</a>]</sup> Ez a nyilvánvalóan triviális feladat érdekes módon mégis jó betekintést nyújt a legfontosabb problémák közül többre is. Az ágensnek valóban elméletet kell alkotnia a világról, bár csak nagyon egyszerűt.</p><p>A <span class="strong"><strong>Bayes-tanulás</strong></span> (<span class="strong"><strong>Bayesian learning</strong></span>) során egyszerűen kiszámítjuk minden egyes hipotézis valószínűségét az adatokra támaszkodva, majd ennek alapján adunk predikciót. Azaz nem egyetlen „legjobb” hipotézist használunk a predikcióhoz, hanem az öszszes hipotézist használjuk, valószínűségükkel súlyozva őket. Reprezentálja <span class="strong"><strong>D</strong></span> az összes adatot, legyen <span class="strong"><strong>d</strong></span> a megfigyelt értékek vektora, ekkor az egyes hipotézisek valószínűségét a Bayes-szabállyal adhatjuk meg:</p><p><code class="code"><em><span class="remark">P</span></em>(<em><span class="remark">h<sub>i</sub></span></em>|<em><span class="remark">d</span></em>) = α<em><span class="remark">P</span></em>(<em><span class="remark">d</span></em>|<em><span class="remark">h<sub>i</sub></span></em>)<em><span class="remark">P</span></em>(<em><span class="remark">h<sub>i</sub></span></em>) 										(20.1)</code></p><p>Tegyük fel, hogy egy ismeretlen <span class="emphasis"><em>X</em></span> mennyiségre vonatkozó predikció a célunk. Ebben az esetben:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0001.gif" alt="Statisztikai tanulás"/></span></p><p>ahol azt feltételeztük, hogy az összes hipotézis meghatároz <span class="emphasis"><em>X</em></span>-re valamilyen eloszlást. Ez az összefüggés azt mutatja, hogy a predikció az egyes hipotézisekből adódó predikciók súlyozott összege. Maguk a hipotézisek tulajdonképpen a nyers adatok és a predikciók közti „közvetítők”. A Bayes-megközelítésben a <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>h<sub>i</sub></em></span>) <span class="strong"><strong>prior hipotézis</strong></span>ek (<span class="strong"><strong>hypothesis prior</strong></span>), illetve a hipotézisek mellett fellépő <span class="emphasis"><em>P</em></span>(<span class="strong"><strong>d</strong></span>|<span class="emphasis"><em>h<sub>i</sub></em></span>) adat<span class="strong"><strong>valószínűség</strong></span>ek (<span class="strong"><strong>likelihood</strong></span>) a kulcsmennyiségek.</p><p>A cukorka példánkban azt feltételezzük, hogy a <span class="emphasis"><em>h</em></span><sub>1</sub>,…, <span class="emphasis"><em>h</em></span><sub>5</sub> a priori valószínűségei megfelelnek a gyártó reklámjában közölt 〈0,1, 0,2, 0,4, 0,2, 0,1〉 értékeknek. Az adatok valószínűségét <span class="strong"><strong>e.f.e</strong></span> (angol rövidítése: <span class="strong"><strong>i.i.d.</strong></span>) feltételezéssel számítjuk – azaz egyforma és független eloszlást (independently and identically distributed) teszünk fel, így: </p><p><span class="inlinemediaobject"><img src="math/mi-20-0002.gif" alt="Statisztikai tanulás"/></span></p><p>Tegyük fel például, hogy a cukorkás zsák valójában csupa citromtípusú (<span class="emphasis"><em>h</em></span><sub>5</sub>), és az első 10 kibontott cukorka mind citromízű, ekkor <span class="emphasis"><em>P</em></span>(<span class="strong"><strong>d</strong></span>|<span class="emphasis"><em>h</em></span><sub>3</sub>) = 0,5<sup>10</sup>, mivel a <span class="emphasis"><em>h</em></span><sub>3</sub> típusú zsákokban a cukorkák fele citrom.<sup>[<a id="id739172" href="#ftn.id739172" class="footnote">195</a>]</sup> A 20.1. (a) ábra mutatja, hogyan változik az öt hipotézis a posteriori<span class="emphasis"><em> </em></span>valószínűsége, ahogy sorban észleljük a 10 citromízű cukorkát. Vegyük észre, hogy a valószínűségek az a priori értékekről indulnak, ennek megfelelően kezdetben <span class="emphasis"><em>h</em></span><sub>3</sub> a legvalószínűbb lehetőség, és ez így is marad még az első cukor felbontása után is. A második citromízű cukorka felbontása után <span class="emphasis"><em>h</em></span><sub>4</sub> a legvalószínűbb, 3 és több esetén <span class="emphasis"><em>h</em></span><sub>5</sub> (a rettegett csupa citrom zsák). 10 citromízű cukorka után már meglehetősen biztosak vagyunk végzetünket illetően. A 20.1. (b) ábra mutatja annak a (20.2) egyenlet alapján jósolt valószínűségét, hogy a következő cukorka citromízű. Várakozásunknak megfelelően monoton növekszik az 1 felé.</p><div class="figure"><a id="id739200"/><p class="title"><strong>20.1. ábra - (a) A (20.1) egyenletből számított <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>h<sub>i</sub></em></span>|<span class="emphasis"><em>d</em></span><sub>1</sub>,…,<span class="emphasis"><em>d<sub>N</sub></em></span>) a posteriori valószínűségek. A megfigyelések száma 1-től 10-ig terjed, és mindegyik megfigyelés citromízű cukorka. (b) A (20.2) egyenlet alapján számított <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>d</em></span><sub><span class="emphasis"><em>N</em></span>+1 </sub>=<span class="emphasis"><em> citrom</em></span>|<span class="emphasis"><em>d</em></span><sub>1</sub>,…,<span class="emphasis"><em>d<sub>N</sub></em></span>) Bayes-predikció.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20.01.png" alt="(a) A (20.1) egyenletből számított P(hi|d1,…,dN) a posteriori valószínűségek. A megfigyelések száma 1-től 10-ig terjed, és mindegyik megfigyelés citromízű cukorka. (b) A (20.2) egyenlet alapján számított P(dN+1 = citrom|d1,…,dN) Bayes-predikció."/></div></div></div><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Példánk azt mutatja, hogy <span class="emphasis"><em>a Bayes-predikcióban az igaz hipotézis végülis dominánssá válik</em></span>. Ez jellemző a Bayes-tanulásra. Bármilyen rögzített priorra, amely nem zárja ki a helyes megoldást, a hamis hipotézisek a posteriori valószínűsége végülis nullához tart. Ennek egyszerűen az az oka, hogy elhanyagolhatóan kicsi annak valószínűsége, hogy végtelen ideig „nem jellemző” adatokat generáljunk. (Ezen a ponton érvelésünk hasonló a 18. fejezetben, a VKH-tanulásnál alkalmazottal.) Ennél is fontosabb, hogy a Bayes-predikció <span class="emphasis"><em>optimális,</em></span> akár kicsi, akár nagy adathalmazunk van. Adott a priori<span class="emphasis"><em> </em></span>hipotéziseloszlás mellett bármely más predikció ritkábban lesz helyes, mint a Bayes-predikció.</p></div><p>A Bayes-tanulás optimalitásának természetesen ára van. Mint a 18. fejezetben láttuk, a valós tanulási problémáknál a hipotézistér rendszerint nagyon nagy vagy végtelen. Néha a (20.2) egyenletben az összegzés (vagy folytonos esetben az integrálás) pontos elvégzése kezelhető problémára vezet, de a legtöbb esetben közelítő vagy egyszerűsített megoldásokra kell szorítkoznunk.</p><a id="ID_821_oldal"/><p>Nagyon elterjedt approximációs módszer – a tudományos feladatokban rendszerint ezt alkalmazzuk –, hogy egyetlen, a <span class="emphasis"><em>legvalószínűbb</em></span> hipotézis alapján végezzük a predikciót, azaz olyan <span class="emphasis"><em>h<sub>i</sub></em></span> alapján, amely maximálja a <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>h<sub>i</sub></em></span>|<span class="strong"><strong>d</strong></span>)-t. Ezt <span class="strong"><strong>maximum a posteriori</strong></span> vagy <span class="strong"><strong>MAP </strong></span>hipotézisnek nevezzük. A MAP hipotézis alapján végzett predikciók közelítőleg Bayes-predikciók. Ez a közelítés annyira jó, amennyire jó a <span class="strong"><strong>P</strong></span>(<span class="emphasis"><em>X</em></span>|<span class="strong"><strong>d</strong></span>) <span class="strong"><strong>≈</strong></span> <span class="strong"><strong>P(X</strong></span>|<span class="emphasis"><em>h</em></span><sub>MAP</sub> közelítés. A cukorka példánkban három egymás utáni citromízű cukor észlelése után <span class="emphasis"><em>h</em></span><sub>MAP </sub>= <span class="emphasis"><em>h</em></span><sub>5</sub>, így a MAP-tanulás alapján a negyedik cukorra 1,0 valószínűséggel citromízűt jósolunk. Ez nyilván sokkal veszélyesebb jóslat, mint a Bayes-predikció, amely 0,8valószínűségű, ahogy a 20.1. ábrán is láthatjuk. Ahogy egyre több adatunk van, a MAP és Bayes-predikciók egyre inkább konvergálnak egymáshoz, mivel a MAP hipotézis alternatívái egyre kevésbé valószínűvé válnak. Bár példánk ezt nem mutatja, de a MAP hipotézis előállítása sokszor lényegesen egyszerűbb, mint a Bayes-tanulásé. Ennek oka, hogy csupán egy optimalizálási probléma megoldását igényli, szemben egy nagyon nagy összegzési (vagy integrálási) problémával. A fejezet későbbi részében majd látunk példákat erre.</p><p>Mind a Bayes-tanulásban, mind a MAP-tanulásban a <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>h<sub>i</sub></em></span>) a priori hipotézis valószínűségek nagy szerepet játszanak. Láttuk a 18. fejezetben, hogy <span class="strong"><strong>túlilleszkedés</strong></span> (<span class="strong"><strong>overfitting</strong></span>) léphet fel, ha túlzottan nagy a hipotézistér kifejezőképessége, vagyis túl sok olyan hipotézist tartalmaz, amely jól illeszkedik az adatokra. A Bayes- és a MAP-tanulás nem alkalmaz valamilyen önkényes korlátot a figyelembe vett hipotézisekre, inkább az a priori valószínűségeket használják fel arra, hogy <span class="emphasis"><em>büntessék a hipotézisek komplexitását</em></span>. A komplex hipotéziseknek tipikusan kisebb az a priori<span class="emphasis"><em> </em></span>valószínűsége – részben azért, mert sokkal több komplex hipotézis van, mint egyszerű. Másrészről a komplex hipotéziseknek nagyobb kapacitása van az adatokra való illeszkedéshez. (Extrém esetet véve egy táblázat 1,0 valószínűséggel és tökéletes pontossággal reprodukálni tudja az adatokat.) Ennek megfelelően az a priori valószínűségben testesül meg a hipotéziskomplexitás és az adatokra való illeszkedési képesség közötti kompromisszum.</p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Ezt a kompromisszumot a logikai esetben figyelhetjük meg legjobban, amikor is <span class="emphasis"><em>H</em></span> csak <span class="emphasis"><em>determinisztikus</em></span> hipotéziseket tartalmaz. Ebben az esetben <span class="strong"><strong>P</strong></span>(<span class="strong"><strong>d</strong></span>|<span class="emphasis"><em>h<sub>i</sub></em></span>)<span class="strong"><strong> </strong></span>értéke akkor 1, ha <span class="emphasis"><em>h<sub>i </sub></em></span>konzisztens az adatokkal, különben 0. A (20.1) egyenlet alapján azt látjuk, hogy <span class="emphasis"><em>h</em></span><sub>MAP </sub>a <span class="emphasis"><em>legegyszerűbb logikai hipotézis</em></span> lesz, <span class="emphasis"><em>amely konzisztens az adatokkal</em></span>. Tehát a maximum a posteriori tanulás Ockham borotvájának egy természetes megvalósulása.</p></div><p>A hipotéziskomplexitás és az adatokra való illeszkedési képesség közötti kompromisszumot új módon világítja meg, ha a (20.1) egyenlet logaritmusát képezzük. A <span class="emphasis"><em>h</em></span><sub>MAP</sub> hipotézis olyan kiválasztása, amely maximálja a <span class="strong"><strong>P</strong></span>(<span class="strong"><strong>d</strong></span>|<span class="emphasis"><em>h<sub>i</sub></em></span>)<span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>h<sub>i</sub></em></span>)-t ugyanaz, mint amikor minimalizáljuk a következő kifejezést:</p><p><code class="code">–log<sub>2</sub><em><span class="remark">P</span></em>(<em><span class="remark">d</span></em>|<em><span class="remark">h<sub>i</sub></span></em>) –log<sub>2</sub><em><span class="remark">P</span></em>(<em><span class="remark">h<sub>i</sub></span></em>)</code></p><p>Használjuk fel az információkódolás és a valószínűség között a 18. fejezetben bevezetett kapcsolatot. Azt látjuk, hogy a –log<sub>2</sub><span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>h<sub>i</sub></em></span>) tag nem más, mint a <span class="emphasis"><em>h<sub>i </sub></em></span>hipotézis specifikálásához szükséges bitek száma. A –log<sub>2</sub> <span class="emphasis"><em>P</em></span>(<span class="strong"><strong>d</strong></span>|<span class="emphasis"><em>h<sub>i</sub></em></span>)-tag viszont azoknak a további biteknek a száma, amelyek ahhoz szükségesek, hogy az adott hipotézis feltételezésével specifikáljuk az adatokat. (Ennek demonstrálására mutatjuk be azt az esetet, amikor a hipotézis pontosan megjósolja az adatokat, ilyenkor nincs szükség egyetlen bitre sem az adatok specifikálásához. Ilyen például a <span class="emphasis"><em>h</em></span><sub>5</sub> hipotézis esete, amikor sorban érkeznek a citromízű cukorkák – és valóban log<sub>2</sub> 1 = 0.) Ebben az értelemben a MAP-tanulás jellemzője, hogy maximálisan <span class="emphasis"><em>tömöríti</em></span> az adatokat. Ezt a feladatot sokkal közvetlenebbül célozza a <span class="strong"><strong>minimális hosszúságú leírás</strong></span> (<span class="strong"><strong>MHL</strong></span>) (<span class="strong"><strong>minimum description length</strong></span>, <span class="strong"><strong>MDL</strong></span>) tanulási módszer, amely a valószínűségekkel való foglalkozás helyett a hipotézis méretének és az adat kódolásának minimalizálására törekszik.</p><p>Az utolsó egyszerűsítést az adja, ha a hipotézistérben <span class="strong"><strong>egyenletes</strong></span> (<span class="strong"><strong>uniform</strong></span>) priort feltételezünk. Ebben az esetben a MAP-tanulás egy olyan <span class="emphasis"><em>h<sub>i </sub></em></span>választására redukálódik, amely maximálja <span class="emphasis"><em>P</em></span>(<span class="strong"><strong>d</strong></span>|<span class="emphasis"><em>h<sub>i</sub></em></span>) -t. Ezt <span class="strong"><strong>maximum-likelihood</strong></span> (<span class="strong"><strong>ML</strong></span>) hipotézisnek nevezzük, és <span class="emphasis"><em>h</em></span><sub>ML</sub>-lel jelöljük. A maximum-likelihood tanulás nagyon elterjedt a statisztikában. Ez egy olyan tudomány, amelynek sok kutatója nem bízik az a priori hipotézisek szubjektív természetében. Ez józan megközelítés akkor, amikor nincs semmi okunk, hogy apriori kitüntessük az egyik hipotézist egy másikkal szemben, például amikor az öszszes hipotézis egyformán komplex. Az ML-tanulás jó közelítését adja a Bayes- és MAP-tanulásnak olyankor, amikor az adathalmaz nagy, hiszen az adatok végül is felülírják a hipotézisek a priori eloszlását, de kis adathalmazok esetén problémák merülnek fel az alkalmazásánál (mint látni fogjuk).</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id738968" href="#id738968" class="para">194</a>] </sup> A statisztikában jártasabb olvasó felismeri, hogy ez a példa valójában az <span class="strong"><strong>urna és golyó</strong></span> (<span class="strong"><strong>urn and ball</strong></span>) feladat változata. Úgy találtuk, hogy az urna és a golyó kevesebb kihívást jelent, mint a cukorka, továbbá a cukorka példa elvezet egy másik feladathoz – elcseréljük-e a zacskót egy barátunkkal, vagy sem (lásd 20.3. feladat).</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id739172" href="#id739172" class="para">195</a>] </sup> Korábbiakban leszögeztük, hogy a cukorkás zsákok nagyon nagyok, másképp az e.f.e. feltétel nem áll fenn. Korrektebb lenne (de kevésbé higiénikus) feltételezni, hogy minden vizsgálat, kóstolás után visszacsomagoljuk a cukorkát, és visszatesszük a zsákba.</p></div></div></div></body></html>
