<?xml version="1.0" encoding="UTF-8" standalone="no"?>

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Összefoglalás"><div class="titlepage"><div><div><h1 class="title"><a id="id719997"/>Összefoglalás</h1></div></div></div><p>A fejezet megmutatta, hogyan használjuk a világról meglévő tudásunkat döntések meghozatalára, még akkor is, amikor egy cselekvés kimenetelei bizonytalanok, és a cselekvésért járó jutalmat nem arathatjuk le csak több cselekvés után. A legfőbb pontok:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>A szekvenciális döntési problémákat bizonytalan környezet esetén, más néven <span class="strong"><strong>Markov döntési folyamat</strong></span>okat vagy <span class="strong"><strong>MDF</strong></span>-ket (<span class="strong"><strong>Markov decision process</strong></span>,<span class="strong"><strong> MDP</strong></span>), teljesen definiálja egy cselekvések valószínűségi kimenetelét meghatározó <span class="strong"><strong>állapotátmenet-modell</strong></span> (<span class="strong"><strong>transition model</strong></span>) és az egyes állapotokbeli jutalmakat meghatározó <span class="strong"><strong>jutalomfüggvény</strong></span> (<span class="strong"><strong>reward function</strong></span>). </p></li><li class="listitem"><p>Egy állapotsorozat hasznossága a sorozathoz tartozó összes jutalom összege, ami lehet az idővel leszámítolt. Egy MDF megoldása egy <span class="strong"><strong>eljárás</strong></span> (<span class="strong"><strong>policy</strong></span>), ami mindegyik ágens által elérhető állapothoz hozzárendel egy döntést. Egy optimális eljárás maximálja a végrehajtása során fellépő állapotsorozatok hasznosságát.</p></li><li class="listitem"><p>Egy állapot hasznossága az azon állapotsorozatok várható hasznossága, amelyek ebből az állapotból kezdődnek, és egy optimális eljárás végrehajtása esetén lépnek fel. Az MDF-k megoldására szolgáló <span class="strong"><strong>értékiteráció</strong></span> (<span class="strong"><strong>value iteration</strong></span>) algoritmusa a működése során az egyenleteket iteratívan oldja meg, az egyes állapotok hasznosságait a szomszédai hasznosságához kapcsolva.</p></li><li class="listitem"><p>Az <span class="strong"><strong>eljárásmód-iteráció</strong></span> (<span class="strong"><strong>policy iteration</strong></span>) felváltva kiszámolja az állapotoknak az aktuális eljáráshoz tartozó hasznosságát, majd az aktuális hasznosságokhoz képest javít az aktuális eljáráson.</p></li><li class="listitem"><p>A részlegesen megfigyelhető MDF-ek vagy RMMDF-ek sokkal nehezebben oldhatók meg, mint az MDF-ek. Megoldásuk során konvertáljuk őket egy olyan MDF-be, amit hiedelmi állapotok folytonos terében definiálunk. Az optimális viselkedés az RMMDF-ekben magában foglalja az információgyűjtést a bizonytalanság csökkentése céljából, és így a jövőbeli jobb döntések meghozatala miatt is.</p></li><li class="listitem"><p>Az RMMDF-környezetekre egy döntéselméleti ágens hozható létre. Az ágens egy <span class="strong"><strong>d</strong></span><span class="strong"><strong>inamikus döntési háló</strong></span>t (<span class="strong"><strong>dynamic decision network</strong></span>) használ az állapotátmenet-modell és megfigyelési modell ábrázolására, a hiedelmi állapotának frissítésére, illetve lehetséges cselekvési sorozatok előre elgondolásához.</p></li><li class="listitem"><p>A <span class="strong"><strong>játékelmélet</strong></span> (<span class="strong"><strong>game theory</strong></span>) olyan szituációkban írja le az ágensek racionális viselkedését, amelyekben több ágens egyidejű interakciója van jelen. A játékok megoldásai Nash-egyensúlyok – olyan stratégiaprofilok, amelyekben egyik ágensnek sincs indítéka a megadott stratégiától eltérni.</p></li><li class="listitem"><p>A <span class="strong"><strong>működési mód tervezés</strong></span> (<span class="strong"><strong>mechanism design</strong></span>) használható fel az ágensek interakcióját megadó olyan szabályok megállapítására, amelyek egy globális hasznosságot az egyéni racionális ágensek tevékenységén keresztül maximálnak. Néha léteznek olyan működésmódok, amelyek ezt a célt úgy érik el, hogy az egyes ágenseknek nem kell meggondolniuk a többi ágens által hozott döntéseket.</p></li></ul></div><p>Az MDF-ek és az RMMDF-ek világára vissza fogunk térni a 21. fejezetben, amikor a megerősítéses tanulás módszerét tanulmányozzuk, ami lehetővé teszi az ágens számára, hogy a tapasztalatai alapján javítsa a viselkedését szekvenciális, bizonytalan környezetekben.</p><div class="section" title="Irodalmi és történeti megjegyzések"><div class="titlepage"><div><div><h2 class="title"><a id="id720137"/>Irodalmi és történeti megjegyzések</h2></div></div></div><p>Richard Bellman kezdeményezte a szekvenciális döntési problémák modern megközelítési módját, és javasolta a dinamikus programozás módszerét általában és külön az értékiterációval kapcsolatban is (Bellman, 1957). Egy PhD-disszertációban Ron Howard vezette be az eljárásmód-iterációt és az átlagjutalom fogalmát végtelen horizontú problémák megoldására (Howard, 1960). Számos további eredményt Bellman és Dreyfus vezettek be (Bellman és Dreyfus, 1962). A módosított eljárásmód-iteráció van Nunentől, illetve Putermantól és Shintől származik (van Nunen, 1976; Puterman és Shin, 1978). Az aszinkron eljárásmód-iterációt Williams és Baird elemezték, tőlük származik a (17.9) egyenletbeli korlát is az eljárásmód-veszteségre (Williams és Baird, 1993). A leszámítolás technikáját stacionárius preferenciák segítségével megfogalmazva Koopmans dolgozta ki (Koopmans, 1972). A (Bertsekas, 1987; Puterman, 1994; Bertsekas és Tsitsiklis, 1996) könyvek precíz bevezetést adnak a szekvenciális döntési problémákhoz. Papadimitriou és Tsitsiklis az MDF-k számítási komplexitásáról közölnek eredményeket (Papadimitriou és Tsitsiklis, 1987).</p><p>Sutton és Watkins nagy hatású munkái, amelyek az MDF-ek megoldására szolgáló megerősítéses tanulási módszerekre irányultak, jelentős szerepet játszottak az MDF-ek megismertetésében az MI-kutatás területén (Sutton és Watkins, 1988), hasonlóan Barto és társainak későbbi áttekintéséhez (Barto és társai, 1995). (Werbos [1977] korábbi munkája számos hasonló ötletet tartalmazott, de nem vált ilyen széles körben elfogadottá.) Az első kapcsolatot az MDF-ek és a mesterséges intelligencia tervezési problémái között Sven Koenig teremtette meg (Koenig, 1991), aki megmutatta hogyan nyújtanak a valószínűségi Strips operátorok egy tömör ábrázolást az állapotátmenet-modellek számára. (Lásd még Wellman, 1990b.) Dean és munkatársai, illetve Tash és Russell munkáikban a nagy állapotterek miatti kombinatorikus problémákon kíséreltek meg úrrá lenni korlátozott keresési horizont és absztrakt állapotok felhasználásával (Dean és társai, 1993); Tash és Russell, 1994). Az információ értékén alapuló heurisztikák felhasználhatók olyan állapottérrészek kiválasztására, ahol a horizont lokális kiterjesztése a döntés minőségének jelentős javulását eredményezi. Ezt a megközelítést használva az ágensek az időkényszerekhez igazíthatják az erőfeszítéseiket, és érdekes viselkedést alakíthatnak ki, mint például a jól ismert „kitaposott utak” használatát az állapottérben való gyors útkeresésnél, elkerülve az optimális döntések újbóli kiszámítását az egyes pontokban. Boutilier és munkatársai mostani munkái propozicionális és elsőrendű kifejezéseken alapuló <span class="emphasis"><em>szimbolikus</em></span> reprezentációk dinamikus programozásbeli felhasználására összpontosultak, mind az állapotátmenet-modellek, mind az értékfüggvények esetében (Boutilier és társai, 2000; 2001).</p><p>Azt a megfigyelést, hogy a részlegesen megfigyelhető Markov döntési folyamatok átalakíthatók hagyományos Markov döntési folyamatokká hiedelmi állapotok felhasználásával, Astrom közli (Astrom, 1965). Az első teljes algoritmust a részlegesen megfigyelhető Markov döntési folyamatok egzakt megoldására Edward Sondik javasolta a PhD-disszertációjában (Sondik, 1971). (Egy későbbi folyóiratcikk [Smallwood és Sondik, 1973] tartalmaz néhány hibát, de jobban elérhető.) Az RMMDF-el kapcsolatos aktuális eredmények áttekintését adja Lovejoy (Lovejoy, 1991). Az első jelentős eredmény az MI területén a Witness-algoritmus volt (Cassandra és társai, 1994; Kaelbling és társai, 1998), ami egy javított változata az RMMDF értékiterációjának. Hamarosan következtek más algoritmusok, köztük egy Hansentól származó megközelítés, ami egy eljárásmódot fokozatosan hoz létre egy végesállapotú automata formájában (Hansen, 1998). Az eljárásmód ezen ábrázolásában a hiedelmi állapotok közvetlenül az automata konkrét állapotaihoz tartoznak. Közelítőleg optimális eljárásmódok az RMMDF-ekhez olyan eljárással hozhatók létre, ahol az előrefelé keresést a lehetséges megfigyelések és cselekvések kimeneteleinek mintavételezésével kombináljuk (Kearns és társai, 2000; Ng és Jordan, 2000). További, az RMMDF-ekre vonatkozó munkákat a 21. fejezetben tárgyalunk.</p><p>A dinamikus döntési hálókat használó ágensarchitektúra alapötleteit Dean és Kanazawa javasolták (Dean és Kanazawa, 1989a). Dean és Wellman könyve, a <span class="emphasis"><em>Planning and Control,</em></span> sokkal részletesebb kapcsolatot teremt a DBH/DDH-modellek és a szűrés klasszikus szabályozáselméletbeli szakirodalma között (Dean és Wellman, 1991). Tatman és Shachter mutatta meg, hogy hogyan alkalmazhatók dinamikus programozási algoritmusok DDH-modellekre (Tatman és Shachter, 1990). Russell különböző módszereket közöl ilyen ágensek felskálázására, és számos nyitott kutatási kérdést fogalmaz meg (Russell, 1998).</p><p>A játékelmélet kezdetei a 17. századra nyúlnak vissza, Christiaan Huygens és Gottfried Leibniz javaslataihoz, a versengő és együttműködő emberi interakciók tudományos és matematikai vizsgálatára. A 19. század folyamán számos vezető közgazdász alkotott egyszerű matematikai példákat, hogy a versengési helyzetek konkrét eseteit elemezze. Az első formális eredmények a játékelméletben Zermelótól származnak (Zermelo, 1913) (aki egy évvel korábban a minimax keresés egy – bár hibás – változatát javasolta játékokra). Emil Borel vezette be a kevert stratégia fogalmát (Borel, 1921). Neumann János bizonyította be, hogy minden kétszemélyes, zérusösszegű játéknak van egy maximin egyensúlya a kevert stratégiák körében és egy jól definiált értéke (Neumann, 1928). Neumann együttműködése a közgazdász Oskar Morgensternnel vezetett a <span class="emphasis"><em>Theory of Games and Economic Behavior</em></span> c. könyv megjelenéséhez 1944-ben, ami a játékelmélet meghatározó könyve. A könyv megjelenése a háború miatti papírhiány miatt késett, míg végül a Rockefeller család egy tagja személyesen támogatta a könyv kiadását.</p><p>John Nash, 1950-ben, 21 évesen jelentette meg elgondolásait az általános játékbeli egyensúlyi helyzetekkel kapcsolatban. Az egyensúlyi megoldás általa megadott definíciója, bár Cournot munkáján alapult (Cournot, 1838), úgy vált ismertté, mint Nash-egyensúly. Az 1959-től fellépő skizofréniája miatti hosszú szünet után Nash 1994-ben közgazdasági Nobel-díjat kapott (megosztva Reinhart Seltennel és Harsányi Jánossal).</p><p>A Bayes–Nash-egyensúlyt Harsányi közli (Harsányi, 1967) és Kadane és Larkey elemzi (Kadane és Larkey, 1982). A játékelmélet ágensek vezérlésében történő felhasználásának bizonyos kérdéseit Binmore tárgyalja (Binmore, 1982).</p><p>A fogolydilemmát mint tantermi példát Albert W. Tucker alkotta meg és Axelrod tárgyalja részletesen (Axelrod, 1985). Az ismétlődő játékokat Luce és Raiffa vezette be (Luce és Raiffa, 1957) mint részleges információjú játékokat (Kuhn, 1953).</p><p>Az első gyakorlati algoritmust részleges információjú játékokra az MI területén Koller és munkatársai fejlesztették ki (Koller és társai, 1996); Koller és Pfeffer cikke olvasmányos bevezetést ad a terület általános részeihez és egy működő rendszert ír le szekvenciális játékok ábrázolására és megoldására (Koller és Pfeffer, 1997). A játékelméletet és az MDF-eket a Markov-játékok elméletében egységesíti Littman (Littman, 1994). Shapley valójában már Bellman előtt közölte az értékiteráció algoritmusát (Shapley, 1953), de az eredményei nem váltak ismertté, talán amiatt, hogy nem a Markov-játékok keretei között voltak megfogalmazva. A játékelmélet általános tárgyalását tartalmazzák a (Myerson, 1991; Fudenberg és Tirole, 1991; Osborne és Rubinstein, 1994) művek. </p><p>A köztulajdon tragédiáját, ami működési mód tervezés egy motiváló problémája, Hardin mutatta be (Hardin, 1968). Hurwicz alkotta meg a működési mód tervezés matematikai alapjait (Hurwicz, 1973). Milgrom egy általa tervezett, akár több milliárd dolláros tételekre is szánt árverés működésmódot közöl (Milgrom, 1997). Az árverések a tervezésben (Hunsberger és Grosz, 2000) és az ütemezésben is felhasználhatók (Rassenti és társai, 1982). Varian rövid áttekintést ad a számításelméleti szakirodalomhoz kapcsolódó hivatkozásokkal együtt (Varian, 1995), a Rosenschein és Zlotkin szerzőpárostól pedig egy könyv terjedelmű tárgyalás jelent meg elosztott MI-alkalmazásokról (Rosenschein és Zlotkin, 1994). Az elosztott MI-ben más nevek alatt folynak kapcsolódó munkák, ideértve az együttes intelligenciát (Collective Intelligence), (Tumer és Wolpert, 2000) és a piacalapú szabályozást (market-based control) (Clearwater, 1996). Az árverések számítási vonatkozásairól gyakran az ACM Conferences on Electronic Commerce-en jelennek meg cikkek.</p></div><div class="section" title="Feladatok"><div class="titlepage"><div><div><h2 class="title"><a id="id720175"/>Feladatok</h2></div></div></div><p><span class="strong"><strong>17.1.	</strong></span></p><p>A 17.1. ábra által mutatott 4 × 3-as világ esetén számítsa ki mely négyzetek és milyen valószínűséggel érhetők el az [1, 1]-ből a [<span class="emphasis"><em>Fel</em></span>,<span class="emphasis"><em> Fel</em></span>,<span class="emphasis"><em> Jobbra</em></span>,<span class="emphasis"><em> Jobbra</em></span>,<span class="emphasis"><em> Jobbra</em></span>] cselekvéssorozattal. Magyarázza el, hogyan kapcsolódik ez a számítás egy rejtett Markov-modell előre elgondolásához („projecting”).</p><p><span class="strong"><strong>17.2.	</strong></span></p><p>Tegyük fel, hogy egy állapotsorozat hasznosságát a sorozat állapotaiban kapott jutalmak maximumaként definiáljuk. Mutassuk meg, hogy ez a hasznosságfüggvény nem eredményez stacionárius preferenciákat az állapotszekvenciák között. Lehetséges-e mégis olyan hasznosságfüggvényt definiálni az állapotokon, hogy a MVH-döntéshozatal optimális viselkedést adjon?</p><p><span class="strong"><strong>17.3.	</strong></span></p><p>Pontosan konvertálható-e egy véges keresési probléma Markov döntési problémává úgy, hogy az utóbbi optimális megoldása az előzőnek is optimális megoldása legyen? Ha igen<span class="emphasis"><em>, </em></span>magyarázza el<span class="emphasis"><em> </em></span>pontosan, hogyan konvertálja a problémát, és hogyan konvertálja a megoldást visszafelé; ha nem, akkor magyarázza el, hogy miért nem (azaz adjon ellenpéldát).</p><p><span class="strong"><strong>17.4.	</strong></span></p><p>Képzeljünk el egy leszámítolatlan MDF-et három állapottal, (1, 2, 3) és sorrendben a következő jutalmakkal –1, –2, 0. A 3-as állapot egy végállapot. Az 1-es és 2-es állapotokban két lehetséges cselekvés van: <span class="emphasis"><em>a</em></span> és <span class="emphasis"><em>b</em></span>. Az állapotátmenet-modell a következő:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Az 1-es állapotban az <span class="emphasis"><em>a</em></span> cselekvés az ágenst a 2-es állapotba mozgatja 0,8 valószínűséggel, míg 0,2 valószínűséggel helyben hagyja.</p></li><li class="listitem"><p>Az 2-es állapotban az <span class="emphasis"><em>a</em></span> cselekvés az ágenst az 1-es állapotba mozgatja 0,8 valószínűséggel, és 0,2 valószínűséggel helyben hagyja.</p></li><li class="listitem"><p>Mind az 1-es, mind a 2-es állapotban a <span class="emphasis"><em>b</em></span> cselekvés az ágenst a 3-as állapotba mozgatja 0,1 valószínűséggel, és 0,9 valószínűséggel helyben hagyja.</p></li></ul></div><p class="Tartalom3">Válaszolja meg a következő kérdéseket:</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p>Mi<span class="emphasis"><em> </em></span>határozható meg <span class="emphasis"><em>kvalitatívan </em></span>az optimális eljárásmódról az 1-es és a 2-es állapotokban?</p></li><li class="listitem"><p>Alkalmazzon eljárásmód-iterációt az optimális eljárásmód, illetve az 1-es és a 2-es állapotok értékének a meghatározására. Minden lépést teljesen illusztráljon. Tegyük fel, hogy a kezdeti eljárásmód mindkét állapotban a <span class="emphasis"><em>b</em></span> cselekvés.</p></li><li class="listitem"><p>Mi történik az eljárásmód-iterációval, ha a kezdeti eljárásmód mindkét állapotban az <span class="emphasis"><em>a</em></span> cselekvés? Segít a leszámítolás? Függ-e az optimális eljárásmód a leszámítolási tényezőtől?</p></li></ol></div><p><span class="strong"><strong>17.5.	</strong></span></p><p>Alkalmanként az MDF-ek megadása tartalmaz egy <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>,<span class="emphasis"><em> a</em></span>) jutalomfüggvényt, ami a véghezvitt cselekvéstől függ, vagy egy <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>,<span class="emphasis"><em> a</em></span>,<span class="emphasis"><em> s</em></span>′) jutalomfüggvényt, ami még a kimeneteli állapottól is függ.</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p>Írja fel a Bellman-egyenleteket ezt a formalizmust használva.</p></li><li class="listitem"><p>Mutassa meg, hogyan lehet egy <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>,<span class="emphasis"><em> a</em></span>,<span class="emphasis"><em> s</em></span>′) jutalomfüggvényt tartalmazó MDF-et átalakítani egy másik, <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>,<span class="emphasis"><em> a</em></span>) jutalomfüggvényt tartalmazó MDF-be úgy, hogy az optimális eljárásmódok az új MDF-ben pontosan megfeleljenek az eredeti MDF-beli optimális eljárásmódoknak.</p></li><li class="listitem"><p>Most ugyanígy konvertálja az <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>,<span class="emphasis"><em> a</em></span>)-t tartalmazó MDF-eket <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>)-t tartalmazó MDF-ekbe.</p></li></ol></div><p><span class="strong"><strong>17.6.	</strong></span></p><div class="informalexample"><p/><p>Tekintsük a 17.1. ábrán látható 4 × 3-as világot.</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p>Valósítson meg egy olyan környezetszimulátort erre a környezetre, amelyben a környezet földrajzi sajátosságai könnyen megváltoztathatók. Ehhez egy bizonyos forráskód már elérhető a könyv forráskódtárában a világhálón.</p></li><li class="listitem"><p>Hozzon létre egy eljárásmód-iterációt használó ágenst, és mérje meg a teljesítményét a környezetszimulátorban különböző kiindulási állapotokból. Végezzen nagyszámú kísérletet mindegyik kiindulási állapotból, és hasonlítsa össze a futásonként kapott összjutalmak átlagát az állapot hasznosságával (amit a létrehozott algoritmus határozott meg).</p></li><li class="listitem"><p>Kísérletezzen a környezet méretének a megnövelésével. Hogyan változik az eljárásmód-iteráció futási ideje a környezet méretének a változásával?</p></li></ol></div><p/></div><p><span class="strong"><strong>17.7.	</strong></span></p><div class="informalexample"><p/><p>A 17.1. ábrán mutatott környezetre keresse meg az <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>)<span class="emphasis"><em> </em></span>függvényre az összes küszöbértéket, azaz azokat az értékeket, amelyek átlépésekor az optimális eljárásmód megváltozik. Ehhez egy módszer szükséges az optimális eljárásmód meghatározásához és egy rögzített <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>)<span class="emphasis"><em> </em></span>esetén az értékének a kiszámításához. (<span class="emphasis"><em>Segítség</em></span>: bizonyítsa be, hogy az érték minden rögzített eljárásmódnál <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>)-sel lineárisan változik.)</p><p/></div><p><span class="strong"><strong>17.8.	</strong></span></p><p>Ebben a feladatban kétszereplős MDF-ekkel foglalkozunk, amelyek a zérusösszegű, többfordulós játékokhoz tartoznak, mint amilyenek a 6. fejezetben szerepelnek. Legyen a két játékos <span class="emphasis"><em>A</em></span> és <span class="emphasis"><em>B</em></span>, és legyen <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>) az <span class="emphasis"><em>A</em></span> játékos jutalma az <span class="emphasis"><em>s </em></span>állapotban. (<span class="emphasis"><em>B </em></span>jutalma mindig egyenlő és ellentétes előjelű.)</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p>Legyen <span class="emphasis"><em>U<sub>A</sub></em></span>(<span class="emphasis"><em>s</em></span>) az <span class="emphasis"><em>s</em></span> állapot hasznossága, amikor <span class="emphasis"><em>A</em></span> lépése következik <span class="emphasis"><em>s</em></span>-ben, és legyen <span class="emphasis"><em>U<sub>B</sub></em></span>(<span class="emphasis"><em>s</em></span>) az <span class="emphasis"><em>s</em></span> állapot hasznossága, amikor <span class="emphasis"><em>B</em></span> következik <span class="emphasis"><em>s</em></span>-ben. Az összes jutalom és hasznosság <span class="emphasis"><em>A</em></span> szempontjából van megfogalmazva (pontosan úgy, ahogyan a minimax játékfában). Írja fel az <span class="emphasis"><em>U<sub>A</sub></em></span>(<span class="emphasis"><em>s</em></span>)-t és az <span class="emphasis"><em>U<sub>B</sub></em></span>(<span class="emphasis"><em>s</em></span>)-t definiáló Bellman-egyenleteket.</p></li><li class="listitem"><p>Magyarázza el, hogyan végezhet kétszemélyes értékiterációt ezekkel az egyenletekkel, és adjon meg egy alkalmas leállási kritériumot.</p></li><li class="listitem"><p>Tekintsük a 6.14. ábrán látható játékot a <a class="xref" href="ch06s08.md#ID_241_oldal">„Feladatok”</a> részben. Rajzolja fel az állapotteret (a játékfa helyett), folytonos vonallal mutatva <span class="emphasis"><em>A</em></span> mozgásait és szaggatott vonallal <span class="emphasis"><em>B</em></span> mozgásait. Jegyezze fel minden állapothoz az <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>) értékét. Hasznos lehet az (<span class="emphasis"><em>s<sub>A</sub></em></span>, <span class="emphasis"><em>s<sub>B</sub></em></span>) állapotok elrendezése egy kétdimenziós rácson, <span class="emphasis"><em>s<sub>A</sub></em></span>-t és <span class="emphasis"><em>s<sub>B</sub></em></span>-t „koordinátaként” használva.</p></li><li class="listitem"><p>Most alkalmazza a kétszemélyes értékiterációt a játék megoldására, és származtassa az optimális eljárásmódot.</p></li></ol></div><p><span class="strong"><strong>17.9.	</strong></span></p><p>Mutassuk meg, hogy egy domináns stratégiai egyensúly egy Nash-egyensúly, de ez visszafelé nem igaz.</p><p><span class="strong"><strong>17.10.	</strong></span></p><p>A kő-papír-olló gyerekjátékban mindegyik játékos egyszerre felmutatja a választását, ami kő, papír vagy olló. A papír becsomagolja a követ, a kő kicsorbítja az ollót, az olló elvágja a papírt. A kiterjesztett kő-papír-olló-tűz-víz változatban a tűz üti a követ, papírt és ollót, a kő, papír és olló üti a vizet, és a víz üti a tüzet. Írja fel a jutalmazási mátrixot, és keressen egy kevert stratégiájú megoldást a játékhoz.</p><p><span class="strong"><strong>17.11.	</strong></span></p><p>Oldja meg a háromujjas snóblijátékot.</p><p><span class="strong"><strong>17.12.	</strong></span></p><p>A Nemzeti Jégkorong Szövetségben (National Hockey League) 1999 előtt a csapatok 2 pontot kaptak a győzelemért, 1-et a döntetlenért és 0-t, ha veszítettek. Ez zérusösszegű játék? 1999-ben a szabályokat úgy módosították, hogy egy csapat 1 pontot kap, ha hosszabbításban kap ki. A győztes csapat továbbra is 2 pontot kap. Hogyan változtatja meg ez a módosítás a fenti kérdésekre a válaszokat? Ha törvényes lenne, mikor volna racionális a két csapatnak titokban megegyezni, hogy a rendes játékidőben döntetlent érnek el, és majd a hosszabbításban döntik el a küzdelmet? Tegyük fel, hogy mindegyik csapat számára a kapott pontszámok jelentik a hasznosságot, és hogy létezik egy kölcsönösen ismert <span class="emphasis"><em>p </em></span>a priori valószínűség, hogy az első csapat hosszabbításban fog nyerni. Milyen <span class="emphasis"><em>p</em></span> értékekre értene egyet mindkét csapat ezzel az egyezséggel? </p><p><span class="strong"><strong>17.13.	</strong></span></p><p>A következő jutalommátrix Blindertől származik, amit Bernstein közöl (Blinder, 1983; Bernstein, 1996), a politikusok és a jegybank (Federal Reserve) közötti játékot ábrázolja.</p><p><span class="inlinemediaobject"><img src="kepek/745-1.png" alt="Feladatok"/></span></p><p>A politikusok lazíthatják vagy szigoríthatják a pénzügyi politikát, míg a jegybank lazíthatja vagy szigoríthatja a monetáris politikát. (És természetesen mindkét oldal választhatja azt, hogy nem tesz semmit.) Mindegyik oldalnak vannak preferenciái, hogy kinek mit kell tennie – egyik oldal sem szeretne rossznak látszani. A mutatott jutalmak egyszerűen sorrendi rangok: 9 az első választásért és végül 1 az utolsóért. Keresse meg a játék Nash-egyensúlyát a tiszta stratégiák között. Ez Pareto-optimális megoldás? Az olvasó esetleg érdemesnek tartja elemezni a jelenlegi kormány politikáját ennek fényében.</p></div></div></body></html>
