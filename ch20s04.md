<?xml version="1.0" encoding="UTF-8" standalone="no"?>

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Példányalapú tanulás"><div class="titlepage"><div><div><h1 class="title"><a id="id742536"/>Példányalapú tanulás</h1></div></div></div><p>Eddigiekben a statisztikai tanulás tárgyalásában arra koncentráltunk, hogy valószínűségi modellek egy <span class="emphasis"><em>behatárolt</em></span> családjának paramétereit egy <span class="emphasis"><em>struktúrájában nem kötött</em></span> adathalmazra illesszük. Például a kevert Gaussokat használó, nem ellenőrzött tanulás feltételezi, hogy az adatok megmagyarázhatók <span class="emphasis"><em>rögzített</em></span> számú <span class="emphasis"><em>Gauss-eloszlás</em></span> <span class="emphasis"><em>összegeként</em></span>. Az ilyen módszereket <span class="strong"><strong>paraméteres tanulás</strong></span>nak (<span class="strong"><strong>parametric learning</strong></span>) nevezzük. A paraméteres tanuló eljárások gyakran nagyon egyszerűek és hatékonyak, de egy meghatározott behatárolt modellcsalád feltételezése sokszor túlzott egyszerűsítése annak, ami az adatokat szolgáltató valós világban történik. Nos, az természetesen igaz, hogy ha nagyon kevés adatunk van, akkor nem reménykedhetünk egy bonyolult és részletes modell megtanulásában, de butaságnak tűnik a hipotézis komplexitását akkor is rögzítve tartani, amikor az adathalmaz nagyon nagyra nő.</p><p>A paraméteres módszerekkel ellentétben a <span class="strong"><strong>nemparaméteres tanulás</strong></span>i (<span class="strong"><strong>nonparametric learning</strong></span>) módszerek lehetővé teszik, hogy a hipotézis bonyolultsága együtt növekedjék az adatokkal. Minél több adatunk van, annál körmönfontabb lehet a hipotézis. Két, nagyon egyszerű nemparaméteres <span class="strong"><strong>példányalapú tanulás</strong></span>t használó módszercsaládot fogunk vizsgálni, amelyeket azért nevezünk így, mert közvetlenül a tanító példányokból hoznak létre hipotéziseket.</p><div class="section" title="Legközelebbi-szomszéd modellek"><div class="titlepage"><div><div><h2 class="title"><a id="id742587"/>Legközelebbi-szomszéd modellek</h2></div></div></div><p>A <span class="strong"><strong>legközelebbi-szomszéd</strong></span> (<span class="strong"><strong>nearest-neighbor</strong></span>) modellek alapötlete az, hogy bármely egyedi <span class="strong"><strong>x</strong></span> bemeneti pont tulajdonságai valószínűleg hasonlók az <span class="strong"><strong>x</strong></span> pont környezetébe eső pontok tulajdonságaihoz. Például ha <span class="strong"><strong>sűrűségbecslés</strong></span>t (<span class="strong"><strong>density estimation) </strong></span>kívánunk végezni – azaz meg akarjuk becsülni egy ismeretlen valószínűség sűrűségfüggvény-értékét <span class="strong"><strong>x</strong></span>-ben –, akkor egyszerűen mérhetjük azt, hogy <span class="strong"><strong>x</strong></span> környezete milyen sűrűn van beszórva pontokkal. Ez nagyon egyszerűen hangzik, amíg rá nem jövünk, hogy meg kell határoznunk, mit is értünk „szomszédság” alatt. Ha a szomszédság túl kicsi, akkor egyáltalán nem fog pontokat tartalmazni. Ha túl nagy, akkor esetleg az <span class="emphasis"><em>összes</em></span> adatpontot tartalmazni fogja, és olyan sűrűséget eredményez, ami mindenütt ugyanakkora. Egy megoldási lehetőség, ha a szomszédságot úgy definiáljuk, hogy legyen éppen elég tág ahhoz, hogy <span class="emphasis"><em>k</em></span> pontot tartalmazzon, ahol <span class="emphasis"><em>k</em></span> elég nagy ahhoz, hogy ésszerű becslést végezhessünk. Rögzített <span class="emphasis"><em>k</em></span> esetén a szomszédság mérete változó, ahol ritkán vannak az adatok, ott a szomszédság tág térrész, ahol az adatok sűrűn helyezkednek el, ott a szomszédság szűk. A 20.12. (a) ábra egy példát mutat: két dimenzióban szétszórt adatpontokat. A 20.13. ábra mutatja az ezen adatokra elvégzett <span class="emphasis"><em>k</em></span>-legközelebbi-szomszéd sűrűségbecslést <span class="emphasis"><em>k = </em></span>3, 10 és 40 esetére. <span class="emphasis"><em>k = </em></span>3 esetében a sűrűségbecslést minden egyes pontban csak 3 szomszédos pontra alapozzuk, és ezért erősen változó eredményt kapunk. <span class="emphasis"><em>k = </em></span>10 esetén a 20.12. (b) ábrán látható valódi sűrűség jó becslését kapjuk. <span class="emphasis"><em>k = </em></span>40 esetén a szomszédság túl tággá válik, és az adatok struktúráját mindenestől elvesztjük. Gyakorlatban – kis dimenziószám esetén – egy 5 és 10 közé eső <span class="emphasis"><em>k</em></span> a legtöbb esetben jó eredményt ad. Keresztvalidáció alkalmazásával is jó értéket kaphatunk a <span class="emphasis"><em>k</em></span>-ra.</p><div class="figure"><a id="id742671"/><p class="title"><strong>20.12. ábra - (a) A 20.8. (a) ábra adatainak 128 pontos részhalmaza, két kiválasztott ponttal, és 10 legközelebbi szomszédjukkal. (b) Az adatok előállításánál használt kevert Gauss-eloszlás 3D ábrája.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-12.png" alt="(a) A 20.8. (a) ábra adatainak 128 pontos részhalmaza, két kiválasztott ponttal, és 10 legközelebbi szomszédjukkal. (b) Az adatok előállításánál használt kevert Gauss-eloszlás 3D ábrája."/></div></div></div><div class="figure"><a id="id742681"/><p class="title"><strong>20.13. ábra - A 20.12. (a) ábrán látható adatokra végzett sűrűségbecslés, <span class="emphasis"><em>k = </em></span>3 (a); <span class="emphasis"><em>k = </em></span>10 (b) és <span class="emphasis"><em>k = </em></span>40 (c) esetén</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-13.png" alt="A 20.12. (a) ábrán látható adatokra végzett sűrűségbecslés, k = 3 (a); k = 10 (b) és k = 40 (c) esetén"/></div></div></div><p>Ahhoz, hogy egy kérdéses pontban a legközelebbi szomszédokat meg tudjuk határozni, egy távolságmetrikára – <span class="emphasis"><em>D</em></span>(<span class="strong"><strong>x</strong></span><sub>1</sub>, <span class="strong"><strong>x</strong></span><sub>2</sub>)-re – van szükségünk. A 20.12. ábra kétdimenziós példájában euklideszi távolságot használtunk. Ez nem felel meg, ha a tér minden dimenziójában valami mást mérünk – például magasságot és súlyt –, mivel egy dimenzió skálájának megváltoztatása megváltoztatja a legközelebbi szomszédok halmazát. Az egyik lehetőség, hogy minden egyes dimenzió skáláját normalizáljuk. Ehhez először megmérjük az összes tulajdonság varianciáját a teljes adathalmazon, majd a tulajdonságértéket úgy fejezzük ki, hogy megadjuk: a tulajdonságérték az adott tulajdonság varianciájának hányszorosa. (Ez a <span class="strong"><strong>Mahalanobis-távolság</strong></span> [<span class="strong"><strong>Mahalanobis distance</strong></span>] – amely a tulajdonságok kovarianciamátrixát veszi figyelembe – speciális esete.) Végül diszkrét esetben alkalmazhatjuk a <span class="strong"><strong>Hamming-távolság</strong></span>ot (<span class="strong"><strong>Hamming distance</strong></span>), amely úgy definiálja <span class="emphasis"><em>D</em></span>(<span class="strong"><strong>x</strong></span><sub>1</sub>, <span class="strong"><strong>x</strong></span><sub>2</sub>)-t, mint azon tulajdonságok számát, amelyben <span class="strong"><strong>x</strong></span><sub>1</sub> és <span class="strong"><strong>x</strong></span><sub>2</sub> eltér.</p><p>A 20.13. ábrán láthatóhoz hasonló sűrűségbecslések a bemeneti tér feletti együttes eloszlásokat határoznak meg. A Bayes-hálóktól eltérően a példányalapú reprezentációk nem tartalmaznak rejtett változókat, ami azt jelenti, hogy nem alkalmazhatunk nem ellenőrzött osztályozást, mint ahogy a kevert Gauss-modellnél tettük. Továbbra is alkalmazhatjuk a kívánt <span class="emphasis"><em>y</em></span> értéknek az <span class="strong"><strong>x</strong></span> bemeneti tulajdonságok alapján történő jóslására a sűrűségbecslést, ha kiszámítjuk <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>y</em></span>|<span class="strong"><strong>x</strong></span>) = <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>y, </em></span><span class="strong"><strong>x</strong></span>)/<span class="emphasis"><em>P</em></span>(<span class="strong"><strong>x</strong></span>)-et, feltéve, hogy a tanító adatok a kívánt értékre nézve is tartalmaznak értékeket.</p><p>A legközelebbi-szomszéd tanulási algoritmus használható közvetlen ellenőrzött tanulásra is. Ha adott egy tesztpélda <span class="strong"><strong>x</strong></span> bemeneti értékekkel, akkor az <span class="emphasis"><em>y = h</em></span>(<span class="strong"><strong>x</strong></span>) az <span class="strong"><strong>x k</strong></span>-legközelebbi-szomszédjának <span class="emphasis"><em>y</em></span> értékeiből nyerhető. Diszkrét esetben többségi szavazással kaphatunk egyetlen jósolt értéket. Folytonos esetben átlagolhatjuk a <span class="emphasis"><em>k</em></span> darab értéket, vagy lokális lineáris regressziót végezhetünk a <span class="emphasis"><em>k</em></span> pontra illesztve, ez utóbbi esetben a jóslást a kialakuló hipersík <span class="strong"><strong>x</strong></span>-beli értéke adja.</p><p>A <span class="emphasis"><em>k</em></span>-legközelebbi-szomszéd tanulási algoritmus nagyon egyszerűen megvalósítható, kevés hangolást igényel, és gyakran elég jól működik. Jó dolog, ha először egy új tanulási problémán próbáljuk ki. Mindazonáltal nagy adathalmazokban hatékony mechanizmust kell találnunk arra, hogy a kérdéses <span class="strong"><strong>x</strong></span> pont legközelebbi szomszédjait megtaláljuk, hiszen túl hosszú lenne minden pontra kiszámítani a távolságot. Egy sor szellemes, a tanító adatok előfeldolgozásán alapuló módszert javasoltak ennek a lépésnek a hatékonynyá tételére. Szerencsétlen módon a legtöbb ilyen módszer nem viselkedik jól a tér dimenziójának növekedésével (azaz a tulajdonságok számának növekedésével).</p><p>A sokdimenziós terek még egy további problémát jelentenek, nevezetesen azt, hogy egy ilyen térben a legközelebbi szomszédok rendszerint nagyon messze vannak. Vegyünk egy <span class="emphasis"><em>N</em></span> elemű, <span class="emphasis"><em>d</em></span> dimenziós egységkockában elhelyezkedő adathalmazt, és tegyük fel, hogy a szomszédságok <span class="emphasis"><em>b</em></span> oldalú hiperkockák, amelyek térfogata <span class="emphasis"><em>b<sup>d</sup></em></span>. (Ugyanez a megfontolás működne hipergömbök feltételezése esetén is, de a hipergömb térfogatképlete bonyolultabb.) Ahhoz, hogy <span class="emphasis"><em>k</em></span> pontot tartalmazzon, az átlagos szomszédság a teljes térfogat – amit egységnyinek tekintünk – <span class="emphasis"><em>k</em></span>/<span class="emphasis"><em>N</em></span>-ed részét kell elfoglalnia. Ennek megfelelően <span class="emphasis"><em>b<sup>d </sup></em></span> = <span class="emphasis"><em>k</em></span>/<span class="emphasis"><em>N</em></span>, vagyis <span class="emphasis"><em>b = </em></span>(<span class="emphasis"><em>k</em></span>/<span class="emphasis"><em>N</em></span>)<sup>1/<span class="emphasis"><em>d</em></span></sup>. Eddig jó. Legyen most <span class="emphasis"><em>d = </em></span>100, <span class="emphasis"><em>k = </em></span>10 és <span class="emphasis"><em>N = </em></span>1 000 000 értékű. Akkor <span class="emphasis"><em>b </em></span>≈<span class="emphasis"><em> </em></span>0,89 – azaz a szomszédság majdnem az egész bemeneti teret elfoglalja! Ez arra utal, hogy sokdimenziós esetben a legközelebbi-szomszéd alapú módszerek nem megbízhatók. Alacsony dimenziószám esetén nincs probléma, <span class="emphasis"><em>d = </em></span>2 esetén <span class="emphasis"><em>b = </em></span>0,003.</p></div><div class="section" title="Kernelmódszerek"><div class="titlepage"><div><div><h2 class="title"><a id="id742950"/>Kernelmódszerek</h2></div></div></div><p>A <span class="strong"><strong>kernelmodell</strong></span>ben (<span class="strong"><strong>kernel model) </strong></span>úgy tekintünk minden tanító példányra, mintha egy kis saját sűrűségfüggvényt – <span class="strong"><strong>kernelfüggvény</strong></span>t (<span class="strong"><strong>kernel function) </strong></span>– generálna. Az eredő sűrűségfüggvény becslés nem más, mint a kis kernelfüggvények súlyozott összege. Egy <span class="strong"><strong>x</strong></span><sub>i</sub> tanító példány egy <span class="emphasis"><em>K</em></span>(<span class="strong"><strong>x</strong></span>, <span class="strong"><strong>x</strong></span><sub>i</sub>) kernelfüggvényt generál, amely a tér minden <span class="strong"><strong>x</strong></span> pontjához egy valószínűséget rendel. Így a sűrűségbecslés:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0028.gif" alt="Kernelmódszerek"/></span></p><p>Normális esetben a kernelfüggvény csak az <span class="strong"><strong>x</strong></span> és <span class="strong"><strong>x</strong></span><sub>i</sub> közötti <span class="emphasis"><em>D</em></span>(<span class="strong"><strong>x</strong></span>,<span class="strong"><strong> x</strong></span><sub>i</sub>) távolságtól függ. A legnépszerűbb kernelfüggvény (természetesen) a Gauss-függvény. Az egyszerűség kedvéért egy gömbszimmetrikus Gauss-függvényt feltételezünk, amelynek minden tengely mentén <span class="emphasis"><em>w</em></span> a szórása, azaz:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0029.gif" alt="Kernelmódszerek"/></span></p><p>ahol <span class="emphasis"><em>d</em></span> az <span class="strong"><strong>x</strong></span> dimenziószáma. Hátra van még a megfelelő <span class="emphasis"><em>w</em></span> érték választásának problémája. Éppúgy, mint az előzőkben egy túl kis szomszédság nagyon hepehupássá teszi a becslést – mint a 20.14. (a) ábrán látható. A (b) ábrán látható, hogy egy közepes <span class="emphasis"><em>w</em></span> érték nagyon jó becslést ad. A (c) ábrán bemutattuk, hogy a túl nagy szomszédság eredményeképp teljesen elveszítjük a struktúrát. Jó <span class="emphasis"><em>w</em></span> értéket választhatunk keresztvalidációval.</p><div class="figure"><a id="id743074"/><p class="title"><strong>20.14. ábra - A 20.12. (a) ábrán látható adatok sűrűségbecslése Gauss-kernelekkel, <span class="emphasis"><em>w = </em></span>0,02 (a); <span class="emphasis"><em>w = </em></span>0,07 (b) és <span class="emphasis"><em>w = </em></span>0,20 (c) esetén.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-14.png" alt="A 20.12. (a) ábrán látható adatok sűrűségbecslése Gauss-kernelekkel, w = 0,02 (a); w = 0,07 (b) és w = 0,20 (c) esetén."/></div></div></div><p>Ellenőrzött tanulást kernelekkel úgy végezhetünk, hogy a tanító példányokból származtatott <span class="emphasis"><em>összes</em></span> jóslás <span class="emphasis"><em>súlyozott</em></span> kombinációját vesszük. (Összehasonlítva a <span class="emphasis"><em>k</em></span>-legközelebbi-szomszéd módszerrel, ott a <span class="emphasis"><em>k</em></span>-legközelebbi-példány súlyozatlan kombinációját használjuk.) Az <span class="emphasis"><em>i</em></span>-edik példánynak a kérdéses <span class="strong"><strong>x</strong></span> pontra vett súlyát a <span class="emphasis"><em>K</em></span>(<span class="strong"><strong>x</strong></span>,<span class="strong"><strong> x</strong></span><sub>i</sub>) kernelfüggvényérték adja. Diszkrét jóslás esetén súlyozott szavazást vehetünk, folytonos esetben súlyozott átlagot vagy súlyozott lineáris regressziót. Figyeljük meg, hogy a kernelekkel végzett jóslás azt igényli, hogy az <span class="emphasis"><em>összes</em></span> tanító példányt figyelembe kell vennünk. A kernelek összekombinálhatók a legközelebbi-szomszéd indexelési sémáival azért, hogy csupán a szomszédos példányok alapján előállított súlyozott jóslást alkalmazzuk.</p></div></div></body></html>
