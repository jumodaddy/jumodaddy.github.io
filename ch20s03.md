<?xml version="1.0" encoding="UTF-8" standalone="no"?>

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Rejtett változókkal történő tanulás: az EM algoritmus"><div class="titlepage"><div><div><h1 class="title"><a id="id741433"/>Rejtett változókkal történő tanulás: az EM algoritmus</h1></div></div></div><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Az előző rész a teljesen megfigyelhető esettel foglalkozott. Számos valós problémában vannak <span class="strong"><strong>rejtett változó</strong></span>k (<span class="strong"><strong>hidden variable</strong></span>s), amelyeket néha <span class="strong"><strong>latens változó</strong></span>knak (<span class="strong"><strong>latent variable</strong></span>s) is hívnak. Ezek nem figyelhetők meg a tanulás céljára rendelkezésre álló adatokban. Például az orvosi feljegyzések gyakran tartalmazzák a megfigyelt tüneteket, az alkalmazott kezelést, esetleg e kezelés eredményét, de ritkán tartalmazzák magának a betegségnek a közvetlen megfigyelését.<sup>[<a id="id741459" href="#ftn.id741459" class="footnote">199</a>]</sup> Megkérdezhetnénk, „ha a betegséget magát nem figyelhetjük meg, akkor miért nem állítunk fel egy modellt nélküle? A választ a 20.7. ábrán tüntettük fel, amely szívbetegségek egy kis, fiktív modelljét mutatja. Három hajlamosító tényező és három megfigyelhető tünet van (amelyek túl nyomasztók ahhoz, hogy megnevezzük őket). Tegyük fel, hogy mindegyik háromféle értéket vehet fel (pl. <span class="emphasis"><em>nincs</em></span>, <span class="emphasis"><em>közepes</em></span>, <span class="emphasis"><em>súlyos</em></span>). A rejtett változó eltávolítása az (a) hálóból a (b) hálóhoz vezet; a paraméterek száma 78-ról 708-ra nő. Tehát <span class="emphasis"><em>a rejtett változók drámaian csökkenthetik a Bayes-háló megadásához szükséges paraméterek számát</em></span>. Ez viszont drámaian csökkentheti a paraméterek megtanulásához szükséges adatok mennyiségét.</p></div><div class="figure"><a id="id741478"/><p class="title"><strong>20.7. ábra - (a) A rejtett változónak feltételezett szívbetegség egyszerű diagnosztikai hálója. Minden egyes változónak három lehetséges értéke van, a változókat a feltételes eloszlásukban szereplő független változók számával címkéztük, a végső számuk 78. (b) A <span class="emphasis"><em>SzívBetegség </em></span>eltávolításával<span class="emphasis"><em> </em></span>készült ekvivalens háló. Vegyük észre, hogy a tünetváltozók most már nem feltételesen függetlenek, ha a szülőket megadjuk. Ez a háló 708 paramétert igényel.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-07.png" alt="(a) A rejtett változónak feltételezett szívbetegség egyszerű diagnosztikai hálója. Minden egyes változónak három lehetséges értéke van, a változókat a feltételes eloszlásukban szereplő független változók számával címkéztük, a végső számuk 78. (b) A SzívBetegség eltávolításával készült ekvivalens háló. Vegyük észre, hogy a tünetváltozók most már nem feltételesen függetlenek, ha a szülőket megadjuk. Ez a háló 708 paramétert igényel."/></div></div></div><p>A rejtett változók fontosak, de bonyolítják a tanulási problémát. A 20.7. (a) ábrán például nem világos, hogy adott szülőcsomópontok esetén hogyan tanulható meg a <span class="emphasis"><em>SzívBetegség</em></span> feltételes eloszlása, mivel nem ismert az értéke az egyes esetekben. Ugyanez a probléma fellép a tünetek eloszlásának tanulásánál is. Ebben a részben egy olyan – <span class="strong"><strong>expectation–maximization</strong></span>,<sup>[<a id="id741506" href="#ftn.id741506" class="footnote">200</a>]</sup> <span class="strong"><strong>EM</strong></span> nevű – algoritmust ismertetünk, amely nagyon általános megoldást ad erre a problémára. Három példát mutatunk be, majd általános leírást adunk. Ez az algoritmus először holmi mágiának tűnik, de amint a mögöttes intuíciót megértettük, a tanulási problémák hatalmas területén egy sor helyen alkalmazhatjuk.</p><div class="section" title="Nem ellenőrzött osztályozás: Gauss-eloszlások keverékének tanulása"><div class="titlepage"><div><div><h2 class="title"><a id="id741525"/>Nem ellenőrzött osztályozás: Gauss-eloszlások keverékének tanulása</h2></div></div></div><p>A <span class="strong"><strong>nem ellenőrzött osztályozás</strong></span> vagy <span class="strong"><strong>klaszterezés</strong></span> (<span class="strong"><strong>unsupervised clustering</strong></span>) az a probléma, amikor objektumok valamilyen gyűjteményében több kategóriát különböztetünk meg. A probléma nem ellenőrzött, mivel a kategóriacímkék nincsenek megadva. Tegyük fel például, hogy felvesszük százezer csillag spektrumát. Az a kérdés, hogy a spektrumok alapján vannak-e különböző <span class="emphasis"><em>típusú</em></span> csillagok, és ha igen, akkor hányféle, és mik a jellemzőik? Mindannyian jól ismerünk olyan neveket, mint „vörös óriás” és „fehér törpe”, de a csillagoknak nincs ilyen címke a kalapjukon – a csillagászoknak nem ellenőrzött osztályozást kell végezniük ahhoz, hogy ezeket a kategóriákat felállítsák. Más példák lehetnek: a fajok, nemek, rendek stb. azonosítása a Linné-féle taxonómiában vagy hétköznapi tárgyak természetes kategóriáinak megalkotása (lásd 10. fejezet).</p><p>A nem ellenőrzött osztályozás az adatokkal kezdődik. A 20.8. (a) ábra 500 adatpontot ábrázol, mindegyik két folytonos attribútum értékét adja meg. Az adatpontok megfelelhetnek pl. csillagoknak, a két attribútum pedig pl. a spektrumvonalak erőssége két adott frekvencián. A következő lépés, hogy meg kell értenünk, milyen valószínűségi eloszlás generálhatta az adatokat. Az osztályozás azt feltételezi, hogy az adatok a <span class="emphasis"><em>P</em></span> <span class="strong"><strong>kevert eloszlás</strong></span>ból (<span class="strong"><strong>mixture distribution</strong></span>) származnak. Egy ilyen eloszlás <span class="emphasis"><em>k</em></span> <span class="strong"><strong>komponens</strong></span>ből (<span class="strong"><strong>component</strong></span>) áll, amelyek mindegyike önmagában egy-egy eloszlás. Az adatpont úgy jön létre, hogy először kiválasztjuk az egyik komponenst, majd ebből a komponensből generálunk egy mintát. Jelöljük a <span class="emphasis"><em>C</em></span> véletlen változóval a komponenseket, amely változó az 1, …, <span class="emphasis"><em>k</em></span> értékeket veheti fel. A kevert eloszlást ekkor a</p><p><span class="inlinemediaobject"><img src="math/mi-20-0015.gif" alt="Nem ellenőrzött osztályozás: Gauss-eloszlások keverékének tanulása"/></span></p><p>összefüggés írja le, ahol <span class="strong"><strong>x</strong></span> jelenti az adatpont attribútumait. Folytonos adatok esetén a komponensekre természetesen adódik a többváltozós Gauss-eloszlás, ami az úgynevezett <span class="strong"><strong>kevert Gauss</strong></span>- (<span class="strong"><strong>mixture of Gaussians</strong></span>) eloszlás családot adja. A kevert Gauss-eloszlás paraméterei: <span class="emphasis"><em>w<sub>i</sub> =</em></span> <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>C =</em></span> <span class="emphasis"><em>i</em></span>) (az egyes komponensek súlya), <span class="emphasis"><em><span class="strong"><strong>μ</strong></span><sub>i</sub> </em></span>(az egyes komponensek várható értéke) <span class="strong"><strong>Σ</strong></span><sub>i</sub> (az egyes komponensek kovarianciája). A 20.8. (b) ábra három Gauss-eloszlás keverékét mutatja, valójában ez volt az (a) ábrán látható adathalmaz forrása.</p><p>Ez esetben a nem ellenőrzött osztályozási probléma abban áll, hogy a 20.8. (a) ábrán látható nyers adatokból kell visszaállítani a 20.8. (b) ábrán láthatóhoz hasonló kevert modellt. Nyilvánvaló, hogy ha <span class="emphasis"><em>tudnánk,</em></span> melyik adatot melyik komponens generálta, akkor könnyű lenne visszaállítanunk az egyes Gauss-komponenseket. Egyszerűen kiválaszthatnánk az egyes komponensekhez tartozó összes adatpontot, majd alkalmazhatnánk a (20.4) egyenletet (tulajdonképpen annak többváltozós verzióját) a Gauss-paramétereknek az adatokhoz való illesztésére. Másrészről, ha <span class="emphasis"><em>tudnánk</em></span> az egyes komponensek paramétereit, akkor – legalábbis valószínűségi értelemben – meg tudnánk adni, hogy melyik adatpont melyik eloszláshoz tartozik. A probléma az, hogy sem az adatpontok komponensekhez rendelését, sem a komponensek paramétereit nem ismerjük.</p><div class="figure"><a id="id741656"/><p class="title"><strong>20.8. ábra - (a) 500 kétdimenziós adatpont, amely három osztály jelenlétét sugallja. (b) Egy három-komponensű kevert Gauss-modell, a súlyok (balról jobbra) 0,2, 0,3 és 0,5. Az (a) ábrán látható adatokat ebből a modellből generáltuk. (c) Az adatokból az EM algoritmussal rekonstruált modell</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-08.png" alt="(a) 500 kétdimenziós adatpont, amely három osztály jelenlétét sugallja. (b) Egy három-komponensű kevert Gauss-modell, a súlyok (balról jobbra) 0,2, 0,3 és 0,5. Az (a) ábrán látható adatokat ebből a modellből generáltuk. (c) Az adatokból az EM algoritmussal rekonstruált modell"/></div></div></div><p>Az EM algoritmus alapötlete ebben az összefüggésben az, hogy <span class="emphasis"><em>úgy teszünk, mintha</em></span> ismernénk a modell paramétereit, majd kiszámítjuk minden egyes pont minden egyes komponenshez való tartozásának valószínűségét. Ezek után újraillesztjük a komponenseket az adatokhoz: mindegyik komponenst a teljes adathalmazhoz illesztjük, az egyes adatpontokat az adott komponenshez tartozás valószínűségével súlyozva. Az eljárást a konvergencia elértéig folytatjuk. Lényegében „teljessé tesszük” az adatokat azzal, hogy a rejtett változók – melyik adat melyik komponenshez tartozik – valószínűségi eloszlását kikövetkeztetjük a pillanatnyilag használt modell segítségével. A kevert Gauss-eloszlás esetén a paramétereknek tetszőleges módon kezdeti értéket adunk, majd a következő két lépést ismételjük:</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p>E-lépés: Számítsuk ki a <span class="emphasis"><em>p<sub>ij</sub></em></span>  = <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>C</em></span> = <span class="emphasis"><em>i</em></span>|<span class="strong"><strong>x</strong></span><sub>j</sub> valószínűségeket, amelyek azt fejezik ki, hogy milyen valószínűséggel generálta az <span class="emphasis"><em>i</em></span>-edik komponens <span class="strong"><strong>x</strong></span><sub>j</sub>-t. A Bayes-szabály alapján <span class="emphasis"><em>p<sub>ij</sub></em></span> = <span class="emphasis"><em>αP</em></span>(<span class="strong"><strong>x</strong></span><sub>j</sub>|<span class="emphasis"><em>C</em></span> = <span class="emphasis"><em>i</em></span>) <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>C</em></span> = <span class="emphasis"><em>i</em></span>). A <span class="emphasis"><em>P</em></span>(<span class="strong"><strong>x</strong></span><sub>j</sub>|<span class="emphasis"><em>C</em></span> = <span class="emphasis"><em>i</em></span>) tényező az <span class="strong"><strong>x</strong></span><sub>j </sub>valószínűsége az <span class="emphasis"><em>i</em></span>-edik Gauss-eloszlásban, míg <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>C</em></span> = <span class="emphasis"><em>i</em></span>) az <span class="emphasis"><em>i</em></span>-edik Gauss-eloszlás súlya. Használjuk a <span class="inlinemediaobject"><img src="math/mi-20-0016.gif" alt="(a) 500 kétdimenziós adatpont, amely három osztály jelenlétét sugallja. (b) Egy három-komponensű kevert Gauss-modell, a súlyok (balról jobbra) 0,2, 0,3 és 0,5. Az (a) ábrán látható adatokat ebből a modellből generáltuk. (c) Az adatokból az EM algoritmussal rekonstruált modell"/></span><span class="emphasis"><em> </em></span>definíciót.</p></li><li class="listitem"><p>M-lépés: Számítsuk ki az új átlag, szórás és komponens súly paramétereket a következők szerint:</p></li></ol></div><p><span class="inlinemediaobject"><img src="math/mi-20-0017.gif" alt="(a) 500 kétdimenziós adatpont, amely három osztály jelenlétét sugallja. (b) Egy három-komponensű kevert Gauss-modell, a súlyok (balról jobbra) 0,2, 0,3 és 0,5. Az (a) ábrán látható adatokat ebből a modellből generáltuk. (c) Az adatokból az EM algoritmussal rekonstruált modell"/></span></p><p>Az E-lépés vagy <span class="emphasis"><em>expectation</em></span> (várhatóérték-képzés) lépés úgy fogható fel, mintha a rejtett <span class="strong"><strong>indikátor változó</strong></span> (<span class="strong"><strong>indicator variable</strong></span>) <span class="emphasis"><em>Z<sub>ij</sub></em></span> valószínűségének <span class="emphasis"><em>p<sub>ij</sub></em></span> várható értékét (<span class="emphasis"><em>expected value</em></span>) számítanánk ki. Ha az <span class="emphasis"><em>i</em></span>-edik komponens generálta <span class="strong"><strong>x</strong></span><sub>j</sub>-t, akkor <span class="emphasis"><em>Z<sub>ij</sub></em></span> értéke 1, különben pedig 0. Az M-lépés vagy <span class="emphasis"><em>maximization</em></span> (maximumkeresés) lépés a rejtett változók adott várható értéke mellett megkeresi azokat a paramétereket, amelyek maximálják az adatokra felállított log likelihood függvényt. </p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>A végső modellt – amelyet az EM segítségével a 20.8. (a) ábra adatain tanultunk – a 20.8. (c) ábra mutatja. Látszólag megkülönböztethetetlen attól az eredeti modelltől, amellyel az adatokat generáltuk. A 20.9. (a) ábra az adatoknak a pillanatnyi modellből számított log likelihood értékét mutatja az EM algoritmus futása során. Két pontra hívjuk fel a figyelmet. Először is, a végső modellre a log likelihood némileg meghaladja az arra a valós modellre számított értéket, amelyekkel az adatokat generáltuk. Ez meglepőnek tűnhet, de egyszerűen azt mutatja, hogy az adatokat véletlenszerűen generáltuk, és nem pontosan tükrözik a mögöttes modellt. A második, hogy <span class="emphasis"><em>az EM minden iterációs lépésben növeli az adatok log likelihood értékét</em></span>. Ez általánosságban is bizonyítható. Továbbá bizonyos feltételek mellett belátható, hogy az EM biztosan eléri a likelihood egy lokális maximumát. (Ritka esetekben elérhet egy nyeregpontba, vagy akár egy lokális minimumba is.) Ebben az értelemben az EM egy gradiensalapú hegymászó algoritmusra emlékeztet, de vegyük észre, hogy nincs „lépésméret” paramétere!</p></div><div class="figure"><a id="id741877"/><p class="title"><strong>20.9. ábra - Az <span class="emphasis"><em>L</em></span> log likelihood értéknek – az EM iterációszámának függvényében való – alakulását bemutató görbék. A vízszintes vonal a helyes modellnek megfelelő log likelihood értéket mutatja. (a) A 20.8. ábrán látható kevert Gauss-modellre vonatkozó görbe. (b) A 20.10. (a) ábrán látható Bayes-hálóra vonatkozó görbe.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-09.png" alt="Az L log likelihood értéknek – az EM iterációszámának függvényében való – alakulását bemutató görbék. A vízszintes vonal a helyes modellnek megfelelő log likelihood értéket mutatja. (a) A 20.8. ábrán látható kevert Gauss-modellre vonatkozó görbe. (b) A 20.10. (a) ábrán látható Bayes-hálóra vonatkozó görbe."/></div></div></div><p class="Tartalom3">A dolgok nem mennek mindig olyan jól, mint ahogy a 20.9. (a) ábra esetleg sugallja. Előfordulhat, hogy az egyik Gauss-komponens úgy összezsugorodik, hogy csak egyetlen adatpontot fed le. Ebben az esetben a varianciája nullához tart, a belőle számított likelihood pedig végtelenhez! Másik probléma lehet, hogy két komponens „összeolvad”, azonos átlagot és varianciát vesz fel, az adatpontokon pedig osztoznak. Az ilyen típusú degenerált lokális maximumok súlyos problémát jelentenek, elsősorban sokdimenziós terekben. Ennek a problémának egyik megoldása lehet megfelelő priorok állítása a modell paramétereire és az EM algoritmus MAP-változatának alkalmazása. Másik lehetőség a komponens újraindítása új paraméterekkel, ha túl kicsivé válik vagy túl közel kerül más komponensekhez. A paraméterek ésszerű kezdeti értékei szintén segítenek a probléma megoldásában.</p></div><div class="section" title="Rejtett változókkal felépített Bayes-hálók tanulása"><div class="titlepage"><div><div><h2 class="title"><a id="id741894"/>Rejtett változókkal felépített Bayes-hálók tanulása</h2></div></div></div><p>A rejtett változókkal felépített Bayes-hálók tanulásánál ugyanazokat a megközelítéseket alkalmazzuk, amelyek jól működtek a kevert Gauss-eloszlásokra. A 20.10. ábra azt a helyzetet mutatja, amikor két zsák cukorkát összekevertek. A cukrokat három tulajdonság írja le, az <span class="emphasis"><em>Íz</em></span> és a <span class="emphasis"><em>Csomagolás</em></span> mellett néhánynál megjelenik egy <span class="emphasis"><em>Lyuk</em></span> a cukorka közepén, másoknál pedig nincs lyuk.</p><p>A cukorkák eloszlását minden egyes zsákban egy <span class="strong"><strong>naiv Bayes</strong></span>-modell írja le; a tulajdonságok adott zsák esetén függetlenek, de mindegyik tulajdonság feltételes valószínűség eloszlása függ a zsáktól. A következő paramétereink vannak: <span class="emphasis"><em>θ</em></span> annak az a priori valószínűsége, hogy a cukorka az 1. zsákból származik; <span class="emphasis"><em>θ</em></span><sub><span class="emphasis"><em>I</em></span>1</sub>, illetve <span class="emphasis"><em>θ</em></span><sub><span class="emphasis"><em>I</em></span>2</sub> annak valószínűsége, hogy a cukorka meggyízű, feltéve, hogy az 1., illetve a 2. zsákból származik; <span class="emphasis"><em>θ</em></span><sub><span class="emphasis"><em>Cs</em></span>1</sub>, illetve <span class="emphasis"><em>θ</em></span><sub><span class="emphasis"><em>Cs</em></span>2</sub> azokat a valószínűségeket jelölik, hogy a csomagolás piros; <span class="emphasis"><em>θ</em></span><sub><span class="emphasis"><em>Ly</em></span>1</sub>, illetve <span class="emphasis"><em>θ</em></span><sub><span class="emphasis"><em>Ly</em></span>2</sub> pedig azokat a valószínűségeket, hogy a cukorka lyukas. Vegyük észre, hogy az egész modell egy kevert modell. (Valójában a kevert Gauss-eloszlásokat is modellezhetjük Bayes-hálóval, ahogy azt a 20.10. (b) ábra mutatja.) Az ábrán a zsák egy rejtett változó, mivel attól a pillanattól, hogy összekevertük a zsákok tartalmát, többet nem tudjuk megmondani egy cukorkáról, hogy melyik zsákból származott. Vissza tudjuk ebben az esetben nyerni a zsákok leírását, ha csupán az összekevert cukorkákat tudjuk megfigyelni?</p><div class="figure"><a id="id741972"/><p class="title"><strong>20.10. ábra - (a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az <span class="strong">X</span> megfigyelhető változó átlaga és kovarianciája a <span class="emphasis"><em>C</em></span> komponenstől függ.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-10.png" alt="(a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az X megfigyelhető változó átlaga és kovarianciája a C komponenstől függ."/></div></div></div><p>Vigyük végig az EM-iterációt erre a problémára. Először nézzük az adatokat. 1000 mintát generáltunk a következő paraméterekkel rendelkező (valódi) modell alapján:</p><p><code class="code"><em><span class="remark">θ = 0,5, θ<sub>I</sub></span></em><sub>1</sub><em><span class="remark"> = θ<sub>Cs</sub></span></em><sub>1</sub><em><span class="remark"> = θ<sub>Ly</sub></span></em><sub>1</sub> = 0,8, <em><span class="remark">θ</span></em><sub><em><span class="remark">I</span></em>2</sub> = <em><span class="remark">θ</span></em><sub><em><span class="remark">Cs</span></em>2</sub> = <em><span class="remark">θ</span></em><sub><em><span class="remark">Ly</span></em>2 </sub>= 0,3						(20.7)</code></p><a id="ID_836_oldal"/><p>Azaz a cukorkák egyforma valószínűséggel származhatnak mindkét zsákból, az első zsák nagyobbrészt meggyízű cukrokat tartalmaz piros csomagolásban, és sok köztük a lyukas, a második inkább nem lyukas citromízűeket, zöld csomagolásban. A lehetséges nyolcféle cukor előfordulási számai a következők:</p><p><span class="inlinemediaobject"><img src="kepek/836-1.png" alt="(a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az X megfigyelhető változó átlaga és kovarianciája a C komponenstől függ."/></span></p><p>A paraméterek alapértékre állításával kezdünk. A könnyű számíthatóság kedvéért a következő értékeket választjuk:<sup>[<a id="id742057" href="#ftn.id742057" class="footnote">201</a>]</sup></p><p><span class="inlinemediaobject"><img src="math/mi-20-0018.gif" alt="(a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az X megfigyelhető változó átlaga és kovarianciája a C komponenstől függ."/></span></p><p>Először nézzük a <span class="emphasis"><em>θ</em></span> paramétert. A teljesen megfigyelhető esetben ezt közvetlenül az 1. zsákból, illetve 2. zsákból származó cukorkák <span class="emphasis"><em>megfigyelt</em></span> számaiból becsülnénk. Mivel a zsák rejtett változó, ehelyett a <span class="emphasis"><em>várható</em></span> számértékekkel számolunk. A várható cukorkaszám <span class="inlinemediaobject"><img src="math/mi-20-0019.gif" alt="(a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az X megfigyelhető változó átlaga és kovarianciája a C komponenstől függ."/></span> annak valószínűsége, az összes cukorkára összegezve, hogy a cukorka az 1. zsákból származik:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0020.gif" alt="(a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az X megfigyelhető változó átlaga és kovarianciája a C komponenstől függ."/></span></p><p>Ezek a valószínűségek a Bayes-hálókra kifejlesztett tetszőleges következtetési algoritmussal számíthatók. Naiv Bayes-hálókra, mint amilyen példánkban is szerepel, a következtetés „kézzel” elvégezhető, a Bayes-szabály és a feltételes függetlenség felhasználásával:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0021.gif" alt="(a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az X megfigyelhető változó átlaga és kovarianciája a C komponenstől függ."/></span></p><p>(Figyeljük meg, hogy a normalizáló konstans szintén a paraméterektől függ.) Alkalmazva ezt a képletet, mondjuk, a 273 pirosan csomagolt meggyízű cukorkára, amelyekben lyuk van, a következőt kapjuk:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0022.gif" alt="(a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az X megfigyelhető változó átlaga és kovarianciája a C komponenstől függ."/></span></p><p>A számok táblázatában található másik hét cukorkafajtával folytatva, θ<sup> (1)</sup> = 0,6124-et kapunk.</p><p>Nézzük a további paramétereket, mint pl. <span class="emphasis"><em>θ</em></span><sub><span class="emphasis"><em>I</em></span>1</sub>-et. A teljesen megfigyelhető esetben ezt közvetlenül a <span class="emphasis"><em>megfigyelt,</em></span> 1. zsákból származó meggy- és citromízű cukorkák számából becsülnénk. Az 1. zsákból származó meggycukorkák <span class="emphasis"><em>várható</em></span> száma:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0023.gif" alt="(a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az X megfigyelhető változó átlaga és kovarianciája a C komponenstől függ."/></span></p><p>Ezek a valószínűségek ismét bármely Bayes-háló algoritmussal kiszámíthatók. Befejezve ezt az eljárást, megkapjuk az összes paraméter új értékét:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0024.gif" alt="(a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az X megfigyelhető változó átlaga és kovarianciája a C komponenstől függ."/></span></p><p>Az adatok log likelihood értéke a kezdeti kb. –2044-ről az első iteráció során kb. –2021-re nő, ahogy a 20.9. (b) ábrán látható. Eszerint a frissítés magát a likelihoodot kb. <span class="emphasis"><em>e</em></span><sup>23</sup> ≈ 10<sup>10</sup> szorzófaktorral növelte. A tizedik iterációra a megtanult modell jobban illeszkedik, mint a valódi (<span class="emphasis"><em>L</em></span> = –1982,214). Ezek után a javulás nagyon lassúvá válik. Ez nem szokatlan az EM algoritmusnál, ezért sok gyakorlatban használt rendszer a tanulás végső fázisában gradiensalapú algoritmusokkal – mint pl. a Newton–Raphson-módszer (lásd 4. fejezet) – kombinálja az EM algoritmust.</p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Az ebből a példából levonható általános tanulság az, hogy <span class="emphasis"><em>a rejtett változókkal történő Bayes-háló tanulásparamétereinek frissítése közvetlenül rendelkezésünkre áll az egyes példákon történő következtetés eredményeképpen. Ráadásul csak</em></span> lokális <span class="emphasis"><em>a posteriori valószínűségekre van szükségünk az egyes paraméterekhez.</em></span> Az általános esetben, amikor az egyes <span class="emphasis"><em>X<sub>i </sub></em></span>változók feltételes valószínűségi paramétereit – azaz a <span class="emphasis"><em>θ<sub>ijk</sub> = P</em></span>(X<span class="emphasis"><em><sub>i</sub> = </em></span>X<sub>ij</sub>|<span class="emphasis"><em>Pa<sub>i</sub> = pa<sub>ik</sub></em></span>) értékeket – tanuljuk, adott szülőcsomópontok esetén, a frissítést a normalizált várható számértékek adják:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0025.gif" alt="(a) A cukorkák kevert modellje. A különböző ízű cukorkák, a csomagolások, valamint a lyukas cukrok aránya a zsáktól függ, amelyet nem figyelünk meg. (b) Egy kevert Gaussra vonatkozó Bayes-háló. Az X megfigyelhető változó átlaga és kovarianciája a C komponenstől függ."/></span></p></div><p>A várható számértékek a példák összegzésével nyerhetők, kiszámítva a <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>X<sub>i </sub></em></span>= x<sub>ij</sub>, <span class="emphasis"><em>Pa<sub>i </sub></em></span>= <span class="emphasis"><em>pa<sub>ik</sub></em></span>) valószínűségeket egy tetszőleges Bayes-háló következtetési algoritmussal. A pontos algoritmusok esetén – beleértve a változó eliminációt is – ezek a valószínűségek mind a következtetés melléktermékeként közvetlenül nyerhetők, a tanuláshoz nincs szükség semmilyen speciális extra számításra. Sőt minden egyes paraméter tanulásához szükséges információ <span class="emphasis"><em>lokálisan</em></span> rendelkezésre áll.</p></div><div class="section" title="Rejtett Markov-modellek tanulása"><div class="titlepage"><div><div><h2 class="title"><a id="id742263"/>Rejtett Markov-modellek tanulása</h2></div></div></div><p>Az EM algoritmus utolsó, itt bemutatott alkalmazása a rejtett Markov-modellek (RMM) (hidden Markov model, HMM) állapotátmenet-valószínűségeinek tanulása. Idézzük fel a 15. fejezetből, hogy a rejtett Markov-modellek olyan dinamikus Bayes-hálóval reprezentálhatók, amelyeknek egyetlen, diszkrét állapotváltozója van, amint ez a 20.11. ábrán is látható. Minden egyes adatpont egy véges hosszúságú megfigyelés<span class="emphasis"><em>sorozatnak</em></span> felel meg, tehát a megoldandó probléma az, hogy megfigyelési sorozatok halmaza (vagy esetleg egyetlen hosszú sorozat) alapján állapotátmenet-valószínűségeket kell megtanulnunk.</p><p class="Tartalom3">Már kimunkáltuk, hogy mi módon tudunk Bayes-hálókat megtanulni, de van egy nehézség: a Bayes-hálóknál minden egyes paraméter elkülönül, a rejtett Markov-modellben viszont időben <span class="emphasis"><em>ismétlődnek</em></span> az egyes <span class="emphasis"><em>θ<sub>ijt</sub></em></span> = <span class="emphasis"><em>P</em></span>(X<sub><span class="emphasis"><em>t</em></span>+1</sub> = <span class="emphasis"><em>j</em></span>|X<sub>t</sub> = <span class="emphasis"><em>i</em></span>) állapotátmenet-valószínűségek (annak valószínűsége, hogy a <span class="emphasis"><em>t</em></span> időpillanatban az <span class="emphasis"><em>i</em></span>-edik állapotból a <span class="emphasis"><em>j</em></span>-edikbe jutunk), azaz <span class="emphasis"><em>θ<sub>ijt</sub></em></span> = <span class="emphasis"><em>θ<sub>ij</sub></em></span> minden <span class="emphasis"><em>t</em></span>-re. Ahhoz, hogy megbecsüljük az <span class="emphasis"><em>i</em></span>-edik állapotból a <span class="emphasis"><em>j</em></span>-edik állapotba való átmenet valószínűségét, egyszerűen kiszámítjuk azon esetek várható arányát, amikor a rendszer az <span class="emphasis"><em>i</em></span>-edik állapotban van és a <span class="emphasis"><em>j</em></span>-edik állapotba megy:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0026.gif" alt="Rejtett Markov-modellek tanulása"/></span></p><div class="figure"><a id="id742357"/><p class="title"><strong>20.11. ábra - Rejtett Markov-modellt reprezentáló kiterített dinamikus Bayes-háló (a 15.14. ábra megismétlése)</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-11.png" alt="Rejtett Markov-modellt reprezentáló kiterített dinamikus Bayes-háló (a 15.14. ábra megismétlése)"/></div></div></div><p>Ismét tetszőleges RMM következtetési algoritmust használhatunk a várható számértékek kiszámítására. A 15.4. ábrán bemutatott <span class="strong"><strong>előre-hátra</strong></span> (<span class="strong"><strong>forward-backward</strong></span>) algoritmus könnyen módosítható úgy, hogy kiszámítsa a szükséges valószínűségeket. Egy fontos szempont, hogy azokra a valószínűségekre van szükségünk, amelyeket inkább <span class="strong"><strong>simítás</strong></span>sal (<span class="strong"><strong>smoothing</strong></span>) nyerünk, nem pedig <span class="strong"><strong>szűrés</strong></span>sel (<span class="strong"><strong>filtering</strong></span>): azaz nekünk utólagosan kell értékelnünk a bizonyítékokat az adott állapotátmenet bekövetkezési valószínűségének becslésekor. Mint a 15. fejezetben mondtuk, a gyilkosság bizonyítékát általában a gyilkosság bekövetkezte (azaz az <span class="emphasis"><em>i</em></span> állapotból a <span class="emphasis"><em>j</em></span> állapotba való átmenetet) <span class="emphasis"><em>után</em></span> nyerjük.</p></div><div class="section" title="Az EM algoritmus általános alakja"><div class="titlepage"><div><div><h2 class="title"><a id="id742410"/>Az EM algoritmus általános alakja</h2></div></div></div><p>Számos példát láttunk az EM algoritmusra. Mindegyik tartalmazta azt a lépést, hogy kiszámítottuk a rejtett változók várható értékét minden példára, majd ezeket a várható értékeket úgy használtuk, mintha megfigyelt értékek lettek volna, és segítségükkel újraszámoltuk a paramétereket. Legyen <span class="strong"><strong>x</strong></span> az összes példában az összes megfigyelt érték, jelölje <span class="strong"><strong>Z</strong></span> az összes példában az összes rejtett változót, és legyen <span class="emphasis"><em><span class="strong"><strong>θ</strong></span></em></span> a valószínűségi modell összes paramétere. Ekkor az EM algoritmus:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0027.gif" alt="Az EM algoritmus általános alakja"/></span></p><p>Az EM algoritmus dióhéjban ez az egyenlet. Az E-lépés az összegzés kiszámítása, ami a „teljessé tett” adatokra, a log likelihoodnak a <span class="emphasis"><em>P</em></span>(<span class="strong"><strong>Z</strong></span> = <span class="strong"><strong>z</strong></span>|<span class="strong"><strong>x</strong></span>, <span class="emphasis"><em><span class="strong"><strong>θ</strong></span></em></span><sup>(<span class="emphasis"><em>i</em></span>)</sup>) eloszlás szerint számított várható értéke. Ez a valószínűség a rejtett változóknak az adatok felhasználásával nyert a posteriori valószínűsége. Az M-lépés viszont nem más, mint ennek a log likelihoodnak a maximalizálása a paraméterek szerint. Kevert Gauss-jelekre a rejtett változók a <span class="emphasis"><em>Z<sub>ij</sub></em></span>-k, ahol <span class="emphasis"><em>Z<sub>ij </sub></em></span>= 1, ha a <span class="emphasis"><em>j</em></span>-edik mintát az <span class="emphasis"><em>i</em></span>-edik komponens generálta. Bayes-hálóknál az egyes példák nem megfigyelt változó értékei a rejtett változók. RMM esetben az <span class="emphasis"><em>i </em></span>→<span class="emphasis"><em> j</em></span> átmenetek a rejtett változók. Ha azonosítottuk a megfelelő rejtett változókat az adott alkalmazásban, akkor az általános formából kiindulva levezethető az EM algoritmus.</p><p>Amint megértettük az általános elgondolást, amin az EM alapul, az összes variáns és javítás könnyen levezethető. Például: sok esetben az E-lépés – a rejtett változók posteriorjainak kiszámítása – kezelhetetlen problémát okoz, mint például a nagy Bayes-hálókban. Kiderült, hogy alkalmazhatunk egy <span class="emphasis"><em>közelítő</em></span> E-lépést, és még ezzel is hatékony algoritmust kapunk. Egy mintavételi algoritmussal – mint amilyen például az MCMC (lásd 14.5. alfejezet) – a tanulási folyamat nagyon szemléletes: minden egyes, az MCMC által bejárt állapotot (a rejtett és megfigyelt változók konfigurációját) úgy kezelünk, mintha teljes megfigyelés volna. Ennek megfelelően a paraméterek minden egyes MCMC-átmenet után frissíthetők. A közelítő következtetés más formái – mint például a variációs és hurkos módszerek – szintén hatékonynak bizonyultak nagyon nagy hálók tanulásánál.</p></div><div class="section" title="Bayes-hálóstruktúra tanulása rejtett változók esetén"><div class="titlepage"><div><div><h2 class="title"><a id="id742504"/>Bayes-hálóstruktúra tanulása rejtett változók esetén</h2></div></div></div><p>A 20.2. alfejezetben a Bayes-hálóstruktúra tanulásának problémáját tárgyaltuk teljes adatok esetén. Amikor rejtett változókat is figyelembe veszünk, a dolgok nehezebbé válnak. A legegyszerűbb esetben a rejtett változók a megfigyeltekkel együtt fel vannak sorolva, bár az értékük nem ismert, de a tanuló algoritmus tud a létezésükről, és helyet kell nekik találjon a hálóstruktúrában. Például egy algoritmus tanulhatja úgy a 20.7. (a) ábrán látható struktúrát, hogy megadjuk neki, hogy egy <span class="emphasis"><em>SzívBetegség</em></span> nevű (háromértékű) változót be kell építsen a  struktúrába. Ha a tanuló algoritmus nem kapja meg ezt az információt, akkor két lehetőség van: vagy úgy viselkedik, mintha az adatok teljesek volnának (ami arra kényszeríti, hogy a 20.7. (b) sokparaméteres modellt tanulja meg), vagy <span class="emphasis"><em>kitalál</em></span> új rejtett változókat a modell egyszerűsítése érdekében. Ez utóbbi megközelítés megvalósítható úgy, hogy új módosítási lehetőségeket vezetünk be a struktúrakeresésbe: a kapcsolatok módosításán túl az algoritmus hozzáadhat vagy törölhet egy rejtett változót, illetve megváltoztathatja argumentumszámát. Természetesen az algoritmus nem fogja tudni, hogy a bevezetett új változó neve <span class="emphasis"><em>SzívBetegség</em></span>; hasonlóképpen nem tud értelmes nevet adni az értékeinek. Szerencsére általában a bevezetett új rejtett változó előzetesen már létező változókkal van kapcsolatban, így egy szakember rendszerint meg tudja vizsgálni az új változót is tartalmazó lokális feltételes eloszlásokat, és meg tudja állapítani a jelentését.</p><p>Éppúgy, mint a teljes adatokra vonatkozó esetben, egy tisztán maximum-likelihood struktúratanulás teljesen összekötött hálót eredményez (ráadásul rejtett változók nélkülit), így használnunk kell a komplexitásbüntetés valamilyen formáját. Használhatjuk az MCMC-t is a Bayes-tanulás approximációjára. Például: ismeretlen komponensszámú kevert Gauss-jelek tanulásakor túlmintavételezhetjük a számot; ekkor a közelítő Gauss-jelek számának közelítő a posteriori eloszlását az MCMC-folyamat mintavételi frekvenciája adja.</p><p>Az eddigiekben az általunk vizsgált eljárásnak volt egy külső hurka, amely a struktúrakeresés folyamata, és egy belső hurka, ami egy paraméteres optimalizálási eljárás. A teljes adatokra vonatkozó esetben a belső hurok nagyon gyors – csupán ki kell nyerni az adathalmazból a feltételes gyakoriságokat. Ha rejtett változók is vannak, akkor a belső hurok az EM algoritmussal történő számos iterációt vagy egy gradiensalapú algoritmust is tartalmazhat. Minden iterációs ciklusban ki kell számítani a Bayes-háló a posteriori eloszlásait, ami önmagában NP-teljes probléma. Ez a megközelítés bonyolult modellek tanulása esetén napjainkig nem bizonyult a gyakorlatban is alkalmazhatónak. Egy lehetséges előrelépés a <span class="strong"><strong>strukturális EM</strong></span> (<span class="strong"><strong>structural EM</strong></span>), ami nagyon hasonlóan működik, mint a normál (parametrikus) EM, leszámítva azt, hogy nemcsak a paramétereket, hanem a struktúrát is frissíti. Éppúgy, mint ahogy a normál EM algoritmus arra használja az aktuális paramétereket, hogy a várható számértékeket meghatározza az E-lépés során, majd alkalmazza ezeket a számértékeket az M-lépésben új paraméterek választására, a strukturális EM az aktuális struktúrát használja a várható számértékek meghatározására, majd arra alkalmazza ezeket a számértékeket az M-lépésben, hogy a potenciális új struktúrák likelihoodját meghatározza. (Ellentétben a külső hurok/belső hurok módszerrel, amelynél minden potenciális struktúrára kiszámítjuk a várható számértékeket.) Ezen az úton a strukturális EM számos strukturális változtatást tud végrehajtani a hálón anélkül, hogy egyszer is újraszámolná a várható számértékeket. Így a módszer képessé válik nemtriviális Bayes-hálóstruktúrák tanulására. Mindamellett sok munkát kell még befektetni, míg azt mondhatjuk majd, hogy a struktúratanulás problémája megoldott.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.id741459" href="#id741459" class="para">199</a>] </sup> Néhány feljegyzés tartalmazza az orvos által felállított diagnózist, de ez a szimptómák oksági következménye, amit viszont a betegség okozott.</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id741506" href="#id741506" class="para">200</a>] </sup> Az eljárást magyarul <span class="strong"><strong>várhatóérték-képzés–maximalizás</strong></span> eljárásnak nevezhetnénk, de a magyar szakirodalomban is az angol elnevezés terjedt el. (<span class="emphasis"><em>A szerk.</em></span>)</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id742057" href="#id742057" class="para">201</a>] </sup> A gyakorlatban jobb véletlen értékeket választani, hogy elkerüljük a szimmetria által okozott lokális maximumokat.</p></div></div></div></body></html>
