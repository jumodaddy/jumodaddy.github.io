<?xml version="1.0" encoding="UTF-8" standalone="no"?>

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Miért működik a tanulás: a tanulás számítási elmélete"><div class="titlepage"><div><div><h1 class="title"><a id="id726946"/>Miért működik a tanulás: a tanulás számítási elmélete</h1></div></div></div><p class="Tartalom3">A legfontosabb megválaszolatlan kérdés, amelyet a 18.2. alfejezetben tettünk fel, a következő: hogyan bizonyosodhat meg valaki arról, hogy a tanulási algoritmusa által létrehozott elmélet helyesen fogja megjósolni a jövőt? Formálisan: honnan tudjuk, hogy a <span class="emphasis"><em>h</em></span> hipotézis jól közelíti az <span class="emphasis"><em>f</em></span> keresett függvényt, ha nem ismerjük <span class="emphasis"><em>f</em></span>-et? Évszázadok óta foglalkoznak ezekkel a kérdésekkel. Amíg nem találjuk meg a válaszokat, addig a gépi tanulást – a legjobb esetben is – csak zavarba hozza saját sikere.</p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Az ebben az alfejezetben tárgyalt megközelítés a <span class="strong"><strong>tanulás számítási elméleté</strong></span>n (<span class="strong"><strong>computational learning theory</strong></span>) alapul, amely az MI, a statisztika és az elméleti számítástudomány közös határterülete. Ennek az elméletnek az alapvető elve a következő: <span class="emphasis"><em>bármely súlyosan hibás elmélet szinte bizonyosan nagy valószínűséggel felismerhető kisszámú példa vizsgálata alapján, mivel helytelen predikciót fog adni. Tehát minden hipotézis, amely egy kielégítően nagy tanító példahalmazzal konzisztens választ ad, nem valószínű, hogy súlyosan hibás lenne: azaz <span class="strong"><strong>valószínűleg közelítőleg helyes</strong></span>nek (<span class="strong"><strong>probably approximately correct</strong></span>) kell lennie.</em></span> Minden olyan tanuló algoritmust, amely valószínűleg<span class="emphasis"><em> </em></span>közelítőleg helyes hipotéziseket ad, <span class="strong"><strong>VKH-tanuló</strong></span> (<span class="strong"><strong>PAC-learning</strong></span>) algoritmusnak nevezünk.</p></div><p>Van néhány bonyolultabb rész az előbbi gondolatmenetben. A fő kérdés a tanító és a tesztpéldák viszonya: végül is mi azt szeretnénk, ha a hipotézis nem csupán a tanító halmazon lenne közelítőleg helyes, hanem elsősorban a teszthalmazon. Az alapvető feltevés az, hogy a tanító és a teszthalmazt <span class="emphasis"><em>ugyanolyan valószínűség-eloszlást</em></span> használva, véletlenszerűen és függetlenül választottuk ugyanabból a példapopulációból. Ezt <span class="strong"><strong>stacionaritás</strong></span>i (<span class="strong"><strong>stationarity</strong></span>) feltevésnek nevezzük. A stacionaritási feltevés nélkül az elmélet semmilyen, a jövőre vonatkozó igénnyel nem léphet fel, mivel nem lesz szükségszerű kapcsolat a múlt és a jövő között. A stacionaritási feltevés támasztja alá azt a gondolatot, hogy a példákat kiválasztó eljárás nem rosszindulatú. Persze ha a tanító halmaz kizárólag furcsa példákat tartalmaz – kétfejű kutyákat például –, akkor a tanuló algoritmus nyilvánvalóan nem tehet mást, mint sikertelen általánosításra jut a felől, hogy mi módon lehet a kutyákat felismerni.</p><div class="section" title="Hány példára van szükség?"><div class="titlepage"><div><div><h2 class="title"><a id="id727020"/>Hány példára van szükség?</h2></div></div></div><p>Néhány definícióra szükségünk lesz ahhoz, hogy ezeket a felismeréseket a gyakorlatba át tudjuk ültetni:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Jelöljük <span class="strong"><strong>X</strong></span>-szel az összes lehetséges példák halmazát.</p></li><li class="listitem"><p>Jelölje <span class="emphasis"><em>D</em></span> azt a valószínűségi eloszlást, amely alapján a példákat választjuk.</p></li><li class="listitem"><p>Legyen <span class="strong"><strong>H</strong></span> a lehetséges hipotézisek halmaza.</p></li><li class="listitem"><p>Legyen <span class="emphasis"><em>N</em></span> a tanító halmaz elemeinek a száma.</p></li></ul></div><p>Előzetesen feltesszük, hogy a keresett <span class="emphasis"><em>f</em></span> függvény eleme <span class="strong"><strong>H</strong></span>-nak. Most már definiálhatjuk azt a <span class="strong"><strong>hibá</strong></span>t (<span class="strong"><strong>error</strong></span>), ami egy <span class="emphasis"><em>h</em></span> hipotézisnek a keresett <span class="emphasis"><em>f</em></span> függvénytől való eltérését jellemzi a mintákra vonatkozó adott eloszlás mellett, mint annak valószínűségét, hogy <span class="emphasis"><em>h</em></span> az <span class="emphasis"><em>f</em></span>-től eltérő választ ad egy példára:</p><p><code class="code">error(<em><span class="remark">h</span></em>) = <em><span class="remark">P</span></em>(<em><span class="remark">h</span></em>(<em><span class="remark">x</span></em>) ≠ <em><span class="remark">f</span></em>(<em><span class="remark">x</span></em>)|<em><span class="remark">x</span></em> a <em><span class="remark">D</span></em> alapján választva)</code></p><p>Ez ugyanaz a mennyiség, mint amelyet az előzőkben bemutatott tanulási görbékkel mértünk a gyakorlatban.</p><div class="figure"><a id="id727132"/><p class="title"><strong>18.12. ábra - A hipotézistér sematikus ábrázolása a keresett <span class="emphasis"><em>f</em></span> függvény körül felvett „ <span class="emphasis"><em>ε</em></span> -gömb” ábrázolásával</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/18-12.png" alt="A hipotézistér sematikus ábrázolása a keresett f függvény körül felvett „ ε -gömb” ábrázolásával"/></div></div></div><p class="Tartalom3">Egy <span class="emphasis"><em>h</em></span> hipotézist <span class="strong"><strong>közelítőleg helyes</strong></span>nek (<span class="strong"><strong>approximately correct</strong></span>) nevezünk, ha error(<span class="emphasis"><em>h</em></span>) ≤ <span class="emphasis"><em>ε</em></span>, ahol <span class="emphasis"><em>ε</em></span> egy kis konstans. A bizonyítás megközelítése az, hogy megmutatjuk: <span class="emphasis"><em>N</em></span> példa vizsgálata után az összes konzisztens hipotézis nagy valószínűséggel közelítőleg helyes lesz. Úgy gondolhatunk egy közelítőleg helyes hipotézisre, mint amely közel van a keresett függvényhez a hipotézistérben: egy – a keresett <span class="emphasis"><em>f </em></span>körül felvett<span class="emphasis"><em> </em></span>– <span class="bold"><strong><span class="emphasis"><em>ε-</em></span>gömb</strong></span>ön (<span class="bold"><strong><span class="emphasis"><em>ε</em></span></strong></span><span class="strong"><strong>-ball</strong></span>) belül van. A 18.12. ábrán bemutatjuk az összes hipotézis <span class="strong"><strong>H</strong></span> terét, amelyet két részre osztottunk, egyik az <span class="emphasis"><em>ε</em></span>-gömb <span class="emphasis"><em>f</em></span> körül, a másik a <span class="strong"><strong>H</strong></span><sub>rossz</sub>-szal jelölt maradék.</p><p class="Tartalom3">A következők alapján kiszámíthatjuk annak valószínűségét, hogy egy „súlyosan hibás” <span class="emphasis"><em>h<sub>r</sub></em></span> ∈ <span class="strong"><strong>H</strong></span><sub>rossz</sub> hipotézis konzisztens lesz az első <span class="emphasis"><em>N</em></span> példával. Tudjuk, hogy error(<span class="emphasis"><em>h<sub>r</sub></em></span>) &gt; <span class="emphasis"><em>ε</em></span>. Tehát annak valószínűsége, hogy egy adott példára helyes választ ad, legfeljebb 1 – <span class="emphasis"><em>ε</em></span>. Ezek után az <span class="emphasis"><em>N</em></span> példára vonatkozó összefüggés:</p><p class="Tartalom3"><code class="code"><em><span class="remark">P</span></em>(<em><span class="remark">h<sub>r</sub></span></em> konzisztens <em><span class="remark">N</span></em> példával) ≤ (1 – <em><span class="remark">ε</span></em>)<sup>N</sup></code></p><p>Annak valószínűségét, hogy <span class="strong"><strong>H</strong></span><sub>rossz</sub> legalább egy konzisztens hipotézist tartalmaz, a lehetséges egyedi hipotézisek száma határozza meg:</p><p><code class="code"><em><span class="remark">P</span></em>(<em><span class="remark">H</span></em><sub>rossz</sub> tartalmaz egy konzisztens hipotézist) ≤ |<em><span class="remark">H</span></em><sub>rossz</sub>|(1 – <em><span class="remark">ε</span></em>)<sup>N</sup> ≤ |<em><span class="remark">H</span></em>|(1 – <em><span class="remark">ε</span></em>)<sup>N</sup></code></p><p>Kihasználtuk, hogy |<span class="strong"><strong>H</strong></span><sub>rossz</sub>| ≤ |<span class="strong"><strong>H</strong></span>|. Ezt a valószínűséget valamely kellően kis <span class="emphasis"><em>δ</em></span> érték alá akarjuk csökkenteni:</p><p><code class="code">|<em><span class="remark">H</span></em>|(1– <em><span class="remark">ε</span></em>)<sup>N </sup>≤ <em><span class="remark">δ</span></em></code></p><p>Felhasználva, hogy (1 – <span class="emphasis"><em>ε</em></span>) ≤ <span class="emphasis"><em>e<sup>–ε</sup></em></span>,<span class="emphasis"><em> </em></span>a célunk elérhető, ha lehetővé tesszük, hogy az algoritmus </p><p><span class="inlinemediaobject"><img src="math/mi-18-0012.gif" alt="A hipotézistér sematikus ábrázolása a keresett f függvény körül felvett „ ε -gömb” ábrázolásával"/></span></p><p>példát megvizsgálhasson. Ennek értelmében, ha egy algoritmus ilyen mennyiségű példával konzisztens hipotézist ad vissza, akkor legalább 1 – <span class="emphasis"><em>δ</em></span> valószínűséggel a hibája legfeljebb <span class="emphasis"><em>ε</em></span>. Más szavakkal közelítőleg helyesnek nevezzük. A szükséges példaszámot, ami <span class="emphasis"><em>ε</em></span> és <span class="emphasis"><em>δ</em></span> függvényeként adható meg, a hipotézistér <span class="strong"><strong>minta komplexitás</strong></span>ának (<span class="strong"><strong>sample complexity</strong></span>) nevezzük.</p><p>Kiderült, hogy kulcskérdés a hipotézistér mérete. Mint korábban láttuk, ha <span class="strong"><strong>H</strong></span> az <span class="emphasis"><em>n</em></span> attribútumon felvehető Boole-függvények halmaza, akkor <span class="inlinemediaobject"><img src="math/mi-18-0013.gif" alt="A hipotézistér sematikus ábrázolása a keresett f függvény körül felvett „ ε -gömb” ábrázolásával"/></span>. Tehát a tér minta komplexitása 2<sup>n</sup> szerint nő. Mivel a lehetséges példák száma szintén 2<sup>n</sup>, ebből az következik, hogy a Boole-függvények terében egyetlen tanuló algoritmus sem tud jobb eredményt elérni, mint egy táblázat, ha csupán arra szorítkozik, hogy egy olyan hipotézist adjon vissza, amely az összes ismert példával konzisztens. Ennek megmutatására egy másik lehetőség az, hogy megfigyeljük: egy tetszőleges ismeretlen példára a hipotézistér ugyannyi pozitív kimenetelt jósoló konzisztens hipotézist tartalmaz, mint amennyi negatív kimenetelt jósol.</p><p>A következő dilemmával kerültünk szembe: ha nem korlátozzuk a szóba jövő függvények terét, akkor az algoritmus nem lesz képes tanulni, viszont ha korlátozzuk a függvények terét, akkor fennáll a veszély, hogy kihagyjuk a keresett függvényt. Két lehetőségünk van, hogy „kimeneküljünk” ebből a dilemmából. Az első, hogy nem csupán ahhoz ragaszkodunk, hogy az algoritmus egy tetszőleges konzisztens hipotézist adjon vissza, hanem ahhoz is, hogy részesítse előnyben az egyszerűeket (mint ahogy a döntési fa tanulásnál tettük). Ezen algoritmusok elméleti tárgyalása túlmegy ennek a könyvnek a keretein, de megemlítjük, hogy azon esetekben, amikor az egyszerű konzisztens hipotézisek keresése kezelhető probléma, a mintakomplexitás-eredmények jobbak, mint a csupán konzisztencián alapuló vizsgálatoké. A másik menekülési lehetőség, amelyet követni is fogunk, hogy a Boole-függvények teljes halmazából a megtanulható függvények részhalmazára fogunk koncentrálni. Az ötlet abban áll, hogy legtöbb esetben nincs szükségünk a Boole-függvények teljes kifejezőerejére, korlátozottabb nyelvek is kielégítők számunkra. A következőkben egy ilyen korlátozottabb nyelvet vizsgálunk meg részletesebben.</p></div><div class="section" title="Döntési listák tanulása"><div class="titlepage"><div><div><h2 class="title"><a id="id727443"/>Döntési listák tanulása</h2></div></div></div><p>A <span class="strong"><strong>döntési lista</strong></span> (<span class="strong"><strong>decision list</strong></span>) egy kötött formájú logikai kifejezés. Egy tesztsorozatból áll, amely tesztek mindegyike literálisok konjunkciója. Ha egy teszt valamelyik példa leírására alkalmazva pozitív eredményt ad, akkor a döntési lista határozza meg a példára adandó választ. Ha a teszt eredménye negatív, akkor a lista következő tesztjével folytatódik a feldolgozás.<sup>[<a id="id727458" href="#ftn.id727458" class="footnote">186</a>]</sup> A döntési listák emlékeztetnek a döntési fákra, de a globális struktúrájuk egyszerűbb, ezzel szemben az egyes tesztek bonyolultabbak, mint a döntési fáknál. A 18.3. ábra egy döntési listát mutat be, amely a következő hipotézist reprezentálja:</p><p><code class="code">∀<em><span class="remark">x</span></em>  <em><span class="remark">VárjunkE</span></em>(<em><span class="remark">x</span></em>) ⇔ <em><span class="remark">Vendégek</span></em>(<em><span class="remark">x</span></em>, <em><span class="remark">Néhány</span></em>) ⋁ (<em><span class="remark">Vendégek</span></em>(<em><span class="remark">x</span></em>, <em><span class="remark">Tele</span></em>) ⋀ <em><span class="remark">Péntek</span></em>/<em><span class="remark">Szombat</span></em>(<em><span class="remark">x</span></em>))</code></p><p class="Tartalom3">Ha megengedünk tetszőleges méretű teszteket, akkor a döntési listák bármely Boole-függvény reprezentálására képesek (lásd 18.15. feladat). Másrészről, ha az egyes tesztek méretét <span class="emphasis"><em>k</em></span> literálisra korlátozzuk, akkor lehetővé válik, hogy a tanuló algoritmus kisszámú példa alapján sikeresen általánosítson. Ezt <span class="bold"><strong><span class="emphasis"><em>k</em></span>-DL</strong></span> nyelvnek nevezzük. Könynyen megmutatható, hogy a <span class="emphasis"><em>k</em></span>-DL nyelvek részhalmazként tartalmazzák a <span class="bold"><strong><span class="emphasis"><em>k</em></span>-DF</strong></span> nyelveket, amelyek a legfeljebb <span class="emphasis"><em>k</em></span> mélységű döntési fák halmazát alkotják. Fontos, hogy egy adott nyelv, amelyre <span class="emphasis"><em>k</em></span>-DL jelöléssel hivatkozunk, függ azoktól az attribútumoktól, amelyekkel a példákat leírjuk. A továbbiakban <span class="emphasis"><em>k</em></span>-DL(<span class="emphasis"><em>n</em></span>) lesz a jelölésünk azokra a <span class="emphasis"><em>k</em></span>-DL nyelvekre, amelyek <span class="emphasis"><em>n</em></span> logikai attribútumot használnak.</p><div class="figure"><a id="id727558"/><p class="title"><strong>18.13. ábra - Az éttermi problémára alkotott döntési lista</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/18-13.png" alt="Az éttermi problémára alkotott döntési lista"/></div></div></div><p class="Tartalom3">Első feladatunk annak megmutatása, hogy a <span class="emphasis"><em>k</em></span>-DL nyelvek megtanulhatók, azaz bármely <span class="emphasis"><em>k</em></span>-DL függvény pontosan közelíthető, ha kellő számú tanító mintán tanultunk. Ehhez ki kell számítanunk a nyelvben felvehető hipotézisek számát. Legyen a tesztek nyelve – <span class="emphasis"><em>n</em></span> attribútumot használva, legfeljebb <span class="emphasis"><em>k</em></span> literális konjunkciója – <span class="emphasis"><em>Conj</em></span>(<span class="emphasis"><em>n</em></span>, <span class="emphasis"><em>k</em></span>). A döntési listákat tesztekből építjük fel, és minden teszthez egy <span class="emphasis"><em>Igen</em></span> vagy <span class="emphasis"><em>Nem</em></span> választ rendelhetünk, illetve harmadik lehetőségként a teszt hiányozhat a listáról, ezért legfeljebb 3<sup>|<span class="emphasis"><em>Conj</em></span>(<span class="emphasis"><em>n</em></span>,<span class="emphasis"><em>k</em></span>)|</sup> különböző teszthalmaz létezhet. Ezek a tesztek tetszőleges sorrendben alkalmazhatók, így</p><p><code class="code">|<em><span class="remark">k</span></em>-DL(<em><span class="remark">n</span></em>)| ≤ 3<sup>|<em><span class="remark">Conj</span></em>(<em><span class="remark">n</span></em>,<em><span class="remark">k</span></em>)|</sup>|<em><span class="remark">Conj</span></em>(<em><span class="remark">n</span></em>,<em><span class="remark">k</span></em>)|!</code></p><p>Az <span class="emphasis"><em>n</em></span> attribútumot használó <span class="emphasis"><em>k</em></span> literális lehetséges konjunkcióinak száma:</p><p><span class="inlinemediaobject"><img src="math/mi-18-0014.gif" alt="Az éttermi problémára alkotott döntési lista"/></span></p><p>Így néhány lépés után:</p><p><span class="inlinemediaobject"><img src="math/mi-18-0015.gif" alt="Az éttermi problémára alkotott döntési lista"/></span></p><p>Ezt a (18.1) egyenletbe helyettesítve megmutathatjuk, hogy a <span class="emphasis"><em>k</em></span>-DL függvények VKH-tanulásához szükséges minták száma polinomiálisan nő <span class="emphasis"><em>n</em></span>-nel:</p><p><span class="inlinemediaobject"><img src="math/mi-18-0016.gif" alt="Az éttermi problémára alkotott döntési lista"/></span></p><p>Ennek értelmében bármely algoritmus, amely konzisztens döntési listát ad vissza, kis <span class="emphasis"><em>k</em></span> értékek esetén elfogadható példaszám alapján VKH-tanulással képes előállítani egy <span class="emphasis"><em>k</em></span>-DL függvényt.</p><p>A következő feladatunk, hogy hatékony algoritmust találjunk, amely konzisztens döntési listát eredményez. A <code class="code">DÖNTÉSI-LISTA-TANULÁS</code> nevű mohó algoritmust fogjuk használni. Ez ismételten talál egy olyan tesztet, amely pontosan a tanító halmaz valamely részhalmazának felel meg. Amikor talál egy ilyen tesztet, akkor azt hozzáadja a döntési listához, egyben eltávolítva a tesztnek megfelelő példákat a tanító halmazból. Ezek után létrehozza a döntési lista hátralevő részét pusztán a maradék példákra alapozva. Ezt addig ismétli, amíg nem marad több példa. Az algoritmust a 18.14. ábra mutatja.</p><p>Ez az algoritmus nem specifikálja, hogy milyen módszert használjunk a következő – a döntési listához adni kívánt – teszt kiválasztásához. Bár a bemutatott formálisan elért eredmények nem függnek a kiválasztás módszerétől, mégis az látszik józannak, ha előnyben részesítjük azokat az egyszerű teszteket, amelyek az osztályozott példák nagy részhalmazainak felelnek meg. Ennek eredményeként a döntési lista a lehető legtömörebb lesz. A legegyszerűbb stratégia, ha azt a legrövidebb <span class="emphasis"><em>t</em></span> tesztet keressük meg, amely egy tetszőleges, egyforma osztálybasorolással rendelkező részhalmaznak megfelel, figyelmen kívül hagyva a részhalmaz méretét. Még ez a megközelítés is egész jól működik, mint az a 18.15. ábrán látható.</p><div class="figure"><a id="id727724"/><p class="title"><strong>18.14. ábra - Egy döntési lista tanulásra használható algoritmus</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/18-14.png" alt="Egy döntési lista tanulásra használható algoritmus"/></div></div></div><div class="figure"><a id="id727733"/><p class="title"><strong>18.15. ábra - A <code class="code">DÖNTÉSI-LISTA-TANULÁS</code> algoritmusnak az éttermi adatokon felvett tanulási görbéje. Az összehasonlítás kedvéért feltüntettük a <code class="code">DÖNTÉSI-FA-TANULÁS</code> algoritmus görbéjét is.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/18-15.png" alt="A DÖNTÉSI-LISTA-TANULÁS algoritmusnak az éttermi adatokon felvett tanulási görbéje. Az összehasonlítás kedvéért feltüntettük a DÖNTÉSI-FA-TANULÁS algoritmus görbéjét is."/></div></div></div></div><div class="section" title="Elemzés"><div class="titlepage"><div><div><h2 class="title"><a id="id727750"/>Elemzés</h2></div></div></div><p>A tanulás számítási elmélete új szempontból világította meg a tanulás problémáját. Az 1960-as évek elején a tanulás elmélet az <span class="strong"><strong>identifikáció határátmenet</strong></span>ben (<span class="strong"><strong>identification in the limit</strong></span>) problémájára koncentrált. E szerint egy identifikációs algoritmusnak mindenképpen vissza kell adnia egy olyan hipotézist, amely pontosan megegyezik a keresett függvénnyel. Erre a következőkben ismertetett eljárás ad egy lehetséges módszert. Először is rendezzük a <span class="strong"><strong>H</strong></span> hipotézistér minden elemét valamilyen egyszerűség mérték szerint. Ezek után válasszuk azt a legegyszerűbb hipotézist, amely konzisztens az összes már rendelkezésre álló példával. Ahogy újabb példák érkeznek, a módszer fel fogja adni az egyszerűbb hipotézist, ha az új példák érvénytelenítették, és helyette egy bonyolultabbat választ. Amikor eléri a keresett függvényt, akkor azt már soha nem fogja feladni. Sajnálatos, hogy számos hipotézistérben óriási nagy lehet a keresett függvény eléréséhez szükséges idő, valamint a szükséges példák száma. Ezért a tanulás számítási elmélete nem ragaszkodik ahhoz, hogy a tanuló ágens megtalálja a környezetét vezérlő „egyedüli helyes törvényt”. Ehelyett inkább egy olyan hipotézist keres, amely egy bizonyos prediktív pontosságot elér. A tanulás számítási elmélete ezen felül erősen hangsúlyozza a hipotézis nyelv kifejezőképessége és a tanulás komplexitása közötti kompromisszum fontosságát. Ez az elmélet vezetett egy fontos tanuló algoritmus osztályhoz, a szupport vektorgépekhez.</p><p>Az általunk bemutatott, VKH-tanulásra vonatkozó eredmények a legrosszabb esetre érvényesek, nem feltétlenül mutatva azt az átlagos esetre vonatkozó mintakomplexitást, amelyeket viszont a bemutatott tanulási görbék tükröznek. Egy, az átlagos esetre vonatkozó analízisnek további feltételezésekkel kell élnie a minták eloszlásáról, valamint
a keresett függvények eloszlásáról. Miközben ezeket a kérdéseket egyre jobban megértjük, a tanulás számítási elmélete folyamatosan a gépi tanulással foglalkozó kutatók hasznos útmutatójának bizonyul. Ezek a kutatók algoritmusaik módosításában vagy algoritmusaik tanulási képességének meghatározásában érdekeltek. A döntési listák mellett a Boole-függvények szinte minden ismert alosztályára, a neurális hálókra (lásd 20. fejezet), valamint az elsőrendű logikai állítások halmazaira (lásd 19. fejezet) születtek eredmények. Az elért eredmények azt mutatják, hogy a tisztán indukciós tanulás rendkívül nehéz. Tisztán indukciós tanuláson azt a helyzetet értjük, amelyben az ágens nem rendelkezik semmilyen, a keresett függvényre vonatkozó, előzetes ismerettel. Mint a 19. fejezetben megmutatjuk, az előzetes tudásnak az induktív tanulás vezérlésére való felhasználása lehetővé teszi, hogy meglehetősen nagy állításhalmazokat elfogadható mintaszám alapján megtanuljunk, még egy olyan nyelven is, amelynek kifejezőereje megegyezik az elsőrendű logikáéval.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id727458" href="#id727458" class="para">186</a>] </sup> Egy döntési lista tehát szerkezetében megegyezik a Lisp nyelv cond állításával.</p></div></div></div></body></html>
