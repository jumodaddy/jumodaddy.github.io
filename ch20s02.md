<?xml version="1.0" encoding="UTF-8" standalone="no"?>

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Teljes adattal történő tanulás"><div class="titlepage"><div><div><h1 class="title"><a id="id739621"/>Teljes adattal történő tanulás</h1></div></div></div><p>A statisztikai tanulási módszerek tárgyalását a legegyszerűbb feladattal kezdjük: <span class="strong"><strong>paramétertanulás</strong></span> (<span class="strong"><strong>parameter learning</strong></span>)<span class="strong"><strong> teljes adat</strong></span> (<span class="strong"><strong>complete data</strong></span>) alapján. A paramétertanulás egy rögzített struktúrájú valószínűségi modell paramétereinek megtalálását foglalja magában. Például meg akarjuk tanulni egy adott struktúrájú Bayes-háló feltételes valószínűségeit. Az adatokat akkor nevezzük teljesnek, ha mindegyik adatpont értékeket hordoz a megtanulandó valószínűségi modell mindegyik paraméterére. A teljes adatok nagyban egyszerűsítik a komplex modellek paramétereinek tanulását. Nagy vonalakban áttekintjük majd a struktúratanulás problematikáját is.</p><div class="section" title="Maximum-likelihood paramétertanulás: diszkrét modellek"><div class="titlepage"><div><div><h2 class="title"><a id="id739646"/>Maximum-likelihood paramétertanulás: diszkrét modellek</h2></div></div></div><p>Tegyük fel, hogy új gyártótól vásárolunk egy zsák citrom- és meggycukorkát, a meggyarány teljesen ismeretlen, bárhol lehet 0 és 1 között. Ez esetben kontinuum számosságú hipotézisünk van. A <span class="strong"><strong>paraméter</strong></span> (<span class="strong"><strong>parameter</strong></span>), amelyet <span class="emphasis"><em>θ</em></span>-val jelölünk, most a meggycukorkák aránya, a hipotézis pedig <span class="emphasis"><em>h<sub>θ</sub></em></span>. (A citromízűek aránya egyszerűen 1 – <span class="emphasis"><em>θ</em></span>.) Ha feltételezzük, hogy a priori minden arány egyformán valószínű, akkor a maximum-likelihood megközelítés az ésszerű. Ha Bayes-hálóval modellezzük a helyzetet, akkor csupán egyetlen véletlen változóra van szükségünk. Legyen ez az <span class="emphasis"><em>Íz</em></span> nevű változó (a zacskóból véletlenszerűen választott cukorka íze). Lehetséges értékei a <span class="emphasis"><em>meggy</em></span> és a <span class="emphasis"><em>citrom</em></span>, ahol a <span class="emphasis"><em>meggy</em></span> valószínűsége <span class="emphasis"><em>θ</em></span> (lásd 20.2. (a) ábra). Tegyük fel, hogy kibontunk <span class="emphasis"><em>N</em></span> cukorkát, amelyek közül <span class="emphasis"><em>c</em></span> meggyízű és <span class="emphasis"><em>ℓ</em></span> = <span class="emphasis"><em>N</em></span> – <span class="emphasis"><em>c</em></span> citromízű. A (20.3) egyenlet alapján ennek a speciális adathalmaznak a valószínűsége:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0003.gif" alt="Maximum-likelihood paramétertanulás: diszkrét modellek"/></span></p><div class="figure"><a id="id739720"/><p class="title"><strong>20.2. ábra - (a) Bayes-háló modell az ismeretlen arányban citrom-, illetve meggyízű cukorkák esetére. (b) Annak a modellje, amikor a csomagolópapír színe függ (valószínűségi alapon) a cukorka ízétől.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-02.png" alt="(a) Bayes-háló modell az ismeretlen arányban citrom-, illetve meggyízű cukorkák esetére. (b) Annak a modellje, amikor a csomagolópapír színe függ (valószínűségi alapon) a cukorka ízétől."/></div></div></div><p class="Tartalom3">A maximum-likelihood hipotézist az a <span class="emphasis"><em>θ</em></span> érték adja, amely maximálja ezt a kifejezést. Ugyanezt az értéket kapjuk, ha a <span class="strong"><strong>log likelihood</strong></span> függvényt maximáljuk.</p><p><span class="inlinemediaobject"><img src="math/mi-20-0004.gif" alt="(a) Bayes-háló modell az ismeretlen arányban citrom-, illetve meggyízű cukorkák esetére. (b) Annak a modellje, amikor a csomagolópapír színe függ (valószínűségi alapon) a cukorka ízétől."/></span></p><p>(A kifejezés logaritmusát képezve a szorzatot szummává redukáltuk, amit rendszerint egyszerűbb maximálni.) A maximum-likelihood <span class="emphasis"><em>θ</em></span> érték megtalálása érdekében differenciáljuk <span class="emphasis"><em>L</em></span>-et <span class="emphasis"><em>θ</em></span> szerint, a kapott kifejezést pedig tegyük egyenlővé nullával:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0005.gif" alt="(a) Bayes-háló modell az ismeretlen arányban citrom-, illetve meggyízű cukorkák esetére. (b) Annak a modellje, amikor a csomagolópapír színe függ (valószínűségi alapon) a cukorka ízétől."/></span></p><p>Magyarán a <span class="emphasis"><em>h</em></span><sub>ML</sub> maximum-likelihood hipotézis azt állítja, hogy a zacskóban a meggyízű cukorkák valós aránya megegyezik az eddig kibontott cukorkáknál megfigyelt aránnyal!</p><p>Úgy tűnik, rengeteget dolgoztunk, hogy felfedezzünk egy nyilvánvaló eredményt. Valójában lefektettünk egy standard módszert a maximum-likelihood paramétertanulásra:</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p>Írjunk fel egy – a paraméter(ek)től függő – kifejezést az adatok együttes valószínűségére (írjuk fel a likelihood függvényt).</p></li><li class="listitem"><p>Írjuk fel minden egyes paraméter szerint a log likelihood függvény deriváltját.</p></li><li class="listitem"><p>Keressük meg azokat a paraméterértékeket, amelyek mellett a deriváltak nulla értéket vesznek fel.</p></li></ol></div><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p class="Tartalom3">A legtrükkösebb lépés általában az utolsó. Az előző példánkban triviálisan megoldható volt, de látni fogjuk, hogy sokszor iteratív megoldásokhoz vagy más numerikus optimalizálási technikákhoz kell folyamodnunk, mint ahogy a 4. fejezetben tárgyaltuk. A példa a maximum-likelihood tanulás egy általános problémáját is illusztrálja: <span class="emphasis"><em>ha az adathalmaz elég kicsi ahhoz, hogy néhány eseményt még nem figyeltünk meg – például nem találtunk még meggyízű cukorkát –, akkor a maximum-likelihood hipotézis nulla valószínűséget rendel ezekhez az eseményekhez.</em></span> Számos trükköt használnak, hogy elkerüljék ezt a problémát, mint például minden esemény kezdeti valószínűségét 1-re állítják nulla helyett.</p></div><p>Nézzünk egy másik példát. Tegyük fel, hogy a cukorka gyártója némi információt akar adni a fogyasztónak, ezért piros és zöld csomagolópapírt használ. A <span class="emphasis"><em>Csomagoló</em></span> mindegyik cukorkához véletlenszerűen kerül kiválasztásra, valamilyen ismeretlen – az íztől függő – feltételes valószínűség-eloszlás szerint. Az ehhez tartozó valószínűségi modellt a 20.2. (b) ábra mutatja. Vegyük észre, hogy három paramétere van: <span class="emphasis"><em>θ</em></span>, <span class="emphasis"><em>θ</em></span><sub>1</sub> és <span class="emphasis"><em>θ</em></span><sub>2</sub>. Ezekkel a parméterekkel a Bayes-hálók standard szemantikáját használva megadható annak a valószínűsége, hogy – mondjuk – egy meggyízű cukorkát találunk egy zöld csomagolóban (<a class="xref" href="ch14s02.md#ID_581_582_oldal">„Az együttes valószínűség-eloszlás függvény leírása”</a> rész):</p><p><span class="inlinemediaobject"><img src="math/mi-20-0006.gif" alt="(a) Bayes-háló modell az ismeretlen arányban citrom-, illetve meggyízű cukorkák esetére. (b) Annak a modellje, amikor a csomagolópapír színe függ (valószínűségi alapon) a cukorka ízétől."/></span></p><p>Most kibontunk <span class="emphasis"><em>N</em></span> cukorkát, amelyek közül <span class="emphasis"><em>c</em></span> meggyízű és <span class="emphasis"><em>ℓ</em></span> citromízű. A csomagolásfajták számai a következők: <span class="emphasis"><em>r<sub>c</sub> </em></span>meggyízű volt pirosba csomagolva<span class="emphasis"><em> </em></span>és<span class="emphasis"><em> g<sub>c</sub> </em></span>zöldbe, míg<span class="emphasis"><em> r<sub>ℓ</sub></em></span> citromízű volt pirosba csomagolva, míg <span class="emphasis"><em>g<sub>ℓ</sub></em></span> zöldbe. Ezen adatok együttes valószínűsége:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0007.gif" alt="(a) Bayes-háló modell az ismeretlen arányban citrom-, illetve meggyízű cukorkák esetére. (b) Annak a modellje, amikor a csomagolópapír színe függ (valószínűségi alapon) a cukorka ízétől."/></span></p><p>Ez meglehetősen ijesztőnek tűnik, de segít, ha a logaritmusát vesszük:</p><p><code class="code"><em><span class="remark">L </span></em>= [<em><span class="remark">c</span></em>log<em><span class="remark">θ </span></em>+<em><span class="remark"> ℓ</span></em>log(1 – <em><span class="remark">θ</span></em>)] + [<em><span class="remark">r<sub>c</sub> </span></em>log<em><span class="remark">θ</span></em><sub>1 </sub>+<sub> </sub><em><span class="remark">g<sub>c</sub> </span></em>log(1 – <em><span class="remark">θ</span></em><sub>1</sub>)] + [<em><span class="remark">r<sub>ℓ</sub> </span></em>log<em><span class="remark">θ</span></em><sub>2</sub> + <em><span class="remark">g<sub>ℓ</sub> </span></em>log(1 – <em><span class="remark">θ</span></em><sub>2</sub>)]</code></p><p>A logaritmusképzés előnye nyilvánvaló: a log likelihood függvény három tag összege, ahol mindegyik tag csupán egyetlen paramétert tartalmaz. Amikor sorban mindegyik paraméter szerint vesszük az összefüggés deriváltját, majd nullává tesszük a deriváltakat, három független egyenlethez jutunk, és mindegyik csupán egyetlen paramétert tartalmaz:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0008.gif" alt="(a) Bayes-háló modell az ismeretlen arányban citrom-, illetve meggyízű cukorkák esetére. (b) Annak a modellje, amikor a csomagolópapír színe függ (valószínűségi alapon) a cukorka ízétől."/></span></p><p>A <span class="emphasis"><em>θ</em></span>-ra kapott megoldás ugyanaz, mint az előbb. A <span class="emphasis"><em>θ</em></span><sub>1</sub>-re kapott megoldás, tehát annak valószínűsége, hogy egy meggyízű cukorka piros papírba csomagolt, nem más, mint a megfigyelt meggyízű cukorka – piros papír arány, hasonló a helyzet <span class="emphasis"><em>θ</em></span><sub>2</sub>-vel.</p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Ezek az eredmények nagyon kényelmesek, és könnyen belátható, hogy kiterjeszthetők bármely Bayes-hálóra, amelynek feltételes valószínűségeit táblázatokkal adjuk meg. A legfontosabb eredmény a következő: <span class="emphasis"><em>teljes adatok esetén a Bayes-háló paramétertanulási problémája elkülönülő tanulási problémákra dekomponálható, egy-egy probléma egy-egy paraméterre.<sup>[<a id="id740005" href="#ftn.id740005" class="footnote">196</a>]</sup></em></span> A második eredmény, hogy az egyes paraméterek adott szülő melletti értékei éppen a szülőértékek mellett megfigyelt gyakoriságokkal egyeznek meg. Éppúgy, mint az előző helyzetben, itt is óvatosnak kell lennünk, hogy kis adathalmazok esetén el tudjuk kerülni a nulla értékeket.</p></div></div><div class="section" title="Naiv Bayes-modellek"><div class="titlepage"><div><div><h2 class="title"><a id="id740011"/>Naiv Bayes-modellek</h2></div></div></div><p>Valószínűleg a gépi tanulás területén használt legelterjedtebb Bayes-háló modell a <span class="strong"><strong>naiv Bayes-modell</strong></span> (<span class="strong"><strong>naive Bayes</strong></span>). Ebben a modellben a <span class="emphasis"><em>C</em></span> „osztályváltozó” (amelyet meg akarunk jósolni) a gyökér, míg az <span class="emphasis"><em>X<sub>i</sub></em></span> attribútumváltozók a levelek. A modell azért „naiv”, mert feltételezi, hogy adott osztály mellett az attribútumok feltételesen függetlenek egymástól. (A 20.2. (b) ábra modellje egy egyváltozós naiv Bayes-modell.) Logikai változókat feltételezve a paraméterek:</p><p><code class="code"><em><span class="remark">θ </span></em>= <em><span class="remark">P</span></em>(<em><span class="remark">C</span></em> = <em><span class="remark">igaz</span></em>),<em><span class="remark"> θ<sub>i</sub></span></em><sub>1</sub> =<em><span class="remark">P</span></em>(<em><span class="remark">X<sub>i</sub></span></em> = <em><span class="remark">igaz</span></em>|<em><span class="remark">C</span></em> = <em><span class="remark">igaz</span></em>), <em><span class="remark">θ</span></em><sub><em><span class="remark">i</span></em>2</sub> = <em><span class="remark">P</span></em>(<em><span class="remark">X<sub>i</sub></span></em> = <em><span class="remark">igaz</span></em>|<em><span class="remark">C</span></em> = <em><span class="remark">hamis</span></em>)</code></p><p>A maximum-likelihood paramétereket pontosan úgy kapjuk meg, mint a 20.2. (b) ábra esetén. Ha a modellt ezen az úton megtanítottuk, akkor felhasználható arra, hogy olyan új példákat osztályozzon, amelyekre a <span class="emphasis"><em>C</em></span> osztályváltozó nem ismert. A megfigyelt <span class="emphasis"><em>x</em></span><sub>1</sub>, ..., <span class="emphasis"><em>x<sub>n</sub></em></span> attribútumértékek mellett az egyes osztályok valószínűségét a következő összefüggés adja:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0009.gif" alt="Naiv Bayes-modellek"/></span></p><div class="figure"><a id="id740137"/><p class="title"><strong>20.3. ábra - A 18. fejezet étterem problémájára alkalmazott naiv Bayes-tanulás tanulási görbéje. A döntési fa tanulás tanulási görbéjét összehasonlítás céljából ábrázoltuk.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-03.png" alt="A 18. fejezet étterem problémájára alkalmazott naiv Bayes-tanulás tanulási görbéje. A döntési fa tanulás tanulási görbéjét összehasonlítás céljából ábrázoltuk."/></div></div></div><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Ha a legvalószínűbb osztályt választjuk, determinisztikus predikció adható. A 20.3. ábra mutatja a módszer tanulási görbéjét, ha a 18. fejezet étterem problémájára alkalmazzuk. A módszer elég jól tanul, de nem olyan jól, mint egy döntési fa tanulás. Ennek oka valószínűleg az, hogy a helyes hipotézis – ami egy döntési fa – nem reprezentálható pontosan naiv Bayes-modellel. A naiv Bayes-tanulás sok alkalmazási területen meglepően jól teljesít, a fokozott teljesítményű, turbó változata <span class="emphasis"><em>(boosted version) </em></span>egyike a leghatékonyabb általános célú tanuló algoritmusoknak (lásd 20.5. feladat). A módszer nagyon nagy méretű problémákhoz is jól alkalmazható, <span class="emphasis"><em>n</em></span> logikai változó esetén is csak 2<span class="emphasis"><em>n </em></span>+ 1 paramétere lesz, és <span class="emphasis"><em>ahhoz, hogy a naiv Bayes-tanulás meghatározza h<sub>ML</sub>-t, nincs szükség keresésre</em></span>. Végül a naiv Bayes-tanulásnak nem jelentenek gondot a zajos adatok, továbbá ha szükséges, akkor képes valószínűségi predikciókat is adni.</p></div></div><div class="section" title="Maximum-likelihood paramétertanulás: folytonos eset"><div class="titlepage"><div><div><h2 class="title"><a id="id740169"/>Maximum-likelihood paramétertanulás: folytonos eset</h2></div></div></div><a id="ID_826_827_oldal"/><p class="Tartalom3">A 14.3. alfejezetben vezettük be a folytonos valószínűségi modelleket, mint például a <span class="strong"><strong>lineáris Gauss</strong></span>- (<span class="strong"><strong>linear-Gaussian</strong></span>) modelleket. Mivel a valós alkalmazásokban mindenütt folytonos változókkal találkozunk, fontos ismernünk a folytonos modellek adatokból történő megtanulásának módszereit. A maximum-likelihood tanulás elvei azonosak a diszkrét esetre vonatkozó tanulás elveivel.</p><p>Kezdjük egy rendkívül egyszerű esettel: egyetlen skalár változó Gauss-sűrűségfüggvényének a paramétereit tanuljuk. Azaz az adatokat a következő összefüggéssel generáljuk:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0010.gif" alt="Maximum-likelihood paramétertanulás: folytonos eset"/></span></p><p>A modell paraméterei a <span class="emphasis"><em>μ</em></span> átlag és a <span class="emphasis"><em>σ</em></span> szórás. (Vegyük észre, hogy a normalizáló konstans is függ <span class="emphasis"><em>σ</em></span> -tól, ezért nem hanyagolhatjuk el.) Legyenek a megfigyelt értékek <span class="emphasis"><em>x</em></span><sub>1</sub>,..., <span class="emphasis"><em>x<sub>N</sub></em></span>. Ekkor a log likelihood:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0011.gif" alt="Maximum-likelihood paramétertanulás: folytonos eset"/></span></p><p>A deriváltakat szokásos módon nullává téve a következőket kapjuk:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0012.gif" alt="Maximum-likelihood paramétertanulás: folytonos eset"/></span></p><p>Tehát az átlag maximum-likelihood becslése a mintaátlag, a szórás maximum-likelihood becslése pedig a minta átlagos szórásnégyzetének négyzetgyöke. Ezek ismét kedvező eredmények, mivel megerősítik a „józan ésszel” követett gyakorlatot.</p><p>Vizsgáljunk most egy lineáris Gauss-modellt, amelyben egy <span class="emphasis"><em>X</em></span> folytonos szülő és <span class="emphasis"><em>Y</em></span> folytonos gyermek van. Mint az <a class="xref" href="ch14s03.md#ID_589_oldal">„Bayes-hálók folytonos változókkal”</a> részben megmutattuk, <span class="emphasis"><em>Y</em></span> Gauss-eloszlású, átlaga lineárisan függ <span class="emphasis"><em>X</em></span>-től, míg varianciája rögzített. A <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>X</em></span>|<span class="emphasis"><em>Y</em></span>) feltételes eloszlás tanulásához maximalizálhatjuk a feltételes likelihood függvényt:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0013.gif" alt="Maximum-likelihood paramétertanulás: folytonos eset"/></span></p><div class="figure"><a id="id740286"/><p class="title"><strong>20.4. ábra - (a) Egy <span class="emphasis"><em>y</em></span> = (<span class="emphasis"><em>θ</em></span><sub>1</sub> + <span class="emphasis"><em>θ<sub>j</sub></em></span> + <span class="emphasis"><em>θ</em></span><sub>2</sub>) egyenlettel leírható lineáris Gauss-modell additív, rögzített varianciájú Gauss-zajjal. (b) E modell alapján generált 50 adatpontból álló halmaz.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-04.png" alt="(a) Egy y = (θ1 + θj + θ2) egyenlettel leírható lineáris Gauss-modell additív, rögzített varianciájú Gauss-zajjal. (b) E modell alapján generált 50 adatpontból álló halmaz."/></div></div></div><p class="Tartalom3">Itt a paraméterek <span class="emphasis"><em>θ</em></span><sub>1</sub>, <span class="emphasis"><em>θ</em></span><sub>2</sub> és <span class="emphasis"><em>σ</em></span>. Mint a 20.4. ábrán szemléltettük, az (<span class="emphasis"><em>x<sub>j</sub></em></span>, <span class="emphasis"><em>y<sub>j</sub></em></span>) párok gyűjteménye adja az adatokat. A szokásos módszereket használva (lásd 20.6. feladat) megkaphatjuk a paraméterek maximum-likelihood értékeit. Itt most egy másik dolgot akarunk megmutatni. Vegyük csupán a <span class="emphasis"><em>θ</em></span><sub>1</sub> és <span class="emphasis"><em>θ</em></span><sub>2</sub> paramétereket, amelyek az <span class="emphasis"><em>x</em></span> és <span class="emphasis"><em>y</em></span> közti lineáris összefüggést definiálják! Nyilvánvaló, hogy a log likelihood ezen paraméterekkel történő maximalizálása azonos azzal, mintha a (20.5) kifejezésben a kitevő számlálóját <span class="emphasis"><em>minimalizálnánk:</em></span></p><p><span class="inlinemediaobject"><img src="math/mi-20-0014.gif" alt="(a) Egy y = (θ1 + θj + θ2) egyenlettel leírható lineáris Gauss-modell additív, rögzített varianciájú Gauss-zajjal. (b) E modell alapján generált 50 adatpontból álló halmaz."/></span></p><p>Az (<span class="emphasis"><em>y</em></span><sub>j</sub> – (<span class="emphasis"><em>θ</em></span><sub>1</sub><span class="emphasis"><em>x<sub>j</sub></em></span> + <span class="emphasis"><em>θ</em></span><sub>2</sub>)) mennyiség valójában az (<span class="emphasis"><em>x<sub>j</sub></em></span>, <span class="emphasis"><em>y<sub>j</sub></em></span>) <span class="strong"><strong>hibá</strong></span>ja (<span class="strong"><strong>error</strong></span>) – azaz a tényleges <span class="emphasis"><em>y</em></span><sub><span class="emphasis"><em>j</em></span> </sub>érték és a becsült érték különbsége. Így <span class="emphasis"><em>E</em></span> nem más, mint a jól ismert <span class="strong"><strong>hibanégyzetek összege</strong></span> (<span class="strong"><strong>sum of squared errors</strong></span>). Ezt a standard <span class="strong"><strong>lineáris regresszió</strong></span> (<span class="strong"><strong>linear regression</strong></span>) minimalizálja. Most megérthetjük, hogy miért: a hibanégyzetek összegének minimalizálása nem más, mint a maximum-likelihood lineáris (egyenessel ábrázolható) modell megadása, <span class="emphasis"><em>feltéve, hogy az adatokat rögzített varianciájú Gauss-zaj mellett generáltuk</em></span>.</p></div><div class="section" title="Bayes-paramétertanulás"><div class="titlepage"><div><div><h2 class="title"><a id="id740463"/>Bayes-paramétertanulás</h2></div></div></div><p class="Tartalom3">A maximum-likelihood tanulás alkalmat ad néhány nagyon egyszerű eljárás létrehozására, de kis adathalmazok esetén súlyos hiányosságokat mutat. Például egyetlen meggyízű cukorka észlelése után az a maximum-likelihood hipotézis, hogy a csomag 100%-a meggytípusú (azaz <span class="emphasis"><em>θ</em></span> = 1,0). Ha nincs olyan hipotézis prior, hogy a csomagok mind vagy csupa meggy-, vagy csupa citromtípusúak, akkor ez nem józan következtetés. A Bayes-megközelítésű paramétertanulás egy hipotézis priort állít fel a lehetséges paraméterértékekre, és ahogy az adatok érkeznek, úgy frissíti az eloszlást.</p><p>A 20.2. (a) cukorka példának egyetlen <span class="emphasis"><em>θ</em></span> paramétere van; annak valószínűsége, hogy egy véletlenszerűen kiválasztott cukorka meggyízű. Bayes-megközelítésben <span class="emphasis"><em>θ</em></span> a Θ valószínűségi változó (ismeretlen) értéke, a hipotézis prior pedig nem más, mint a <span class="strong"><strong>P</strong></span>(Θ) a priori eloszlás. Így P(Θ = <span class="emphasis"><em>θ</em></span>) annak a priori<span class="emphasis"><em> </em></span>valószínűsége, hogy a csomag <span class="emphasis"><em>θ</em></span> arányban tartalmaz meggyízű cukrokat.</p><div class="figure"><a id="id740499"/><p class="title"><strong>20.5. ábra - Példák a béta[<span class="emphasis"><em>a</em></span>,<span class="emphasis"><em>b</em></span>] eloszlásra különböző [<span class="emphasis"><em>a</em></span>,<span class="emphasis"><em>b</em></span>] értékek esetén</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-05.png" alt="Példák a béta[a,b] eloszlásra különböző [a,b] értékek esetén"/></div></div></div><p>Ha a <span class="emphasis"><em>θ</em></span> tetszőleges értéket felvehet 0 és 1 között, akkor a<span class="emphasis"><em> </em></span><span class="strong"><strong>P</strong></span>(Θ)-nek egy folytonos eloszlásnak kell lennie, amely csak 0 és 1 között nem nulla értékű, és integrálja 1. Egy lehetséges jelölt az egyenletes eloszlás <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>θ</em></span>) = <span class="emphasis"><em>U</em></span>[0, 1]<span class="emphasis"><em> </em></span>(<span class="emphasis"><em>θ</em></span>). (Lásd 13. fejezet.) Az egyenletes eloszlás a <span class="strong"><strong>béta-eloszlás</strong></span>ok (<span class="strong"><strong>beta distribution</strong></span>s) családjának tagja. Minden egyes béta-eloszlás két <span class="strong"><strong>hiperparaméter</strong></span>rel<sup>[<a id="id740570" href="#ftn.id740570" class="footnote">197</a>]</sup> (<span class="strong"><strong>hyperparameter</strong></span>) – <span class="emphasis"><em>a</em></span>-val és <span class="emphasis"><em>b</em></span>-vel – definiálható a következő egyenlet szerint:</p><p><code class="code">béta[<em><span class="remark">a</span></em>,<em><span class="remark">b</span></em>]<em><span class="remark"> </span></em>(<em><span class="remark">θ</span></em>) = <em><span class="remark">αθ<sup>a</sup></span></em><sup>–1</sup> (1–<em><span class="remark">θ</span></em>)<sup><em><span class="remark">b</span></em>–1</sup>					(20.6)</code></p><p>A (20.6) megadja <span class="emphasis"><em>θ</em></span>-ra a [0, 1] tartományban a sűrűségfüggvény értékét. Az <span class="emphasis"><em>α</em></span> normáló konstans <span class="emphasis"><em>a</em></span>-tól és <span class="emphasis"><em>b</em></span>-től függ. (Lásd 20.8. feladat.) A 20.5. ábrán bemutatjuk, hogy hogyan néz ki az eloszlás különböző <span class="emphasis"><em>a</em></span>-k és <span class="emphasis"><em>b</em></span>-k esetén. Az eloszlás átlaga <span class="emphasis"><em>a</em></span>/(<span class="emphasis"><em>a +</em></span> <span class="emphasis"><em>b</em></span>), tehát nagyobb <span class="emphasis"><em>a</em></span> értékek arra utalnak, hogy Θ-t 1-hez közelebb hisszük, mint 0-hoz. Az <span class="emphasis"><em>a +</em></span> <span class="emphasis"><em>b</em></span> nagyobb értékei az eloszlást csúcsosabbá teszik, ami a Θ értéke felőli nagyobb bizonyosságunkat jelenti. Látható, hogy a béta-család a hipotézis prior lehetőségek hasznos választékát nyújtja.</p><p>Rugalmasságán túl a béta-családnak van még egy csodálatos tulajdonsága: ha a Θ priorja béta[<span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>b</em></span>], akkor egy adatpont megfigyelése után Θ a posteriori eloszlása is béta-eloszlás. A béta-családot a logikai változók eloszlása <span class="strong"><strong>konjugált prior</strong></span>jának (<span class="strong"><strong>conjugate prior</strong></span>) nevezzük.<sup>[<a id="id740696" href="#ftn.id740696" class="footnote">198</a>]</sup> Lássuk, hogyan is működik ez. Tegyük fel, hogy megfigyeltünk egy meggyízű cukrot, ekkor:</p><p><code class="code">P(<em><span class="remark">θ</span></em>|<em><span class="remark">D</span></em><sub>1</sub> = <em><span class="remark">meggy</span></em>) = <em><span class="remark">αP</span></em>(<em><span class="remark">D<sub>1</sub> = meggy</span></em>|<em><span class="remark">θ</span></em>)<em><span class="remark">P</span></em>(<em><span class="remark">θ</span></em>)</code></p><p><code class="code">		     = <em><span class="remark">α</span></em>'<em><span class="remark">θ</span></em> ·<em><span class="remark"> </span></em>beta[<em><span class="remark">a</span></em>,<em><span class="remark">b</span></em>](<em><span class="remark">θ</span></em>) =<em><span class="remark"> α</span></em>'<em><span class="remark">θ </span></em>·<em><span class="remark"> θ<sup>a</sup></span></em><sup>–1</sup>(1–<em><span class="remark"> θ</span></em>)<sup><em><span class="remark">b</span></em>–1</sup></code></p><p><code class="code">		     = <em><span class="remark">α</span></em>'<em><span class="remark">θ<sup>a </sup></span></em>(1–<em><span class="remark"> θ</span></em>)<sup><em><span class="remark">b</span></em>–1 </sup>=<sup> </sup>beta[<em><span class="remark">a +</span></em> 1,<em><span class="remark">b</span></em>](<em><span class="remark">θ</span></em>)</code></p><p>Tehát egy meggyízű cukrot észlelve egyszerűen inkrementáljuk az <span class="emphasis"><em>a</em></span> paramétert, hasonlóképpen, ha egy citromízűt észlelünk, akkor inkrementáljuk a <span class="emphasis"><em>b</em></span> paramétert – ezzel megkapjuk az a posteriori eloszlást. Ezek szerint úgy tekinthetünk <span class="emphasis"><em>a</em></span>-ra és <span class="emphasis"><em>b</em></span>-re, mint <span class="strong"><strong>virtuális számláló</strong></span>kra (<span class="strong"><strong>virtual count</strong></span>s), abban az értelemben, hogy a béta[<span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>b</em></span>] prior pontosan úgy viselkedik, mintha egyenletes eloszlású priorral indultunk volna, és <span class="emphasis"><em>a </em></span>– 1 meggycukorkát és <span class="emphasis"><em>b </em></span>– 1 citromízű cukorkát láttunk volna.</p><p>Tanulmányozva a béta-eloszlások sorozatát növekvő – de állandó arányú – <span class="emphasis"><em>a</em></span> és <span class="emphasis"><em>b</em></span> mentén, jól láthatjuk, hogyan változik a Θ paraméter a posteriori eloszlása az adatok beérkezése során. Tegyük fel például, hogy a vizsgált zacskó 75%-a meggytípusú. A 20.5. (b) ábra mutatja a béta[3, 1], béta[6, 2], béta[30, 10] eloszlássorozatot. Nyilvánvaló, hogy az eloszlás egy – a valós Θ körül elhelyezkedő – keskeny csúcs felé tart. Nagy adathalmazok esetén a Bayes-tanulás (legalábbis ebben az esetben) ugyanahhoz az eredményhez konvergál, mint amit a maximum-likelihood tanulás adott.</p><p>A 20.2. (b) ábrán a hálónak három paramétere volt: <span class="emphasis"><em>θ</em></span>, <span class="emphasis"><em>θ</em></span><sub>1</sub> és <span class="emphasis"><em>θ</em></span><sub>2</sub>, ahol <span class="emphasis"><em>θ</em></span><sub>1</sub> volt annak valószínűsége, hogy piros csomagolás van egy meggycukorkán, míg <span class="emphasis"><em>θ</em></span><sub>2</sub> az, hogy egy citromízű cukron van piros csomagolás. A Bayes hipotézis priornak mindhárom paramétert le kell fednie – azaz P(Θ, Θ<sub>1</sub>, Θ<sub>2</sub>)-t kell specifikálnunk. Rendszerint <span class="strong"><strong>paraméterfüggetlenség</strong></span>et (<span class="strong"><strong>parameter independence</strong></span>) tételezünk fel:</p><p><code class="code"><em><span class="remark">P</span></em>(Θ, Θ<sub>1</sub>, Θ<sub>2</sub>) = <em><span class="remark">P</span></em>(Θ)<em><span class="remark">P</span></em>(Θ<sub>1</sub>)<em><span class="remark">P</span></em>(Θ<sub>2</sub>)</code></p><p>Ezen feltételezés esetén minden egyes paraméternek saját béta-eloszlása lehet, amelyet külön-külön frissíthetünk az adatok érkeztekor.</p><div class="figure"><a id="id740967"/><p class="title"><strong>20.6. ábra - Egy Bayes-tanulásnak megfelelő Bayes-háló. A Θ, Θ<sub>1</sub>, Θ<sub>2</sub> változók a posteriori eloszlásai kikövetkeztethetők az a priori eloszlásokból és az <span class="emphasis"><em>Íz<sub>i</sub></em></span>, <span class="emphasis"><em>Csomagoló<sub>i</sub></em></span> változókra vonatkozó tényekből.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-06.png" alt="Egy Bayes-tanulásnak megfelelő Bayes-háló. A Θ, Θ1, Θ2 változók a posteriori eloszlásai kikövetkeztethetők az a priori eloszlásokból és az Ízi, Csomagolói változókra vonatkozó tényekből."/></div></div></div><p>Ha már az volt az ötletünk, hogy az ismeretlen paramétereket valószínűségi változókkal reprezentáljuk – amilyen például Θ –, akkor természetes módon adódik, hogy azokat beépítsük magába a Bayes-hálóba. Ahhoz, hogy ezt megtehessük, minden egyes példa leírásához másolatot kell készítenünk a változókról. Ha például három cukorkát figyeltünk meg, akkor szükségünk van a következő változókra: <span class="emphasis"><em>Íz</em></span><sub>1</sub>, <span class="emphasis"><em>Íz</em></span><sub>2</sub>, <span class="emphasis"><em>Íz</em></span><sub>3</sub> és <span class="emphasis"><em>Csomagoló</em></span><sub>1</sub>, <span class="emphasis"><em>Csomagoló</em></span><sub>2</sub>, <span class="emphasis"><em>Csomagoló</em></span><sub>3</sub>. A Θ paraméterváltozó határozza meg minden egyes <span class="emphasis"><em>Íz<sub>i</sub></em></span> változó valószínűségét:</p><p><code class="code"><em><span class="remark">P</span></em>(<em><span class="remark">Íz<sub>i</sub></span></em> = <em><span class="remark">meggy</span></em>|Θ = <em><span class="remark">θ</span></em>)<em><span class="remark"> = θ</span></em></code></p><p>Hasonlóképpen a csomagoló valószínűsége Θ<sub>1</sub>-től és Θ<sub>2</sub>-től függ. Például:</p><p><code class="code"><em><span class="remark">P</span></em>(<em><span class="remark">Csomagoló<sub>i</sub></span></em> = <em><span class="remark">piros</span></em>|<em><span class="remark">Íz<sub>i</sub></span></em> = <em><span class="remark">meggy</span></em>, Θ<sub>1 </sub>= <em><span class="remark">θ</span></em><sub>1</sub>) = <em><span class="remark">θ</span></em><sub>1</sub></code></p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Ezek után az egész Bayes-tanulási folyamat formalizálható egy megfelelően konstruált Bayes-háló következtetési problémájaként, amint a 20.6. ábrán látható. Egy új példány predikciója egyszerűen azt jelenti, hogy új példányváltozókat adunk a hálóhoz, amelyből egyesekre rákérdezünk. A tanulás és a predikció ezen formalizmusa nyilvánvalóvá teszi, hogy a Bayes-tanuláshoz nem kell semmilyen extra „tanulási elv”. Megállapíthatjuk továbbá, hogy <span class="emphasis"><em>lényegében csak egyetlen tanulási algoritmus van</em></span>, ami a Bayes-háló következtetési algoritmusa.</p></div></div><div class="section" title="Bayes-hálóstruktúrák tanulása"><div class="titlepage"><div><div><h2 class="title"><a id="id741373"/>Bayes-hálóstruktúrák tanulása</h2></div></div></div><p>Az eddigiekben azt feltételeztük, hogy a Bayes-háló struktúrája ismert, és csak a paramétereket próbáljuk megtanulni. A háló struktúrája a terület alapvető oksági viszonyaira vonatkozó tudást reprezentálja, amit sok esetben egy szakember, de még egy naiv felhasználó is, nagyon egyszerűen meg tud adni. Néhány esetben azonban az oksági összefüggések nem állnak rendelkezésre vagy vitatottak – például bizonyos nagyvállalatok régóta állítják, hogy a dohányzás nem okoz rákot. Ilyenkor fontos megérteni, hogy a Bayes-háló struktúrája mi módon tanulható meg az adatokból. Jelenleg a struktúratanulási algoritmusok gyerekcipőben járnak, ezért csak egy elnagyolt vázlatot adunk a legfontosabb elvekről.</p><p>A legkézenfekvőbb megközelítés, ha egy jó modell érdekében<span class="emphasis"><em> keresést</em></span> folytatunk. Elindulhatunk egy kapcsolatokat nem tartalmazó modellel, majd elkezdünk szülőcsomópontot adni minden csomóponthoz, az előbbiekben bemutatott módszerekkel illesztve a paramétereket, és mérjük a modell pontosságát. Másik lehetőség, hogy egy becsült struktúrával indulunk, és hegymászó vagy szimulált lehűtést alkalmazó algoritmusokat használunk a módosításokhoz, minden egyes struktúraváltoztatás után újrahangolva a paramétereket. A módosítások közé tartozik az élek megfordítása, hozzáadása, törlése. Nem szabad ciklusokat létrehoznunk a folyamat során, ezért sok algoritmus azt feltételezi, hogy adott a változók egy rendezése, és egy csomópont szülője csak azon csomópontok közül kerülhet ki, amelyek előbb jönnek a rendezésben (éppúgy, mint a 14. fejezet konstrukciós eljárásában). A teljes általánosság kedvéért a lehetséges rendezések között is keresnünk kell.</p><p>Két alternatív módszer van arra, hogy észrevegyük, amikor egy jó megoldást találtunk. Az első annak tesztelése, hogy az aktuális adatok kielégítik-e azokat a feltételes függetlenségre vonatkozó állításokat, amelyek a struktúrában implicit módon benne vannak. Például az étterem probléma egy naiv Bayes-modellje feltételezi, hogy:</p><p><code class="code"><em><span class="remark">P</span></em>(<em><span class="remark">Péntek/Szombat</span></em>, <em><span class="remark">Bár</span></em>|<em><span class="remark">VárjunkE</span></em>) <em><span class="remark">P</span></em>(<em><span class="remark">Péntek/Szombat</span></em>|<em><span class="remark">VárjunkE</span></em>)<em><span class="remark">P(Bár</span></em>|<em><span class="remark">VárjunkE</span></em>)</code></p><p>és leellenőrizhetjük az adatokon, hogy ugyanez az egyenlet fennáll-e a megfelelő feltételes gyakoriságok közt is. Viszont még akkor is, ha a struktúra a terület valós oksági természetét írja le, az adathalmaz – a statisztikai ingadozások miatt – az egyenletet soha nem elégíti ki <span class="emphasis"><em>pontosan.</em></span> Ezért megfelelő statisztikai próbákat kell elvégeznünk, hogy eldöntsük: elég bizonyítékunk van-e arra, hogy a függetlenségi hipotézisek sérülnek. Az eredményül kapott háló bonyolultsága az ezen tesztekben alkalmazott küszöbtől függ majd – minél szigorúbb a függetlenségi teszt, annál több kapcsolatot adunk majd a struktúrához, és annál nagyobb lesz a túlilleszkedés veszélye.</p><p>A jelen fejezetben bemutatott gondolatoknak jobban megfelelő megközelítés annak értékelése, hogy a javasolt modell mennyire magyarázza meg az adatokat (valószínűségi értelemben). Mindamellett óvatosnak kell lennünk ennek mérésénél. Ha egyszerűen a maximum-likelihood hipotézist akarjuk megtalálni, akkor egy teljesen összekötött hálónál fogunk kikötni, mivel további szülőcsomópontok hozzáadása egy csomóponthoz nem csökkentheti a valószínűséget (lásd 20.9. feladat). Valamilyen módon büntetnünk kell a modell bonyolultságát. A MAP (vagy MLH) megközelítés egyszerűen levon egy büntetőtagot az egyes struktúrák valószínűségéből (miután hangolta paramétereiket), ezek után hasonlítja össze a különböző struktúrákat. A Bayes-megközelítés a struktúrák és paraméterek együttes priorját használja. Rendszerint túl sok – a változók számán szuperexponenciális – struktúra van ahhoz, hogy mind felett összegezzünk, így a gyakorlatban legtöbben az MCMC (Markov lánc Monte Carlo) módszert használják, hogy mintát vegyenek a struktúrákból.</p><p>A bonyolultság büntetése (akár MAP, akár Bayes-megközelítésben) fontos kapcsolatot hoz be az optimális struktúra és a feltételes valószínűségek hálóbeli reprezentációs módja között. Táblázatosan ábrázolt eloszlás esetén a bonyolultság büntetése a szülőcsomópontok számával exponenciálisan nő, míg, mondjuk, zajos-vagy eloszlások esetén csak lineárisan. Ez azt jelenti, hogy a zajos-vagy (illetve más tömören paraméterezett) modellek tanulása több szülőcsomópontot eredményez, mint a táblázatos eloszlás tanulása.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id740005" href="#id740005" class="para">196</a>] </sup> Nem táblázatos formára lásd a 20.7. feladatot, amelyben mindegyik paraméter hatással van számos feltételes valószínűségre.</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id740570" href="#id740570" class="para">197</a>] </sup> Azért hívjuk hiperparamétereknek, mert <span class="emphasis"><em>θ</em></span> eloszlásának paramétereiről van szó, ahol <span class="emphasis"><em>θ</em></span> maga is egy paraméter.</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id740696" href="#id740696" class="para">198</a>] </sup> További konjugált priorok: a diszkrét többváltozós eloszlások paramétereire a <span class="strong"><strong>Dirichlet</strong></span> család, a Gauss-eloszlások paramétereire a <span class="strong"><strong>Normal–Wishart</strong></span> család. Lásd Bernardo és Smith (1994).</p></div></div></div></body></html>
