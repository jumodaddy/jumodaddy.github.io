<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Eljárásmód-iteráció"><div class="titlepage"><div><div><h1 class="title"><a id="id714742"/>Eljárásmód-iteráció</h1></div></div></div><p>Az előző alfejezetben megfigyeltük, hogy optimális eljárásmódot akkor is kaphatunk, amikor a hasznosságfüggvény becslése pontatlan. Ha egy cselekvés egyértelműen jobb, mint a többi, akkor a releváns állapotok hasznosságainak a pontos nagyságát nem szükséges precízen tudnunk. Ez a megérzés egy alternatív módot javasol az optimális eljárásmód megkeresésére. Az <span class="strong"><strong>eljárásmód-iteráció</strong></span>s (<span class="strong"><strong>policy iteration</strong></span>) algoritmus egy <span class="emphasis"><em>π</em></span><sub>0 </sub>kezdeti eljárásmódtól indulva a következő két lépést váltogatja:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="strong"><strong>Eljárásmód-értékelés</strong></span> (<span class="strong"><strong>policy evaluation</strong></span>): Egy adott <span class="emphasis"><em>π</em></span><sub><span class="emphasis"><em>i</em></span> </sub>eljárásmódnál számítsuk ki <span class="inlinemediaobject"><img src="math/mi-17-0016.gif" alt="Eljárásmód-iteráció"/></span>-t, az egyes állapotok hasznosságát mintha <span class="emphasis"><em>π<sub>i</sub></em></span> volna végrehajtva.</p></li><li class="listitem"><p><span class="strong"><strong>Eljárásmód-javítás</strong></span> (<span class="strong"><strong>policy improvement</strong></span>): Számítsunk ki egy új <span class="emphasis"><em>π</em></span><sub><span class="emphasis"><em>i</em></span>+1</sub> MVH-eljárásmódot, felhasználva az <span class="emphasis"><em>U<sub>i</sub></em></span>-n alapuló egylépéses előrenézést (mint a (17.4) egyenletben).</p></li></ul></div><p>Az algoritmus leáll, amikor az eljárásmód-javítás lépése nem eredményez változást a hasznosságokban. Ennél a pontnál tudjuk, hogy az <span class="emphasis"><em>U<sub>i</sub></em></span> hasznosságfüggvény a Bellman-frissítés egy fix pontja, így a Bellman-egyenletek egy megoldása, és <span class="emphasis"><em>π<sub>i</sub></em></span>-nek egy optimális eljárásmódnak kell lennie. Mivel csak véges sok eljárásmód létezik a véges állapottérben, és megmutatható, hogy minden iteráció jobb eljárásmódot eredményez, az eljárásmód-iterációnak le kell állnia. Az algoritmus a 17.7. ábrán látható.</p><p>Az eljárásmód-javítás lépése nyilvánvalóan egyértelmű, de hogyan valósítsuk meg az eljárásmód-értékelést. Kiderül, hogy ennek elvégzése sokkal egyszerűbb, mint a szabványos Bellman-egyenletek megoldása (amit az értékiteráció végez el), mivel az eljárásmód egy cselekvést minden állapotban rögzít. Az <span class="emphasis"><em>i</em></span>-edik iterációban a <span class="emphasis"><em>π<sub>i</sub></em></span> eljárásmód a <span class="emphasis"><em>π<sub>i</sub></em></span>(<span class="emphasis"><em>s</em></span>) cselekvést írja elő. Ez azt jelenti, hogy a (17.5) Bellman-egyenlet egy egyszerűsített változatával állunk szemben, ami az <span class="emphasis"><em>s</em></span> hasznosságát (<span class="emphasis"><em>π<sub>i</sub></em></span> mellett) a következőképpen kapcsolja a szomszédai hasznosságához:</p><p><span class="inlinemediaobject"><img src="math/mi-17-0017.gif" alt="Eljárásmód-iteráció"/></span></p><p>Például tegyük fel, hogy <span class="emphasis"><em>π<sub>i</sub></em></span> a 17.2. (a) ábrán látható eljárásmód. Ekkor azt kapjuk, hogy <span class="emphasis"><em>π<sub>i</sub></em></span>(1, 1) = <span class="emphasis"><em>Fel</em></span>, <span class="emphasis"><em>π<sub>i</sub></em></span>(1, 2) = <span class="emphasis"><em>Fel</em></span> és így tovább, illetve az egyszerűsített Bellman-egyenletek:</p><p><code class="code"><em><span class="remark">U<sub>i</sub></span></em>(1, 1) = –0,04 + 0,8<em><span class="remark">U<sub>i</sub></span></em>(1, 2) + 0,1<em><span class="remark">U<sub>i</sub></span></em>(1, 1) + 0,1<em><span class="remark">U<sub>i</sub></span></em>(2, 1)</code></p><p><code class="code"><em><span class="remark">U<sub>i</sub></span></em>(1, 2) = –0,04 + 0,8<em><span class="remark">U<sub>i</sub></span></em>(1, 3) + 0,2<em><span class="remark">U<sub>i</sub></span></em>(1, 2)</code></p><p><code class="code">…</code></p><p>A lényeges az, hogy ezek az egyenletek lineárisak, mivel a „max” operátort eltávolítottuk. Ha az állapotok száma <span class="emphasis"><em>n</em></span>,<span class="emphasis"><em> n</em></span> lineáris egyenlet adódik <span class="emphasis"><em>n</em></span> ismeretlennel, ami egzakt módon megoldható <span class="emphasis"><em>O</em></span>(<span class="emphasis"><em>n</em></span><sup>3</sup>) időben standard lineáris algebrai módszerekkel.</p><div class="figure"><a id="id714986"/><p class="title"><strong>17.7. ábra - Az eljárásmód-iterációs algoritmus egy optimális eljárásmód kiszámítására</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/17-07.png" alt="Az eljárásmód-iterációs algoritmus egy optimális eljárásmód kiszámítására"/></div></div></div><a id="ID_720_oldal"/><p class="Tartalom3">Kis állapotterekre az eljárásmód-kiértékelés egzakt megoldómódszerekkel gyakran a leghatékonyabb megközelítés. Nagy állapotterekre az <span class="emphasis"><em>O</em></span>(<span class="emphasis"><em>n</em></span><sup>3</sup>)<span class="emphasis"><em> </em></span>idő megengedhetetlen lehet. Szerencsére nem szükséges egzakt eljárásmód-kiértékelést végezni. Ehelyett elvégezhetünk bizonyos számú egyszerűsített értékiterációs lépést (egyszerűsített, mivel az eljárásmód rögzített), hogy a hasznosságok elfogadhatóan jó becsléséhez jussunk. Az egyszerűsített Bellman-frissítés ehhez a folyamathoz a következő</p><p><span class="inlinemediaobject"><img src="math/mi-17-0018.gif" alt="Az eljárásmód-iterációs algoritmus egy optimális eljárásmód kiszámítására"/></span></p><p>amit <span class="emphasis"><em>k</em></span>-szor ismétlünk, hogy a következő hasznosságbecslés előálljon. Az adódó algoritmust <span class="strong"><strong>módosított eljárásmód-iteráció</strong></span>nak (<span class="strong"><strong>modified policy iteration</strong></span>) nevezzük. Ez gyakran sokkal hatékonyabb, mint a szabványos eljárásmód-iteráció vagy értékiteráció.</p><p>Az eddig leírt algoritmusok megkövetelik a hasznosságok vagy eljárásmódok összes állapotbeli egyszerre történő frissítését. Megmutatható, hogy ez nem szigorúan szükséges. Valójában minden iterációnál kiválaszthatjuk az állapotok egy <span class="emphasis"><em>tetszőleges</em></span> részhalmazát, és alkalmazhatjuk rá <span class="emphasis"><em>bármelyik</em></span> fajta frissítést (eljárásmód-javítást vagy egyszerűsített értékiterációt). Ezt az általánosított algoritmust <span class="strong"><strong>aszinkron eljárásmód-iteráció</strong></span>nak (<span class="strong"><strong>asynchronous policy iteration</strong></span>) nevezzük. A kezdeti eljárásmód és a hasznosságfüggvény adott feltételek melletti megválasztásánál az aszinkron eljárásmód-iteráció garantáltan konvergál az optimális eljárásmódhoz. A feldolgozandó állapotok megválasztásának szabadsága azt jelenti, hogy hatékonyabb heurisztikus algoritmusokat tervezhetünk – például olyan algoritmusokat, amelyek azon állapotok értékének frissítésére koncentrálnak, amelyeket egy jó eljárásmód valószínűleg elér. A valós életben ez nagyon hasznosnak tűnik: ha valakinek nincs szándékában a mélybe vetni magát egy szikláról, akkor nem kell aggódással töltenie az időt a bekövetkező állapotok pontos értékéről.</p></div></body></html>
