<?xml version="1.0" encoding="UTF-8" standalone="no"?>

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Döntési fák megalkotása tanulással"><div class="titlepage"><div><div><h1 class="title"><a id="id721218"/>Döntési fák megalkotása tanulással</h1></div></div></div><p>A döntési fa tanulás egyike a legegyszerűbb, mégis az eddigiekben az egyik legsikeresebbnek bizonyult tanulási algoritmusnak. Jó bevezetésként szolgál az induktív tanulás területén, és ráadásul könnyen implementálható. Először bemutatjuk a cselekvő alrendszert, majd megmutatjuk, hogy miképpen lehet tanítani. Mindeközben olyan elveket mutatunk be, amelyek az induktív tanulás minden területére jellemzők.</p><div class="section" title="Cselekvő komponensként használt döntési fák"><div class="titlepage"><div><div><h2 class="title"><a id="id721224"/>Cselekvő komponensként használt döntési fák</h2></div></div></div><p>Egy <span class="strong"><strong>döntési fa</strong></span> (<span class="strong"><strong>decision tree</strong></span>) bemenetként egy <span class="strong"><strong>attribútum</strong></span>okkal (<span class="strong"><strong>attribute</strong></span>s) leírt objektumot vagy szituációt kap, és egy „döntést” ad vissza eredményként – a bemenetre adott válasz jósolt értékét. A bemeneti attribútumok lehetnek diszkrétek vagy folytonosak. Jelen tárgyalásban diszkrét bemeneteket tételezünk fel. A kimeneti érték szintén lehet diszkrét vagy folytonos; egy diszkrét értékkészletű függvény tanulását <span class="strong"><strong>osztályozás</strong></span> (<span class="strong"><strong>classification</strong></span>) tanulásnak, míg a folytonos függvény tanulását <span class="strong"><strong>regresszió</strong></span>nak (<span class="strong"><strong>regression</strong></span>) nevezzük.<span class="emphasis"><em> Bináris (Boolean)</em></span> osztályozásra fogunk koncentrálni, ahol minden példát vagy <span class="strong"><strong>igaznak </strong></span>(<span class="strong"><strong>pozitív</strong></span>), vagy <span class="strong"><strong>hamisnak</strong></span> (<span class="strong"><strong>negatív</strong></span>) sorolunk be.</p><p>A döntési fa egy tesztsorozat elvégzése során jut el a döntéshez. A fa minden egyes belső csomópontja valamely tulajdonság értékére vonatkozó tesztnek felel meg, a csomópontból kilépő ágakat pedig a teszt lehetséges kimeneteivel címkézzük. Minden egyes levélcsomópont megadja azt az értéket, amelyet vissza kell adnunk, ha ezt a levelet elértük. Úgy tűnik, hogy a döntési fa reprezentáció az emberek számára rendkívül természetes; valójában számos „Hogyan csináljuk?” kézikönyvet (például az autójavítási kézikönyveket) írtak meg egyetlen hatalmas, több száz oldalra kiterjedő döntési faként.</p><p>Valamelyest egyszerűbb példa lehet az a probléma, hogy várjunk-e egy étteremben egy asztal felszabadulására. Az a célunk, hogy tanulással kialakítsuk a <span class="emphasis"><em>VárjunkE</em></span> <span class="strong"><strong>célpredikátum</strong></span> (<span class="strong"><strong>goal predicate</strong></span>) definícióját. Ahhoz, hogy ezt tanulási feladatként kezelhessük, először meg kell határozzuk, hogy milyen attribútumok állnak rendelkezésre ahhoz, hogy ezen a problématerületen leírjuk a példákat. A 19. fejezetben megmutatjuk, hogyan lehet automatizálni ezt a feladatot; most egyszerűen tegyük fel, hogy a következő attribútumlista mellett döntöttünk:</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p><span class="emphasis"><em>Alternatíva</em></span>: van-e a közelben megfelelő alternatívaként kínálkozó étterem.</p></li><li class="listitem"><p><span class="emphasis"><em>Bár</em></span>: van-e az étteremnek kényelmes bár része, ahol várakozhatunk.</p></li><li class="listitem"><p><span class="emphasis"><em>Péntek/Szombat</em></span>: igaz értéket vesz fel pénteken és szombaton.</p></li><li class="listitem"><p><span class="emphasis"><em>Éhes</em></span>: éhesek vagyunk-e.</p></li><li class="listitem"><p><span class="emphasis"><em>Vendégek</em></span>: hány ember van az étteremben (értékkészlet <span class="emphasis"><em>Senki</em></span>, <span class="emphasis"><em>Néhány</em></span> és <span class="emphasis"><em>Tele</em></span>).</p></li><li class="listitem"><p><span class="emphasis"><em>Drága</em></span>: az étterem menyire drága ($, $$, $$$).</p></li><li class="listitem"><p><span class="emphasis"><em>Eső</em></span>: esik-e odakint az eső.</p></li><li class="listitem"><p><span class="emphasis"><em>Foglalás</em></span>: foglaltunk-e asztalt.</p></li><li class="listitem"><p><span class="emphasis"><em>Konyha</em></span>: az étterem típusa (francia, olasz, thai vagy burger).</p></li><li class="listitem"><p><span class="emphasis"><em>BecsültVárakozás</em></span>: a pincér becsülte várakozási idő (0–10, 10–30, 30–60, &gt;60 perc)</p></li></ol></div><div class="figure"><a id="id721384"/><p class="title"><strong>18.2. ábra - Egy döntési fa annak eldöntésére, hogy várjunk-e asztalra</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/18-02.png" alt="Egy döntési fa annak eldöntésére, hogy várjunk-e asztalra"/></div></div></div><p>A szerzők egyike (SR) által erre a problémára rendszerint használt döntési fa a 18.2. ábrán látható. Vegyük észre, hogy a fa nem használja a <span class="emphasis"><em>Drága</em></span> és a <span class="emphasis"><em>Konyha</em></span> attribútumokat, valójában irrelevánsnak tekinti azokat. A példákat a döntési fa a gyökérnél kezdi feldolgozni, követi a megfelelő ágakat, amíg egy levélhez el nem ér. Például egy <span class="emphasis"><em>Vendégek</em></span> = <span class="emphasis"><em>Tele</em></span> és <span class="emphasis"><em>BecsültVárakozás </em></span>= 0–10 attribútumokkal jellemezhető példa pozitív kimenetet fog eredményezni (azaz várni fogunk egy asztalra).</p></div><div class="section" title="A döntési fák kifejezőképessége"><div class="titlepage"><div><div><h2 class="title"><a id="id721414"/>A döntési fák kifejezőképessége</h2></div></div></div><p>Logikai felírást használva minden egyes döntési fa, amely a <span class="emphasis"><em>VárjunkE</em></span> célpredikátum egy hipotézise, a következő formában felírt állításnak felel meg:</p><p><code class="code">∀<em><span class="remark">s</span></em>  <em><span class="remark">VárjunkE</span></em>(<em><span class="remark">s</span></em>) ⇔<em><span class="remark"> </span></em>(<em><span class="remark">P</span></em><sub>1</sub>(<em><span class="remark">s</span></em>) ∨ <em><span class="remark">P</span></em><sub>2</sub>(<em><span class="remark">s</span></em>) ∨ … ∨ <em><span class="remark">P<sub>n</sub></span></em>(<em><span class="remark">s</span></em>))</code></p><p>ahol mindegyik <span class="emphasis"><em>P<sub>i</sub></em></span>(<span class="emphasis"><em>s</em></span>) feltétel azon tesztek konjunkciójának felel meg, amelyeket a gyökértől egy pozitív kimenetet jelentő levélig megtett út során végeztünk. Bár ez egy elsőrendű logikában felírt mondatnak tűnik, valójában bizonyos értelemben ítéletlogikai kifejezés, mivel csak egyetlen változót tartalmaz és az összes predikátum unáris. Valójában a döntési fa a <span class="emphasis"><em>VárjunkE</em></span> és az attribútumértékek bizonyos logikai kapcsolatát írja le. A döntési fa nem használható olyan tesztek reprezentálására, amelyek kettő vagy több különböző objektumra vonatkoznak – például:</p><p><code class="code">∃<em><span class="remark">r</span></em><sub>2</sub>  <em><span class="remark">Közel</span></em>(<em><span class="remark">r</span></em><sub>2</sub>,<em><span class="remark"> r</span></em>) ∧ <em><span class="remark">Drága</span></em>(<em><span class="remark">r</span></em>,<em><span class="remark"> p</span></em>) ∧ <em><span class="remark">Drága</span></em>(<em><span class="remark">r</span></em><sub>2</sub>,<em><span class="remark"> p</span></em><sub>2</sub>) ∧ <em><span class="remark">Olcsóbb</span></em>(<em><span class="remark">p</span></em><sub>2</sub>,<em><span class="remark"> p</span></em>)</code></p><p>(van a közelben egy olcsóbb étterem?). Nyilvánvalóan felvehetnénk egy <span class="emphasis"><em>OlcsóbbÉtteremKözel</em></span> nevű logikai attribútumot, de az <span class="emphasis"><em>összes</em></span> ilyen attribútum hozzáadása kezelhetetlenné teszi a problémát. A 19. fejezet mélyebbre hatol az elsőrendű logikában történő tanulás területén.</p><p>Az ítéletlogikai nyelvek területén a döntési fák teljes kifejezőképességgel bírnak, ami azt jelenti, hogy tetszőleges logikai (Boole) függvény felírható döntési faként. Ezt triviálisan megvalósíthatjuk, ha a függvény igazságtáblájának minden sorát megfeleltetjük a döntési fa egy útjának. Ez exponenciálisan növekvő döntési fára vezet, mivel az igazságtábla exponenciálisan növekvő számú sort tartalmaz. Nyilvánvalóan a döntési fák sok függvényt jóval kisebb fával képesek reprezentálni.</p><p>Ugyanakkor némely függvényfajtánál ez valóban problémát jelent. Például ha a függvényünk a <span class="strong"><strong>paritásfüggvény</strong></span> (<span class="strong"><strong>parity function</strong></span>), amely akkor és csak akkor ad 1-et, ha páros számú bemenet 1 értékű, akkor egy exponenciálisan nagy döntési fára lesz szükség. Hasonlóan nehéz a <span class="strong"><strong>többségfüggvény</strong></span> (<span class="strong"><strong>majority function</strong></span>) reprezentálása döntési fával, amely függvény akkor ad 1-et, ha bemeneteinek több mint fele 1 értékű.</p><p>Más szavakkal, a döntési fák bizonyos függvények esetén jók, mások esetén roszszak. Van <span class="emphasis"><em>bármilyen</em></span> olyan reprezentáció, amely <span class="emphasis"><em>minden</em></span>fajta függvény esetén hatékony? A válasz sajnos az, hogy nincs. Ezt általánosan is meg tudjuk mutatni. Vizsgáljuk az összes <span class="emphasis"><em>n</em></span> bemeneti attribútummal rendelkező logikai (Boole) függvényt. Hány különböző függvény van ebben a halmazban? Ez éppen a lehetséges felírható igazságtáblák számával egyezik meg, hiszen egy függvényt az igazságtáblája ad meg. Az igazságtáblának 2<sup>n</sup> sora van, mivel minden bemeneti esetet <span class="emphasis"><em>n</em></span> attribútummal adunk meg. Úgy tekinthetjük a tábla „válasz” oszlopát, mint egy 2<sup>n</sup> bites számot, amely definiálja a függvényt. Mindegy, hogy a függvények milyen reprezentációját választjuk, néhánynak (valójában szinte mindegyiknek) a reprezentálásához tényleg szükség lesz ennyi bitre.</p><p>Ha 2<sup>n</sup> bitre van szükség a függvény megadásához, akkor <span class="emphasis"><em>n</em></span> attribútum esetén <span class="inlinemediaobject"><img src="math/mi-18-0001.gif" alt="A döntési fák kifejezőképessége"/></span> a lehetséges függvények száma. Ez megrázóan nagy szám. Például csupán hat Boole-változó esetén is <span class="inlinemediaobject"><img src="math/mi-18-0002.gif" alt="A döntési fák kifejezőképessége"/></span> különböző logikai (Boole) függvény állítható elő. Szükségünk lesz néhány szellemes algoritmusra, hogy egy ilyen hatalmas térben konzisztens hipotézist tudjunk találni.</p></div><div class="section" title="A döntési fák példák alapján történő felépítése"><div class="titlepage"><div><div><h2 class="title"><a id="id721633"/>A döntési fák példák alapján történő felépítése</h2></div></div></div><p>Egy logikai döntési fa által kezelhető példa a bemeneti attribútumok<span class="emphasis"><em> X</em></span> vektorából és egyetlen logikai kimeneti értékből, <span class="emphasis"><em>y</em></span>-ból áll. A 18.3. ábra egy (<span class="emphasis"><em>X</em></span><sub>1</sub>, <span class="emphasis"><em>y</em></span><sub>1</sub>)...(<span class="emphasis"><em> X</em></span><sub>12</sub>, <span class="emphasis"><em>y</em></span><sub>12</sub>) példahalmazt mutat. Azokat nevezzük pozitív példáknak (<span class="emphasis"><em>X</em></span><sub>1</sub>, <span class="emphasis"><em>X</em></span><sub>3</sub>, ...), amelyekben a <span class="emphasis"><em>VárjunkE</em></span> értéke igaz, és azok a negatív példák, amelyekben az értéke hamis (<span class="emphasis"><em>X</em></span><sub>2</sub>, <span class="emphasis"><em>X</em></span><sub>5</sub>, …). A példák teljes halmazát <span class="strong"><strong>tanító halmaz</strong></span>nak (<span class="strong"><strong>training set</strong></span>) nevezzük.</p><p>Az a probléma, hogy a tanító halmaznak megfelelő döntési fát találjunk, bonyolultnak tűnik ugyan, de valójában van egy triviális megoldása. Egyszerűen egy olyan fát konstruálhatunk, amelyben minden példához egy külön saját utat hozunk létre egy – a példához tartozó – levélcsomóponthoz. A levélhez vezető út mentén sorra teszteljük az attribútumokat, és a példához tartozó tesztértéket követjük, a levél pedig a példa besorolását adja. Ha ismét ugyanazt a példát<sup>[<a id="id721711" href="#ftn.id721711" class="footnote">183</a>]</sup> vesszük, akkor a döntési fa a helyes osztályozási eredményt adja. Szerencsétlen módon nemigen ad információt egyetlen más esetről sem!</p><div class="figure"><a id="id721721"/><p class="title"><strong>18.3. ábra - Példák az étterem problématerületre</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/18-03.png" alt="Példák az étterem problématerületre"/></div></div></div><p class="Tartalom3">Ezzel a triviális megoldással az a baj, hogy egyszerűen memorizálja a példákat. Mivel nem nyer ki semmilyen mintázatot a példákból, ezért nem várhatjuk, hogy extrapolálni tudjon azokra a mintákra, amelyeket még sohasem látott. Az Ockham borotvája elvét alkalmazva nekünk azt a <span class="emphasis"><em>legkisebb</em></span> döntési fát kell megtalálnunk, amely konzisztens a példákkal. Sajnálatos módon a „legkisebb” összes értelmes definíciója esetén a legkisebb döntési fa megtalálása kezelhetetlen problémát jelent. Viszont némi egyszerű heurisztika bevetésével jó eredményt érhetünk el egy „kicsike” fa megtalálásában. A <code class="code">DÖNTÉSI-FA-TANULÁS</code> algoritmus alapötlete az, hogy teszteljük először a legfontosabb attribútumot. A „legfontosabb” alatt azt az attribútumot értjük, amelyik a legnagyobb változást okozza a példák besorolásában. Ezen a módon – reményünk szerint – kisszámú teszttel helyes osztályozáshoz jutunk, ami azt jelenti, hogy a fában minden út rövid lesz, tehát az egész fa kicsi lesz.</p><p class="Tartalom3">A 18.4. ábrán látható, hogy hogyan indul az algoritmus. 12 példánk van, amelyeket pozitív és negatív példahalmazokba sorolunk. Ezek után eldöntjük, hogy melyik attribútumot teszteljük először a fában. A 18.4. (a) ábrán megmutattuk, hogy a <span class="emphasis"><em>Konyha</em></span> attribútum rossz választás lenne, mert tesztjének 4 kimenetele van, és mindegyik esetén ugyannyi pozitív és negatív példánk lesz az eredményül kapott halmazokban. Másrészt viszont a 18.4. (b) ábrán látható, hogy a <span class="emphasis"><em>Vendégek</em></span> egy meglehetősen fontos attribútum, hiszen ha értéke <span class="emphasis"><em>Senki</em></span> vagy <span class="emphasis"><em>Néhány</em></span>, akkor határozott választ tudunk adni (az első esetben <span class="emphasis"><em>Nem</em></span>, a másodikban<span class="emphasis"><em> Igen</em></span> a válaszunk). Ha az attribútum értéke <span class="emphasis"><em>Tele</em></span>, akkor a példák vegyes halmazát kapjuk. Általánosságban megállapítható, hogy miután az első attribútum tesztje csoportokra bontotta a példákat, mindegyik teszteredmény egy újabb döntési fa tanulási problémát eredményez, kevesebb példával és egygyel kevesebb attribútummal. Négy esetet kell áttekintenünk ezekben a rekurzív problémákban:</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p>Ha van néhány pozitív és néhány negatív példánk, akkor válasszuk a legjobb attribútumot a szétosztásukra. A 18.4. (b) ábrán bemutattuk, hogy az <span class="emphasis"><em>Éhes</em></span> attribútum alkalmas a megmaradó példák osztályozására.</p></li><li class="listitem"><p>Ha valamennyi megmaradt példánk pozitív (vagy mind negatív), akkor készen vagyunk: válaszolhatunk <span class="emphasis"><em>Igen</em></span>-t vagy <span class="emphasis"><em>Nem</em></span>-et. A 18.4. (b) ábra bemutatja ezt a <span class="emphasis"><em>Senki</em></span>, illetve a <span class="emphasis"><em>Néhány</em></span> esetekben.</p></li><li class="listitem"><p>Ha nem marad példa a teszt egyik kimenetele esetén, akkor ez azt jelenti, hogy nem figyeltünk meg ilyen esetet, és a szülőcsomópontban többségben levő választ adjuk.</p></li><li class="listitem"><p>Ha nem maradt attribútumunk, amelyet tesztelhetnénk, de mind pozitív, mind negatív példáink maradtak, akkor bajban vagyunk. Ez azt jelenti, hogy ezeknek a példáknak pontosan azonos jellemzőik vannak, de különböző osztályokba tartoznak. Ez egyrészt akkor fordulhat elő, ha néhány adat nem megfelelő, azt mondjuk, hogy <span class="strong"><strong>zajos</strong></span>ak (<span class="strong"><strong>noise</strong></span>) az adatok. Másrészt akkor is előállhat ez a helyzet, ha az attribútumok nem adnak elég információt a szituáció teljes leírására, vagy a problématér valójában nemdeterminisztikus. Egyszerű megoldása lehet ennek a problémának a többségi szavazás használata.</p></li></ol></div><div class="figure"><a id="id721816"/><p class="title"><strong>18.4. ábra - A példahalmaz attribútumteszteléssel történő szétosztása. (a) A <span class="emphasis"><em>Konyha</em></span> alapján történő szétosztás nem visz közelebb ahhoz, hogy a pozitív és negatív példákat megkülönböztessük. (b) A <span class="emphasis"><em>Vendégek</em></span> attribútum tesztje jól szeparálja a pozitív és negatív példákat. Miután a <span class="emphasis"><em>Vendégek</em></span> tesztjét elvégeztük, az <span class="emphasis"><em>Éhes</em></span> egy eléggé jó második tesztlehetőséget ad.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/18-04.png" alt="A példahalmaz attribútumteszteléssel történő szétosztása. (a) A Konyha alapján történő szétosztás nem visz közelebb ahhoz, hogy a pozitív és negatív példákat megkülönböztessük. (b) A Vendégek attribútum tesztje jól szeparálja a pozitív és negatív példákat. Miután a Vendégek tesztjét elvégeztük, az Éhes egy eléggé jó második tesztlehetőséget ad."/></div></div></div><p>A <code class="code">DÖNTÉSI-FA-TANULÁS</code> algoritmust a 18.5. ábrán mutatjuk be. Az <code class="code">ATTRIBÚTUM-VÁLASZTÁS</code> módszer bemutatása a következő alfejezetben található.</p><p>Az algoritmus által a 12 mintából álló adathalmaz alapján létrehozott fa a 18.6. ábrán látható. Az eredményként kapott fa nyilvánvalóan különbözik a 18.2. ábrán látható eredetitől, annak ellenére, hogy az adatokat egy, az eredeti fát használó ágens generálta. Arra gondolhatnánk, hogy az algoritmus nem ért el különösebben jó eredményt a helyes függvény megtanulásában. Mindamellett ez téves következtetés lenne. A tanuló algoritmus a <span class="emphasis"><em>példákat</em></span> látja, nem az eredeti függvényt. Valójában a létrehozott hipotézis (lásd 18.6. ábra) nem csupán megfelel az összes példának, de lényegesen egyszerűbb is, mint az eredeti fa. A tanuló algoritmusnak semmi oka sincs az <span class="emphasis"><em>Eső</em></span> és <span class="emphasis"><em>Foglalás</em></span> attribútumokra vonatkozó teszteket felvenni, mivel ezek nélkül is tudja osztályozni az összes példát. Mellesleg, felfedett egy érdekes és váratlan viselkedést: az első szerző hétvégeken várni fog thai ételekre.</p><div class="figure"><a id="id721864"/><p class="title"><strong>18.5. ábra - A döntési fa tanulásra szolgáló algoritmus</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/18-05.png" alt="A döntési fa tanulásra szolgáló algoritmus"/></div></div></div><p class="Tartalom3">Természetesen ha több mintát gyűjtöttünk volna, akkor egy, az eredetihez hasonlóbb fát hozhattunk volna létre az induktív tanulással. A 18.6. ábrán látható fa elkerülhetetlenül hibákat is el fog követni; például sohasem látott olyan helyzetet, amelyben a várakozási idő 0–10 perc, de az étterem tele van. Egy olyan esetben, amikor az <span class="emphasis"><em>Éhes</em></span> attribútum hamis értékű, a fa azt választja, hogy ne várjunk, pedig én (S. Russell) bizonyára várnék. Felmerül tehát az a kézenfekvő kérdés, hogy ha az algoritmus egy konzisztens, de inkorrekt fát indukál, akkor mennyire lesz inkorrekt ez a fa? Ezt azután fogjuk kísérletileg elemezni, hogy először elmagyaráztuk az attribútumteszt-választási lépés részleteit.</p><div class="figure"><a id="id725868"/><p class="title"><strong>18.6. ábra - A 12 példás tanító minta halmazból származtatott döntési fa</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/18-06.png" alt="A 12 példás tanító minta halmazból származtatott döntési fa"/></div></div></div></div><div class="section" title="Attribútumteszt-választás"><div class="titlepage"><div><div><h2 class="title"><a id="id725878"/>Attribútumteszt-választás</h2></div></div></div><p>A döntési fa tanulás során az attribútumok kiválasztására szolgáló eljárás arra irányul, hogy minimalizáljuk az eredményül kapott fa mélységét. Az alapötlet az, hogy azt az attribútumot válasszuk, amellyel a lehető legmesszebbre jutunk a példák pontos osztályozásában. Egy tökéletes attribútum a példákat egy csupa pozitív és egy csupa negatív példát tartalmazó halmazra osztja. A <span class="emphasis"><em>Vendégek</em></span> nem tökéletes attribútum, de meglehetősen jó. Egy valójában haszontalan attribútum, mint például a <span class="emphasis"><em>Konyha</em></span>, tesztjének eredményeként a kapott halmazokban nagyjából ugyanolyan arányban lesz pozitív és negatív példa, mint az eredeti halmazban.</p><p>Mindössze arra van szükségünk, hogy formális mértéket találjunk arra, hogy mit jelent a „meglehetősen jó” és a „valójában haszontalan”, ezek után implementálni tudjuk a 18.5. ábrán látható <code class="code">ATTRIBÚTUM-VÁLASZTÁS</code> függvényt. A mérték akkor érje el maximumát, amikor az attribútum tökéletes, és akkor legyen minimális, amikor az attribútumnak egyáltalán nincs semmi haszna. Egy megfelelő mérték az attribútum által szolgáltatott <span class="strong"><strong>információ</strong></span> (<span class="strong"><strong>information</strong></span>) várható értéke, ahol az információt abban a matematikai értelmezésben használjuk, ahogy először Shannon és Weaver definiálta (Shannon és Weaver, 1949). Az információ definíciójának megértéséhez gondoljunk például annak a kérdésnek a megválaszolására, hogy egy feldobott pénzérme fej oldala lesz-e felül. Az, hogy a válaszban mennyi információ rejlik, az az előzetes a priori tudástól függ. Minél kevesebbet tudunk, annál több a szolgáltatott információ. Az információelmélet <span class="strong"><strong>bit</strong></span>ekben méri az információtartalmat. Egy bit információ ahhoz elég, hogy egy olyan kérdésre, amelyről semmilyen előzetes elképzelésünk sem volt (például egy szabályos érme feldobásának eredménye) igen/nem választ megadjunk. Általánosságban, ha a lehetséges <span class="emphasis"><em>v<sub>i</sub></em></span> válaszok valószínűsége <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>v<sub>i</sub></em></span>), akkor a válasz <span class="emphasis"><em>I</em></span> információtartalmát a következő összefüggés adja meg: </p><p><span class="inlinemediaobject"><img src="math/mi-18-0003.gif" alt="Attribútumteszt-választás"/></span></p><p>A pénzfeldobás esetre ellenőrizve az összefüggést, a következőt kapjuk:</p><p><span class="inlinemediaobject"><img src="math/mi-18-0004.gif" alt="Attribútumteszt-választás"/></span></p><p>Ha az érme hamisított oly módon, hogy a dobás 99%-ban fejre jön ki, akkor <span class="emphasis"><em>I</em></span>(1/100, 99/100) = 0,08 bitre van szükség, és ha a fej kimenetel valószínűsége 1-hez tart, akkor az aktuális kimenetel megjóslásához szükséges információ 0-hoz tart.</p><p>Döntési fa tanulás esetén a megválaszolásra váró kérdés az, hogy egy adott példának mi a helyes besorolása. Egy jó döntési fa választ ad erre a kérdésre. Még mielőtt egyetlen attribútumot teszteltünk volna, a válaszok valószínűségét becsülhetjük a tanító halmazban található pozitív és negatív minták arányával. Tegyük fel, hogy a tanító halmazban <span class="emphasis"><em>p</em></span> pozitív és <span class="emphasis"><em>n</em></span> negatív példa található. Ez esetben a helyes válasz információtartalmának becslése:</p><p><span class="inlinemediaobject"><img src="math/mi-18-0005.gif" alt="Attribútumteszt-választás"/></span></p><p>A 18.3. ábrán bemutatott étterem tanító halmaz <span class="emphasis"><em>p</em></span> = <span class="emphasis"><em>n </em></span>= 6 mintát tartalmaz, tehát 1 bit információra van szükségünk.</p><p>Egyetlen <span class="emphasis"><em>A</em></span> attribútum tesztje nem fogja megadni mindezt az információt, de valamennyit megad belőle. Pontosan mérni tudjuk, hogy mennyit, ha megnézzük, hogy mennyi információra van még szükségünk az attribútumteszt <span class="emphasis"><em>után</em></span>. Bármely <span class="emphasis"><em>A</em></span> attribútum, amely <span class="emphasis"><em>v</em></span> különböző értéket vehet fel, <span class="emphasis"><em>E</em></span><sub>1</sub>, ...,<span class="emphasis"><em> E<sub>v</sub></em></span> részhalmazokra bontja az <span class="emphasis"><em>E</em></span> tanító halmazt, az <span class="emphasis"><em>A</em></span> lehetséges értékei szerint. Mindegyik <span class="emphasis"><em>E<sub>i</sub></em></span><sub> </sub>részhalmaz <span class="emphasis"><em>p<sub>i</sub></em></span> pozitív és <span class="emphasis"><em>n<sub>i</sub></em></span> negatív példát tartalmaz, így ezen az ágon továbbhaladva <span class="emphasis"><em>I</em></span>(<span class="emphasis"><em>p<sub>i</sub></em></span>/(<span class="emphasis"><em>p<sub>i</sub></em></span> + <span class="emphasis"><em>n<sub>i</sub></em></span>), <span class="emphasis"><em>n<sub>i</sub></em></span>/(<span class="emphasis"><em>p<sub>i</sub></em></span> + <span class="emphasis"><em>n<sub>i</sub></em></span>)) bit információra van szükségünk a válasz megadásához. A tanító halmazból véletlen mintavétellel nyert minta esetén annak valószínűsége, hogy az <span class="emphasis"><em>A</em></span> attribútum ezen a mintán az <span class="emphasis"><em>i</em></span>-edik értéket veszi fel: (<span class="emphasis"><em>p<sub>i</sub></em></span> + <span class="emphasis"><em>n<sub>i</sub></em></span>)/(<span class="emphasis"><em>p</em></span> + <span class="emphasis"><em>n</em></span>). Ennek megfelelően az <span class="emphasis"><em>A</em></span> attribútum tesztje után az átlagos információszükséglet, amely a példa osztályozásához kell:</p><p><span class="inlinemediaobject"><img src="math/mi-18-0006.gif" alt="Attribútumteszt-választás"/></span></p><a id="ID_761_oldal"/><p>Az attribútum tesztjéből származó <span class="strong"><strong>információnyereség</strong></span> (<span class="strong"><strong>information gain</strong></span>) az eredeti (<span class="emphasis"><em>A</em></span> tesztje előtti) információszükséglet és a teszt utáni új információszükséglet különbségeként kapható meg:</p><p><span class="inlinemediaobject"><img src="math/mi-18-0007.gif" alt="Attribútumteszt-választás"/></span></p><p>Az <code class="code">ATTRIBÚTUM-VÁLASZTÁS</code> függvényben használt heurisztika csupán azt takarja, hogy válasszuk a legnagyobb nyereséget biztosító attribútumot. Visszatérve a 18.4. ábrán vizsgált attribútumokhoz:</p><p><span class="inlinemediaobject"><img src="math/mi-18-0008.gif" alt="Attribútumteszt-választás"/></span></p><p>Ez megerősíti azt az intuíciónkat, hogy a <span class="emphasis"><em>Vendégek</em></span> jobb attribútum a tanítóhalmaz részhalmazokra szabdalására, mint a <span class="emphasis"><em>Konyha</em></span>. Az a helyzet, hogy a <span class="emphasis"><em>Vendégek</em></span> attribútumnak van a legnagyobb információnyeresége, ezért a döntési fa tanuló algoritmus ennek tesztjét választaná a fa gyökércsomópontjába.</p></div><div class="section" title="A tanuló algoritmus teljesítményének becslése"><div class="titlepage"><div><div><h2 class="title"><a id="id726177"/>A tanuló algoritmus teljesítményének becslése</h2></div></div></div><p>Egy tanuló algoritmus akkor jó, ha olyan hipotéziseket hoz létre, amelyek jól jósolják meg az általuk előzetesen nem látott példák osztályba sorolását. A 18.5. fejezetben azt mutatjuk be, hogy mi módon lehet előre megbecsülni a jóslás minőséget. Ebben a fejezetben egy olyan módszertant vizsgálunk meg, amellyel az osztályba soroló képességet méréssel lehet becsülni.</p><p>Nyilvánvalóan akkor jó egy jóslás, ha igaznak bizonyul, így a hipotézis minőségét megbecsülhetjük az ismertté vált tényleges osztálybasorolások alapján. Ezt egy <span class="strong"><strong>teszthalmaz</strong></span>nak (<span class="strong"><strong>test set</strong></span>) nevezett mintahalmaz segítségével végezhetjük el. Ha az összes rendelkezésünkre álló példát tanításra használjuk, akkor továbbiakat kell gyűjtenünk a teszteléshez. Ezért gyakran kényelmesebb a következő módszert alkalmazni: </p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p>Gyűjtsünk egy nagy példahalmazt.</p></li><li class="listitem"><p>Osszuk két diszjunkt részre: a <span class="strong"><strong>tanító halmaz</strong></span>ra (<span class="strong"><strong>training set</strong></span>) és a <span class="strong"><strong>teszthalmaz</strong></span>ra (<span class="strong"><strong>test set</strong></span>).</p></li><li class="listitem"><p>Alkalmazzuk a tanító algoritmust a tanító halmazon, és így generáljunk egy <span class="emphasis"><em>h</em></span> hipotézist.</p></li><li class="listitem"><p>Mérjük meg a teszthalmazon, hogy a <span class="emphasis"><em>h</em></span> hipotézis a halmaz hány százalékára ad helyes osztálybasorolást.</p></li><li class="listitem"><p>Ismételjük meg az 1–4 lépéseket különböző tanító halmaz méretekre, és mindegyik mérethez különböző véletlenszerűen kiválasztott tanító halmazokra.</p></li></ol></div><p class="Tartalom3">Ennek az eljárásnak az eredményeként egy adathalmazt kapunk, amelynek feldolgozásával megkaphatjuk az átlagos jóslási képességet a tanító halmaz méretének függvényében. Ezt a függvényt ábrázolva kapjuk az adott algoritmusnak egy adott tématerületre vonatkozó <span class="strong"><strong>tanulási görbé</strong></span>jét (<span class="strong"><strong>learning curve</strong></span>). A <code class="code">DÖNTÉSI-FA-TANULÁS</code> algoritmus étterem példáinkkal felvett tanulási görbéje a 18.7. ábrán látható. Vegyük észre, hogy a tanító halmaz méretével javul a predikció minősége! (Ez okból az ilyen görbéket ún. <span class="strong"><strong>boldog görbé</strong></span>knek (<span class="strong"><strong>happy graph</strong></span>s) is nevezik.) Ez annak a jele ugyanis, hogy valóban van valami mintázat az adatokban, és az algoritmus felfedezi ezt a mintázatot.</p><div class="figure"><a id="id726269"/><p class="title"><strong>18.7. ábra - Tanulási görbe az étterem problématérre véletlenszerűen generált 100 példa esetén. A görbe 20 kísérlet eredményét összegzi.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/18-07.png" alt="Tanulási görbe az étterem problématérre véletlenszerűen generált 100 példa esetén. A görbe 20 kísérlet eredményét összegzi."/></div></div></div><p class="Tartalom3">A tanuló algoritmusnak nyilvánvalóan nem szabad az előtt „látnia” a tesztadatokat, mielőtt a megtanított hipotézist teszteltük velük. Szerencsétlen módon nagyon könnyű beleesni abba a hibába, hogy tanítás közben a tesztadatokra<span class="strong"><strong> kukucskál</strong></span>unk (<span class="strong"><strong>peeking</strong></span>). Ez tipikusan a következő módon zajlik: egy tanuló algoritmus általában számos hangolási lehetőséggel rendelkezik, amellyel az algoritmus viselkedését változtatni tudjuk. Például számos különböző kritérium lehet, amelyek alapján a következő attribútumot kiválasztjuk a döntési fa tanulásnál. Különböző beállítások mentén számos különböző hipotézist generálunk, majd mindegyiket leteszteljük a teszthalmazon, és a legjobb hipotézis predikciós eredményét tekintjük eredménynek. Fájdalmas bár, de kukucskálás történt! Ennek oka, hogy a hipotézist<span class="emphasis"><em> a</em></span> <span class="emphasis"><em>teszthalmazon elért eredménye alapján választottuk ki,</em></span> így a teszthalmaz által hordozott információ beszivárgott a tanuló algoritmusba. Ennek a mesének az a tanulsága, hogy bármely eljárásnak, amelynek része a hipotéziseknek a teszthalmazon nyújtott teljesítményük alapján való összehasonlítása, egy <span class="emphasis"><em>új</em></span> teszthalmazt kell használnia, amelyen a végül kiválasztott hipotézis teljesítményét méri. A gyakorlatban ez túl körülményes, így az emberek továbbra is ily módon szenynyezett adathalmazaikon folytatják kísérleteiket.</p></div><div class="section" title="Zaj és túlilleszkedés"><div class="titlepage"><div><div><h2 class="title"><a id="id726303"/>Zaj és túlilleszkedés</h2></div></div></div><p>Korábban már láttuk, hogy ha van kettő vagy több olyan példa, amelyeknek azonos a leírásuk (az attribútumokra nézve), de eltérő az osztálybasorolásuk, akkor a <code class="code">DÖNTÉSI-FA-TANULÁS</code> algoritmus nem lehet képes olyan döntési fát találni, amely minden példával konzisztens. Az általunk korábbiakban említett megoldás a következő. Ha determinisztikus osztályzásra van igény, akkor minden levélcsomópont adja vissza a hozzá tartozó halmaz többségi osztályát. Más esetekben pedig adja vissza a relatív gyakoriságok alapján becsült osztályba tartozási valószínűségeket. Sajnos, ez még távol van a történet végétől. Könnyen elképzelhető, sőt valójában elég valószínű, hogy a tanuló algoritmus akkor is talál egy olyan döntési fát, amely az összes példával konzisztens, ha nagyon fontos információ hiányzik. Ennek oka, hogy az algoritmus felhasználhat <span class="emphasis"><em>irreleváns</em></span> attribútumokat, ha vannak ilyenek, hogy hamis megkülönböztetést tegyen a példák közt.</p><p>Vizsgáljuk meg azt a problémát, amikor egy kockadobás eredményét akarjuk megjósolni. Tegyük fel, hogy több napon át kísérleteket végzünk számos kockával. Mindegyik tanító példát a következő attribútumokkal írunk le:</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p><span class="emphasis"><em>Nap:</em></span> az a nap, amelyen a kockadobást végeztük (hétfő, kedd, szerda, csütörtök).</p></li><li class="listitem"><p><span class="emphasis"><em>Hónap:</em></span> a hónap, amelyben a kockadobást végeztük (január vagy február).</p></li><li class="listitem"><p><span class="emphasis"><em>Szín:</em></span> a kocka színe (vörös vagy kék).</p></li></ol></div><p>Mindaddig, amíg két dobásnak nem lesz pontosan azonos a leírása, a <code class="code">DÖNTÉSI-FA-TANULÁS</code> képes lesz egzakt hipotézist találni. Minél több attribútum van, annál valószínűbb, hogy egzakt hipotézist kapunk. Viszont az összes ilyen hipotézis teljesen hamis lesz! Amit szeretnénk, az az, hogy a <code class="code">DÖNTÉSI-FA-TANULÁS</code> – miután elég példát látott már – egyetlen levélcsomópontot adjon vissza, minden egyes dobási eredményhez közelítőleg 1/6 valószínűséggel.</p><p>Amikor a hipotézisek nagy halmaza lehetséges, akkor óvatosnak kell lennünk, nehogy arra használjuk az ebből eredő nagy szabadságot, hogy értelmetlen „szabályosságot” találjunk az adatokban. Ezt a problémát <span class="strong"><strong>túlilleszkedés</strong></span>nek (<span class="strong"><strong>overfitting</strong></span>) nevezzük. Ez rendkívül általános jelenség, akkor is jelentkezhet, amikor a keresett függvénynek egyáltalán nincs valószínűségi jellege. Mindamellett ez a probléma az összes tanulási algoritmust sújtja, nem csak a döntési fákat.</p><p>A túlilleszkedés kezelésének kimerítő matematikai tárgyalása meghaladja e könyv kereteit. A probléma megoldására itt csupán egy egyszerű – <span class="strong"><strong>döntési fa metszés</strong></span> (<span class="strong"><strong>decision tree pruning</strong></span>) nevű – módszert mutatunk be. A metszés (vagy nyesés) azon az alapon működik, hogy megakadályozzuk a nem nyilvánvalóan releváns attribútumok tesztje mentén a mintahalmaz ismételt (rekurzív) felosztását, még akkor is, ha az adatok osztálybasorolása az adott csomópontban nem egyforma. Az a kérdés, hogyan vesszük észre egy attribútumról, hogy irreleváns? </p><p>Tegyük fel, hogy egy irreleváns attribútumra alapozva osztottuk ketté a mintahalmazunkat. Általánosságban szólva azt várjuk, hogy ez esetben az eredményül kapott részhalmazokban nagyjából ugyanabban az arányban fognak szerepelni az egyes osztályokba tartozó minták, mint az eredeti halmazban. Ekkor az információnyereség közel nulla.<sup>[<a id="id726376" href="#ftn.id726376" class="footnote">184</a>]</sup> Tehát az információnyereség (annak hiánya) jó jelzés lehet az irrelevanciára. Most az a kérdés merül fel, hogy mekkorának kell lennie az információnyereségnek ahhoz, hogy ezen attribútum mentén szétosszuk a mintahalmazt?</p><p>Ezt a kérdést a statisztikai <span class="strong"><strong>szignifikanciateszt</strong></span>et (<span class="strong"><strong>significance test</strong></span>) használva válaszolhatjuk meg. Ez a teszt azzal a feltételezéssel indul, hogy a példákban egyáltalán nincs közös mintázat (ez az úgynevezett <span class="strong"><strong>nullhipotézis</strong></span>). Ezek után az aktuális adathalmazt vizsgáljuk: meg akarjuk határozni annak mértékét, hogy az adathalmaz mennyire tér el a tökéletesen mintázat nélküli helyzettől. Ha az eltérés mértéke statisztikailag már valószínűtlen (rendszerint 5% vagy ennél kisebb valószínűséget értünk ezalatt), akkor ezt annak bizonyítékaként vesszük, hogy az adatokban jelen van egy alapvető mintázat. A valószínűséget a véletlen mintavételezés esetén várható eltérések standard eloszlását feltételezve számítjuk ki.</p><p>Ebben az esetben a nullhipotézis az, hogy az attribútum irreleváns, ennek megfelelően egy végtelen nagy mintahalmazra vett információnyereség nulla lenne. Azt kell kiszámítanunk, hogy a nullhipotézist feltéve egy ν méretű mintahalmazban a várt pozitív és negatív eseteloszlástól a megfigyelt eloszlás eltérése milyen valószínűséggel léphet fel. Az eltérés mértékét megadhatjuk a részhalmazok tényleges pozitív és negatív esetszámának (<span class="emphasis"><em>p<sub>i</sub></em></span>, <span class="emphasis"><em>n<sub>i</sub>)</em></span> a nullhipotézis fennállása esetén várt esetszámokkal <span class="inlinemediaobject"><img src="math/mi-18-0009.gif" alt="Zaj és túlilleszkedés"/></span> való összehasonlításával:</p><p><span class="inlinemediaobject"><img src="math/mi-18-0010.gif" alt="Zaj és túlilleszkedés"/></span></p><p>A teljes eltérés kényelmesen számítható mértékét a következő összefüggés adja:</p><p><span class="inlinemediaobject"><img src="math/mi-18-0011.gif" alt="Zaj és túlilleszkedés"/></span></p><p>A nullhipotézist feltételezve a <span class="emphasis"><em>D</em></span> eltérés mérték ν– 1 szabadságfokú <span class="emphasis"><em>χ</em></span><sup>2</sup> (khí-négyzet) eloszlást követ. Annak valószínűségét, hogy az attribútum valóban irreleváns, standard <span class="emphasis"><em>χ</em></span><sup>2</sup> táblázatok vagy statisztikai szoftver segítségével számíthatjuk ki. A 18.11. feladatban tűztük ki célul, hogy a <code class="code">DÖNTÉSI-FA-TANULÁS</code> algoritmusban elvégezzük azokat a változtatásokat, amelyek a metszés ezen – <span class="emphasis"><em>χ</em></span><sup>2</sup> <span class="strong"><strong>metszés</strong></span> (<span class="emphasis"><em>χ</em></span><sup>2</sup> <span class="strong"><strong>pruning</strong></span>) néven ismert – formájához vezetnek.</p><p>A zaj metszés segítségével kezelhető: az osztályozási hibák lineáris növekedést okoznak a predikciós hibában, míg az esetleírásban fellépő hibák aszimptotikus hatást gyakorolnak, amely egyre rosszabb, ahogy a döntési fa egyre kisebb halmazok kezelésére zsugorodik. Ha az adatok nagy zajjal terheltek, akkor a metszéssel készült döntési fák lényegesen jobb eredményt adnak, mint a metszés nélkül készültek. Ráadásul a metszéssel készült fák gyakran jóval kisebbek, ezért könnyebben érthetők.</p><p>Egy másik túlilleszkedést csökkentő technika a <span class="strong"><strong>keresztvalidáció</strong></span> (<span class="strong"><strong>cross-validation</strong></span>). Bármely tanulási eljárásnál alkalmazható, nem csupán a döntési fa tanulásnál. Az alapvető ötlet annak megbecsülése, hogy az egyes hipotézisek mennyire jól fogják megjósolni a még nem látott esetekre adandó válaszokat. Ezt úgy becsülhetjük, hogy az ismert adatok egy részét félretesszük, és ezekkel teszteljük a megmaradt adatok alapján tanulással létrehozott fa predikciós képességét. <span class="emphasis"><em>K</em></span>-szoros keresztvalidációnak nevezzük, ha <span class="emphasis"><em>k</em></span> kísérletet végzünk, és minden esetben az adatok más és más 1/<span class="emphasis"><em>k</em></span>-ad részét tesszük félre validációs tesztcélra, majd a végén átlagoljuk az eredményeket. Az 5 és a 10 elterjedten használt <span class="emphasis"><em>k</em></span> értékek. Szélső esetként használják a <span class="emphasis"><em>k </em></span>= <span class="emphasis"><em>n</em></span> választást, amelyet hagyj-ki-egyet keresztvalidációs módszerként ismerünk. A keresztvalidációt tetszőleges döntési fa tanulási módszerrel együtt alkalmazhatjuk (beleértve a metszést is), célja mindig az, hogy olyan döntési fa kiválasztását segítsük elő, amely jó predikciós képességgel rendelkezik. A kukucskálási jelenség elkerülése végett ezek után ezt a predikciós képességet egy új teszthalmazon kell mérnünk.</p></div><div class="section" title="A döntési fák alkalmazhatóságának kiterjesztése"><div class="titlepage"><div><div><h2 class="title"><a id="id726524"/>A döntési fák alkalmazhatóságának kiterjesztése</h2></div></div></div><p>Egy sor problémát meg kell oldanunk ahhoz, hogy a döntési fa indukciót a problémák szélesebb körére kiterjeszthessük. Nagyjából felvázoljuk mindegyiket, azt javasolva, hogy a teljesebb megértés érdekében a kapcsolódó feladatokat oldja meg az olvasó.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="strong"><strong>Hiányzó adatok</strong></span> (<span class="strong"><strong>missing data</strong></span>): Számos területen nem ismerhető meg minden példa összes attribútuma. Lehet, hogy az értékek nem kerültek tárolásra, vagy túl drága lenne a mérésük. Ez két problémát vet fel: először, ha van is egy teljes döntési fánk, akkor hogyan tudunk egy olyan mintát osztályozni, amelynek egy tesztelendő attribútuma hiányzik? Másodszor, hogyan módosítsuk az információnyereségre vonatkozó formulát, ha néhány példa ezen attribútumának értéke nem ismert? Ezeket a kérdéseket a 18.2. feladatban vizsgáljuk.</p></li><li class="listitem"><p><span class="strong"><strong>Sokértékű attribútum</strong></span>ok (<span class="strong"><strong>multivalued attribute</strong></span>s): Ha egy attribútum nagyszámú értéket vehet fel, akkor az információnyereség nagysága nem megfelelő mértéke az attribútum hasznosságának. Szélsőséges esetben egy olyan attribútumot használnánk, mint például az <span class="emphasis"><em>ÉtteremNév</em></span> attribútum, amely minden példára más és más értéket ad. Ez esetben példák minden részhalmaza egyelemű lesz, egyedi osztálybasorolással, így az információnyereség erre az attribútumra veszi fel maximális értékét. Mindamellett ez az attribútum irreleváns, értéktelen. Egy lehetséges megoldás a <span class="strong"><strong>nyereségarány</strong></span> (<span class="strong"><strong>gain ratio</strong></span>) használata (lásd 18.13. feladat).</p></li><li class="listitem"><p><span class="strong"><strong>Folytonos és egész értékű bemeneti attribútumok</strong></span> (<span class="strong"><strong>continuous and integer-valued</strong></span><span class="strong"><strong> input attributes</strong></span>): A folytonos vagy az egész értékű bemeneti attribútumok – mint például a <span class="emphasis"><em>Magasság</em></span> és a <span class="emphasis"><em>Súly</em></span> – által felvehető értékek halmaza végtelen. A döntési fa tanuló algoritmusok az ilyen esetekre nem generálnak végtelen elágazású csomópontokat, inkább megkeresik azt a <span class="strong"><strong>küszöbpont</strong></span>ot (<span class="strong"><strong>split point</strong></span>), amely a legnagyobb információnyereséget eredményezi. Például egy adott csomópontnál az lehet a helyzet, hogy a <span class="emphasis"><em>Súly </em></span>&gt; 160 teszt adja a legnagyobb információnyereséget. Léteznek hatékony dinamikus programozási módszerek jó küszöbpontok megtalálására, de még mindig messze ez a legtöbb erőforrást igénylő része a döntési fa tanulási eljárások valós problémákra való alkalmazásának.</p></li><li class="listitem"><p><span class="strong"><strong>Folytonos értékkészletű kimeneti attribútumok</strong></span> (<span class="strong"><strong>continuous-valued output attributes</strong></span>): Ha egy numerikus értéket akarunk megjósolni, például egy műtárgy árát, akkor nem osztályozásra van szükségünk, hanem egy <span class="strong"><strong>regressziós fá</strong></span>ra (<span class="strong"><strong>regression tree</strong></span>). Egy ilyen fa nem egyetlen értéket (osztályt) ad vissza az egyes levél csomópontjaiban, hanem egy attribútumhalmaz lineáris függvényét. Például a műtárgyak regreszsziós fájának kézzel festett metszetekre vonatkozó ága a területnek, a kornak és a színek számának lineáris függvényéhez vezethet. A tanuló algoritmusnak kell azt megoldania, hogy mikor álljon le a minták csomópontokban történő szétosztásával, áttérve a maradék attribútumokat (vagy azok bizonyos részhalmazát) felhasználó lineáris regresszió megalkotására.</p></li></ul></div><p class="Tartalom3">Egy döntési fa tanulásra szolgáló, valós problémák megoldását célzó rendszernek képesnek kell lennie mindezen problémák kezelésére. A folytonos értékű változók kezelése különösen fontos, mivel mind a fizikai, mind a gazdasági folyamatok numerikus értékekkel jellemezhetők. Számos, üzleti forgalomban kapható programcsomag készült, amelyek megfelelnek mindezen a kritériumoknak, és segítségükkel több száz – valamilyen konkrét területen – alkalmazott rendszert fejlesztettek ki. Az ipar és kereskedelem sok területén, ha minták alapján kialakított osztályozásra van szükség, akkor elsősorban döntési fákkal próbálkoznak. A döntési fák egyik fontos tulajdonsága, hogy az ember számára jól érthető a tanuló algoritmus által előállított eredmény. (Valójában olyan gazdasági döntések esetén, amelyeknek megkülönböztetés elleni jogszabályoknak kell megfelelniük, ez <span class="emphasis"><em>jogi követelmény</em></span>.) A neurális hálózatoknál hiányzik ez a fontos tulajdonság (lásd 20. fejezet).</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id721711" href="#id721711" class="para">183</a>] </sup> Nagyon fontos, hogy megkülönböztessük az azonos példát egy <span class="emphasis"><em>azonos leírással rendelkező példától</em></span>. Erre a fontos különbségre még visszatérünk a 19. fejezetben.</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id726376" href="#id726376" class="para">184</a>] </sup> Valójában a nyereség pozitív lesz, kivéve azt az esetet, amikor az arányok mind pontosan egyformák (lásd 18.10. feladat).</p></div></div></div></body></html>
