<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Kernelgépek"><div class="titlepage"><div><div><h1 class="title"><a id="id747307"/>Kernelgépek</h1></div></div></div><p>A neurális hálók tárgyalása után maradt egy dilemmánk. Az egyrétegű hálóknak nagyon egyszerű és hatékony tanulási algoritmusuk van, de nagyon korlátozott a kifejezőképességük, csupán lineáris döntési határokat képesek megtanulni a bemeneti térben. Másrészt a többrétegű hálók sokkal kifejezőbbek – általános nemlineáris függvényeket képesek reprezentálni –, de a rengeteg lokális minimum jelenléte, illetve a sokdimenziós súlytér miatt nagyon nehéz a tanításuk. Ebben az alfejezetben egy relatíve új tanulómódszer családot fedezünk fel, az úgynevezett <span class="strong"><strong>szupport vektor gépek</strong></span>et (<span class="strong"><strong>support vector machines</strong></span>, <span class="strong"><strong>SVM</strong></span>), vagy általánosabban a <span class="strong"><strong>kernelgép</strong></span>eket <span class="strong"><strong>(kernel machine</strong></span>s). Bizonyos fokig a kernelgépek a két oldal legjobb tulajdonságait egyesítik. Azaz ezek a módszerek hatékony tanítási algoritmusokat alkalmaznak, <span class="emphasis"><em>ugyanakkor </em></span>képesek bonyolult, nemlineáris függvények reprezentálására.</p><p>A kernelgépek teljes mélységű tárgyalása meghaladja ennek a könyvnek a kereteit, de a fő gondolatot egy példán keresztül illusztráljuk. A 20.27. (a) ábra egy kétdimenziós bemeneti teret mutat, amelyet az <span class="strong"><strong>x = </strong></span>(<span class="emphasis"><em>x</em></span><sub>1</sub>, <span class="emphasis"><em>x</em></span><sub>2</sub>) attribútumok írnak le. A pozitív példák (<span class="emphasis"><em>y = +</em></span>1) egy kör alakú rész belsejében, a negatív példák (<span class="emphasis"><em>y = </em></span>–1) azon kívül helyezkednek el. Nyilvánvaló, hogy a probléma megoldására nem létezik lineáris szeparátor. Tegyük fel, hogy valamilyen számított tulajdonságok segítségével új formára hozzuk a példákat – azaz az összes bemeneti <span class="strong"><strong>x</strong></span> vektort leképezzük a tulajdonságértékekből formált új, <span class="emphasis"><em>F</em></span>(<span class="strong"><strong>x</strong></span>) vektorra. A példában használjuk a következő három tulajdonságot:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0042.gif" alt="Kernelgépek"/></span></p><p>Rövidesen látni fogjuk, hogy honnan vesszük ezeket a tulajdonságokat, de most csak nézzük meg azt, hogy mi is történt. A 20.27. (b) ábra mutatja az adatokat az új, a tulajdonságok által definiált háromdimenziós térben: ebben a térben <span class="emphasis"><em>lineárisan szeparálhatók</em></span>! Ez a jelenség eléggé általános: ha az adatokat megfelelően sokdimenziós térbe képezzük le, akkor mindig lineárisan szeparálhatók lesznek. Itt mi csak három dimenziót használtunk,<sup>[<a id="id747395" href="#ftn.id747395" class="footnote">208</a>]</sup> de ha <span class="emphasis"><em>N</em></span> pontunk van – akkor speciális esetek kivételével –, egy <span class="emphasis"><em>N – </em></span>1 vagy ennél magasabb dimenziós térben a pontok mindig lineárisan szeparálhatók lesznek (lásd 20.21. feladat).</p><div class="figure"><a id="id747419"/><p class="title"><strong>20.27. ábra - (a) Egy kétdimenziós tanító halmaz, amelyben a pozitív példákat fekete, a negatívakat fehér körök jelölik. Az <span class="inlinemediaobject"><img src="math/mi-20-0043.gif" alt="(a) Egy kétdimenziós tanító halmaz, amelyben a pozitív példákat fekete, a negatívakat fehér körök jelölik. Az valódi elválasztó határt is bejelöltük. (b) Ugyanazok az adatok az háromdimenziós térbe való leképezés után. Az (a) ábrán látható kör alakú döntési határ a háromdimenziós térben lineáris döntési felületbe ment át."/></span><span class="emphasis"><em> </em></span>valódi elválasztó határt is bejelöltük. (b) Ugyanazok az adatok az <span class="inlinemediaobject"><img src="math/mi-20-0044.gif" alt="(a) Egy kétdimenziós tanító halmaz, amelyben a pozitív példákat fekete, a negatívakat fehér körök jelölik. Az valódi elválasztó határt is bejelöltük. (b) Ugyanazok az adatok az háromdimenziós térbe való leképezés után. Az (a) ábrán látható kör alakú döntési határ a háromdimenziós térben lineáris döntési felületbe ment át."/></span> háromdimenziós térbe való leképezés után. Az (a) ábrán látható kör alakú döntési határ a háromdimenziós térben lineáris döntési felületbe ment át.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-27.png" alt="(a) Egy kétdimenziós tanító halmaz, amelyben a pozitív példákat fekete, a negatívakat fehér körök jelölik. Az valódi elválasztó határt is bejelöltük. (b) Ugyanazok az adatok az háromdimenziós térbe való leképezés után. Az (a) ábrán látható kör alakú döntési határ a háromdimenziós térben lineáris döntési felületbe ment át."/></div></div></div><p>Ennyi az egész? Egyszerűen létrehozunk egy nagy halom számított tulajdonságot, és a megfelelő sokdimenziós térben megkeressük a lineáris szeparátort? Sajnálatos módon nem ilyen egyszerű. Emlékezzünk, hogy a <span class="emphasis"><em>d</em></span> dimenziós térben a lineáris szeparátort egy <span class="emphasis"><em>d</em></span> paraméteres egyenlet határozza meg, így aztán az a veszély fenyeget, hogy ha <span class="emphasis"><em>d </em></span>≈ <span class="emphasis"><em>N </em></span>(ahol <span class="emphasis"><em>N</em></span> az adatpontok száma), akkor könnyen túlilleszkedhetünk az adatokra. (Ez ahhoz hasonló, mint amikor egy magas fokszámú polinommal túlillesztünk adatokat, ahogy ezt a 18. fejezetben tárgyaltuk.) Ezen okból a kernelgépek rendszerint az <span class="emphasis"><em>optimális</em></span> lineáris szeparátort találják meg. Azt nevezzük optimálisnak, amelynek legnagyobb a <span class="strong"><strong>tartalék</strong></span>a (<span class="strong"><strong>margin</strong></span>): a lineáris szeparátor és a pozitív példák között az egyik oldalon, illetve a lineáris szeparátor és a negatív példák között a másikon. (Lásd 20.28. ábra.) A számítógépes tanulás elmélet módszereit (lásd 18.5. alfejezet) használva megmutatható, hogy ez a szeparátor az új példák robusztus általánosítására nézve nagyon jó tulajdonságokkal rendelkezik.</p><div class="figure"><a id="id747481"/><p class="title"><strong>20.28. ábra - Az első két dimenzióra vetített közelkép: a 20.27. (b) ábra optimális szeparátora. A szeparátort vastag vonallal jelöltük, a hozzá legközelebbi pontokat – a <span class="strong">szupport vektor</span>okat – bekarikáztuk. A <span class="strong">tartalék</span> nem más, mint a pozitív és negatív példák közti elválasztó sáv szélessége.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/20-28.png" alt="Az első két dimenzióra vetített közelkép: a 20.27. (b) ábra optimális szeparátora. A szeparátort vastag vonallal jelöltük, a hozzá legközelebbi pontokat – a szupport vektorokat – bekarikáztuk. A tartalék nem más, mint a pozitív és negatív példák közti elválasztó sáv szélessége."/></div></div></div><p>Hogyan találjuk meg ezt a szeparátort? Kiderül, hogy ez egy <span class="strong"><strong>kvadratikus programozás</strong></span>sal (<span class="strong"><strong>quadratic programming</strong></span>) megoldható optimalizálási feladat. Tegyük fel, hogy <span class="strong"><strong>x</strong></span><sub>i</sub> példáink vannak, az osztálybasorolásuk <span class="emphasis"><em>y<sub>i</sub> = </em></span>±1, és a bemeneti térben optimális szeparátort akarunk találni. Ekkor a megoldandó kvadratikus programozási feladat azon paraméterértékek megtalálása, amelyek az <span class="emphasis"><em>α<sub>i</sub></em></span> ≥ 0 és Σ<span class="emphasis"><em><sub>i</sub>α<sub>i</sub></em></span> <span class="emphasis"><em>y<sub>i</sub> = </em></span>0 korlátozó feltételek mellett maximálják a következő kifejezést:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0045.gif" alt="Az első két dimenzióra vetített közelkép: a 20.27. (b) ábra optimális szeparátora. A szeparátort vastag vonallal jelöltük, a hozzá legközelebbi pontokat – a szupport vektorokat – bekarikáztuk. A tartalék nem más, mint a pozitív és negatív példák közti elválasztó sáv szélessége."/></span></p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Bár ennek a kifejezésnek a deriválása nem nagyon fontos pontja a történetnek, de azért van két lényeges tulajdonsága. Először is a kifejezésnek egyetlen, globális maximuma van, ami hatékonyan megtalálható. Másodszor <span class="emphasis"><em>az adatok kizárólag pontpárok skalárszorzataként jelennek meg a kifejezésben</em></span>. Ez a második tulajdonság magára a szeparátorra is igaz, ha az optimális <span class="emphasis"><em>α<sub>i</sub></em></span>-ket kiszámítottuk, akkor:</p><p><span class="inlinemediaobject"><img src="math/mi-20-0046.gif" alt="Az első két dimenzióra vetített közelkép: a 20.27. (b) ábra optimális szeparátora. A szeparátort vastag vonallal jelöltük, a hozzá legközelebbi pontokat – a szupport vektorokat – bekarikáztuk. A tartalék nem más, mint a pozitív és negatív példák közti elválasztó sáv szélessége."/></span></p></div><p>Az ezen egyenlettel definiált optimális szeparátor utolsó fontos tulajdonsága az, hogy az egyes adatpontokkal asszociált <span class="emphasis"><em>α<sub>i</sub></em></span> súlyok mind <span class="emphasis"><em>nullák</em></span>, kivéve a szeparátorhoz legközelebb eső pontokat – ezeket nevezzük <span class="strong"><strong>szupport vektor</strong></span>oknak (<span class="strong"><strong>support vector</strong></span>). (Azért nevezzük így őket, mert ők „tartják” a szeparáló síkot.) Mivel rendszerint jóval kevesebb szupport vektor van, mint adatpont, ezért az optimális szeparátort meghatározó tényleges paraméterszám rendszerint jóval kisebb <span class="emphasis"><em>N</em></span>-nél.</p><p>Rendszerint nem várhatjuk el, hogy lineáris szeparátort találjunk az <span class="strong"><strong>x</strong></span> bemeneti térben, de könnyen belátható, hogy a sokdimenziós <span class="emphasis"><em>F</em></span>(<span class="strong"><strong>x</strong></span>) tulajdonságtérben találhatunk lineáris szeparátorokat. Ennek érdekében a (20.17) egyenletben <span class="strong"><strong>x</strong></span><sub>i</sub> · <span class="strong"><strong>x</strong></span><sub>j</sub>-t egyszerűen kicseréljük <span class="emphasis"><em>F</em></span>(<span class="strong"><strong>x</strong></span><sub>i</sub>) · <span class="emphasis"><em>F</em></span>(<span class="strong"><strong>x</strong></span><sub>j</sub>)-re. Ez önmagában nem túlzottan figyelemre méltó – <span class="strong"><strong>x</strong></span> kicserélése <span class="emphasis"><em>F</em></span>(<span class="strong"><strong>x</strong></span>)-re <span class="emphasis"><em>bármely</em></span> tanuló algoritmusban elérné a kívánt hatást –, de a skalárszorzatnak van néhány érdekes tulajdonsága. Az <span class="emphasis"><em>F</em></span>(<span class="strong"><strong>x</strong></span><sub>i</sub>) · <span class="emphasis"><em>F</em></span>(<span class="strong"><strong>x</strong></span><sub>j</sub>) gyakran kiszámítható anélkül, hogy először kiszámítanánk minden pontra <span class="emphasis"><em>F</em></span>-et. A (20.16) egyenlettel definiált háromdimenziós tulajdonságtér példánkban némi algebrai átalakításokkal megmutatható, hogy:</p><p><code class="code"><em><span class="remark">F</span></em>(<em><span class="remark">x</span></em><sub>i</sub>) · <em><span class="remark">F</span></em>(<em><span class="remark">x</span></em><sub>j</sub>) = (<em><span class="remark">x</span></em><sub>i</sub> · <em><span class="remark">x</span></em><sub>j</sub>)<sup>2</sup></code></p><p>Az (<span class="strong"><strong>x</strong></span><sub>i</sub> · <span class="strong"><strong>x</strong></span><sub>j</sub>)<sup>2</sup> kifejezést <span class="strong"><strong>kernelfüggvény</strong></span>nek (<span class="strong"><strong>kernel function</strong></span>) nevezzük, és <span class="emphasis"><em>K</em></span>(<span class="strong"><strong>x</strong></span><sub>i</sub>, <span class="strong"><strong>x</strong></span><sub>j</sub>)-vel jelöljük. A kernelgépek szempontjából ez egy olyan függvény, amely pontpárokra alkalmazható avégett, hogy valamilyen tulajdonságtérben kiszámítsuk a skalárszorzatukat. Ennek megfelelően újrafogalmazhatjuk állításunkat: a (20.17) egyenletben <span class="strong"><strong>x</strong></span><sub>i</sub> · <span class="strong"><strong>x</strong></span><sub>j</sub>-t egyszerűen kicserélve a <span class="emphasis"><em>K</em></span>(<span class="strong"><strong>x</strong></span><sub>i</sub>, <span class="strong"><strong>x</strong></span><sub>j</sub>) kernelfüggvényre, a sokdimenziós<span class="emphasis"><em> F</em></span>(<span class="strong"><strong>x</strong></span>) tulajdonságtérben találhatunk lineáris szeparátorokat. Így a tanulást a sokdimenziós térben végezhetjük, de csupán kernelfüggvények értékét kell kiszámítanunk, nem kell az öszszes pontra a tulajdonságok teljes készletét kiszámítani.</p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>A következő lépés – aminek most már kézenfekvőnek kell lennie – az, hogy meglássuk, a <span class="emphasis"><em>K</em></span>(<span class="strong"><strong>x</strong></span><sub>i</sub>, <span class="strong"><strong>x</strong></span><sub>j</sub>) = (<span class="strong"><strong>x</strong></span><sub>i</sub> · <span class="strong"><strong>x</strong></span><sub>j</sub>)<sup>2</sup> kernelben nincs semmi különleges. Ez egy bizonyos sokdimenziós tulajdonságtérnek felel meg, de más kernelfüggvények más tulajdonságterekkel vannak kapcsolatban. A <span class="strong"><strong>Mercer-tétel</strong></span> (1909) azt mondja ki, hogy bármely „ésszerű”<sup>[<a id="id747870" href="#ftn.id747870" class="footnote">209</a>]</sup> kernelfüggény megfelel <span class="emphasis"><em>valamilyen</em></span> tulajdonságtérnek. Ez a tulajdonságtér még egész ártatlannak látszó kernelek esetén is nagyon nagy lehet. Például a <span class="emphasis"><em>K</em></span>(<span class="strong"><strong>x</strong></span><sub>i</sub>, <span class="strong"><strong>x</strong></span><sub>j</sub>) = (1 + <span class="strong"><strong>x</strong></span><sub>i</sub> · <span class="strong"><strong>x</strong></span><sub>j</sub>)<sup><span class="emphasis"><em>d</em></span> </sup><span class="strong"><strong>polinomiális kernel</strong></span> (<span class="strong"><strong>polynomial kernel</strong></span>) egy olyan tulajdonságtérnek felel meg, amelynek dimenziója <span class="emphasis"><em>d</em></span>-ben exponenciális. Ha a (20.17) egyenletben ilyen kerneleket használunk, akkor <span class="emphasis"><em>hatékonyan kereshetünk lineáris szeparátorokat sok milliárd (vagy egyes esetekben végtelen) dimenziós terekben</em></span>. Az eredményként kapott lineáris szeparátorokat visszavetítve az eredeti bemeneti térbe, a pozitív és negatív példákat elválasztó tetszőlegesen tekervényes, nemlineáris határfelületeket kaphatunk.</p></div><p>Említettük az előző részben, hogy a kernelgépek kiemelkedően teljesítenek a kézírásos számjegyek felismerésében, de gyorsan felhasználják őket más alkalmazásokhoz is, különösen olyanokhoz, ahol sok bemeneti tulajdonság van. Ennek a folyamatnak részeként számos új kernelt dolgoztak ki, amelyek karakterfüzérekre, fákra és más nemnumerikus adattípusokra alkalmazhatók. Az is megfigyelhető, hogy a kernelmódszer nemcsak optimális szeparátorok keresésére alkalmas, hanem bármely olyan algoritmusra, amely úgy átalakítható, hogy csak adatpontpárok skalárszorzatát használja, mint a (20.17), illetve (20.18) egyenlet. Amint ezt sikerült megtennünk, a skalárszorzat kicserélhető egy kernelfüggvényre, és elkészítettük az algoritmus <span class="strong"><strong>kernelesített</strong></span> (<span class="strong"><strong>kernelized</strong></span>) változatát. Ez az átalakítás többek közt a <span class="emphasis"><em>k</em></span>-legközelebbi-szomszéd algoritmusra és a perceptron tanulásra is könnyen elvégezhető.</p><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.id747395" href="#id747395" class="para">208</a>] </sup> Az olvasó felfedezheti, hogy elég lett volna csupán <span class="emphasis"><em>f</em></span><sub>1</sub> és <span class="emphasis"><em>f</em></span><sub>2</sub> használata, de a 3D leképezés jobban illusztrálja az ötletet.</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id747870" href="#id747870" class="para">209</a>] </sup> Itt ésszerű alatt azt értjük, hogy a <span class="strong"><strong>k</strong></span><sub>ij</sub> = <span class="emphasis"><em>K</em></span>(<span class="strong"><strong>x</strong></span><sub>i</sub>, <span class="strong"><strong>x</strong></span><sub>j</sub>) mátrix pozitív definit; lásd A) függelék.</p></div></div></div></body></html>
