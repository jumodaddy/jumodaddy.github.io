<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="section" title="Passzív megerősítéses tanulás"><div class="titlepage"><div><div><h1 class="title"><a id="id751804"/>Passzív megerősítéses tanulás</h1></div></div></div><a id="ID_875_oldal"/><p>Az egyszerűség kedvéért egy ismert, teljes mértékben hozzáférhető környezet állapot-reprezentációját használó passzív ágens esetét tárgyaljuk először. Passzív tanulás esetén az ágens <span xml:lang="hu" class="emphasis"><em>π</em></span> stratégiája rögzített, az <span class="emphasis"><em>s</em></span> állapotban mindig a <span xml:lang="hu" class="emphasis"><em>π</em></span>(<span class="emphasis"><em>s</em></span>) cselekvést hajtja végre. A cél egyszerűen a stratégia jóságának – tehát az <span class="emphasis"><em>U<sup>π</sup></em></span> (<span class="emphasis"><em>s</em></span>) hasznosságfüggvénynek – a megtanulása. Példaként a 17. fejezetben bevezetett 4 × 3-as világot fogjuk használni. A 21.1. ábra mutatja ezt a világot, és a megfelelő hasznosságokat. Nyilvánvaló, hogy a passzív tanulás hasonló a <span class="strong"><strong>stratégiakiértékelés</strong></span>i (<span class="strong"><strong>policy evaluation</strong></span>) feladathoz, amely része a 17.3. alfejezetben ismertetett <span class="strong"><strong>stratégiaiteráció</strong></span>s (<span class="strong"><strong>policy iteration</strong></span>) algoritmusnak. A legfontosabb különbség, hogy a passzív ágens nem ismeri az <span class="strong"><strong>állapotátmenet-modell</strong></span>t (<span class="strong"><strong>transition model</strong></span>), a <span class="emphasis"><em>T</em></span>(<span class="emphasis"><em>s</em></span>, <span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>s'</em></span>)-t, amely annak a valószínűséget adja meg, hogy az <span class="emphasis"><em>a</em></span> cselekvés hatására az <span class="emphasis"><em>s</em></span> állapotból az <span class="emphasis"><em>s'</em></span> állapotba jutunk, továbbá nem ismeri a <span class="strong"><strong>jutalomfüggvény</strong></span>t (<span class="strong"><strong>reward function</strong></span>), <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>)-et, amely minden állapothoz megadja az ott elnyerhető jutalmat.</p><div class="figure"><a id="id751912"/><p class="title"><strong>21.1. ábra - (a) A 4 × 3-as világban alkalmazott <span xml:lang="hu" class="emphasis"><em>π</em></span> stratégia. Ez optimális stratégia, ha nem alkalmazunk leértékelést, és minden állapotban, amely nem végállapot, <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>)= −0,04. (b) A 4 × 3-as világ állapotainak hasznossága a <span xml:lang="hu" class="emphasis"><em>π</em></span> stratégia esetén.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/21-01.png" alt="(a) A 4 × 3-as világban alkalmazott π stratégia. Ez optimális stratégia, ha nem alkalmazunk leértékelést, és minden állapotban, amely nem végállapot, R(s)= −0,04. (b) A 4 × 3-as világ állapotainak hasznossága a π stratégia esetén."/></div></div></div><p>Az ágens – <span xml:lang="hu" class="emphasis"><em>π</em></span> stratégiája felhasználásával – egy sor <span class="strong"><strong>kísérlet</strong></span>et (<span class="strong"><strong>trial</strong></span>) végez. Minden egyes kísérletben az (1,1) állapotból indul, és az állapotátmenetek valamilyen sorozatát észleli, amíg el nem érkezik a (4,2), illetve a (4,3) végállapotok valamelyikébe. Észlelései alapján információt kap mind az aktuális állapotról, mind az ott nyert jutalomról. Például így nézhetnek ki a tipikus kísérletek:</p><p><code class="code">(1,1)<sub>–0,04</sub>→(1,2)<sub>–0,04</sub>→(1,3)<sub>–0,04</sub>→(1,2)<sub>–0,04</sub>→(1,3)<sub>–0,04</sub>→(2,3)<sub>–0,04</sub>→(3,3)<sub>–0,04</sub>→(4,3)<sub>+1</sub></code></p><p><code class="code">(1,1)<sub>–0,04</sub>→(1,2)<sub>–0,04</sub>→(1,3)<sub>–0,04</sub>→(2,3)<sub>–0,04</sub>→(3,3)<sub>–0,04</sub>→(3,2)<sub>–0,04</sub>→(3,3)<sub>–0,04</sub>→(4,3)<sub>+<em><span class="remark">1</span></em></sub></code></p><p><code class="code">(1,1)<sub>–0,04</sub>→(2,1)<sub>–0,04</sub>→(3,1)<sub>–0,04</sub>→(3,2)<sub>–0,04</sub>→(4,2)<sub>–1</sub></code></p><p>Vegyük észre, hogy mindegyik állapotészlelés mellett – alsó indexként – feltüntettük az elnyert jutalmat. Célunk a jutalmakban rejlő információ felhasználása arra, hogy az egyes <span class="emphasis"><em>s</em></span> állapotokhoz – amelyek nem végállapotok – rendelhető várható hasznosságot, <span class="emphasis"><em>U<sup>π</sup></em></span> (<span class="emphasis"><em>s</em></span>)-t megtanuljuk. A hasznosságot úgy definiáljuk, mint a <span xml:lang="hu" class="emphasis"><em>π</em></span> stratégia követése esetén az összegzett (leértékelt) jutalom várható értéke. Ez – ahogy a (17.3) egyenletben a <a class="xref" href="ch17.md#ID_713_oldal">„Optimalitás szekvenciális döntési problémákban”</a> részben – már felírtuk:</p><p><span class="inlinemediaobject"><img src="math/mi-21-0001.gif" alt="(a) A 4 × 3-as világban alkalmazott π stratégia. Ez optimális stratégia, ha nem alkalmazunk leértékelést, és minden állapotban, amely nem végállapot, R(s)= −0,04. (b) A 4 × 3-as világ állapotainak hasznossága a π stratégia esetén."/></span></p><p>Mindegyik összefüggésünkben figyelembe veszünk egy <span class="emphasis"><em>γ</em></span> <span class="strong"><strong>leértékelési tényező</strong></span>t (<span class="strong"><strong>discount factor</strong></span>), de a 4 × 3 világban <span class="emphasis"><em>γ </em></span>=<span class="emphasis"><em> </em></span>1-et használunk.</p><div class="section" title="Közvetlen hasznosságbecslés"><div class="titlepage"><div><div><h2 class="title"><a id="id752098"/>Közvetlen hasznosságbecslés</h2></div></div></div><p>Widrow és Hoff az <span class="strong"><strong>adaptív szabályozáselmélet</strong></span>tel (<span class="strong"><strong>adaptive control theory</strong></span>) foglalkozva egy egyszerű módszert fedeztek fel a <span class="strong"><strong>közvetlen hasznosságbecslés</strong></span>re (<span class="strong"><strong>direct utility estimation</strong></span>) (Widrow és Hoff, 1960). Az ötlet a következő: az állapot hasznossága nem más, mint az adott állapotból kiindulva az összegzett jutalom várható értéke, és minden egyes kísérlet ennek az értéknek egy <span class="emphasis"><em>mintájával szolgál</em></span> minden bejárt állapotra. Például az előzőkben bemutatott háromelemű tipikus kísérlethalmaz első kísérlete egy 0,72 értékű összegzett jutalommintát ad az (1,1) állapotra, két – 0,76 és 0,84 értékű – mintát az (1,2) állapotra, továbbá két – 0,80 és 0,88 értékű – mintát az (1,3) állapotra és így tovább. Ennek megfelelően az algoritmus mindegyik kísérleti lépéssorozat végén kiszámítja az összes állapotra a megfigyelt hátralevő jutalmat, és ez alapján frissíti az állapot hasznosságát, egyszerűen minden egyes állapotra mozgóablak átlagolást végezve, és az eredményt egy táblában tárolva. Ha a kísérletek száma a végtelenhez tart, a minták átlaga a (21.1) egyenlettel megadott valódi várható értékhez fog tartani.</p><p>Nyilvánvaló, hogy a közvetlen hasznosságbecslés nem más, mint egy példánya a felügyelt tanításnak. A tanító példapárok bemeneti része az állapot, kimeneti része pedig a megfigyelt hátralevő jutalom. Tehát a megerősítéses tanulást standard induktív tanulási problémára redukáltuk, a 18. fejezetben tárgyaltaknak megfelelően. A 21.4. alfejezetben áttekintjük a hasznosságfüggvények hatékonyabb reprezentálására szolgáló eszközök használatát, például a neurális hálókét. Ezeknek a reprezentációknak a használata esetén a tanulási technika közvetlenül a megfigyelt adatokra alkalmazható.</p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>A közvetlen hasznosságbecsléssel sikerült a megerősítéses tanulást standard induktív tanulási problémára redukálnunk, amelyről már nagyon sok mindent tudunk. Sajnálatos módon egy nagyon fontos információforrás hiányzik, nevezetesen az, hogy az állapotok hasznossága nem független egymástól! <span class="emphasis"><em>Minden egyes állapot hasznossága egyenlő az ebben az állapotban elnyerhető jutalom és az őt követő állapotok várható hasznosságának összegével.</em></span> Tehát a hasznosságértékek kielégítik a rögzített stratégiára vonatkozó Bellman-egyenletet (lásd még (17.10) egyenlet):</p><p><span class="inlinemediaobject"><img src="math/mi-21-0002.gif" alt="Közvetlen hasznosságbecslés"/></span></p></div><p>Azzal, hogy figyelmen kívül hagyja az állapotok közötti összefüggéseket, a közvetlen hasznosság becslése eljátssza a tanulás lehetőségeit. Például a bemutatott három kísérlet közül a második érinti a (3,2) állapotot, amelyet előzőleg az ágens nem érintett még. A következő átmenet után (3,3)-ba jut, amely az első kísérlet alapján nagy hasznosságot mutat. A Bellman-összefüggés azonnal rámutat, hogy valószínűleg akkor a (3,2)-nek is nagy a hasznossága, mivel a (3,3)-ba vezet, de a közvetlen hasznosságbecslés semmit se tanul meg ebből a kísérlet végéig. Általánosabban megfogalmazva azt mondhatjuk, hogy a közvetlen hasznosságbecslésre úgy tekinthetünk, mint ami jóval nagyobb <span class="emphasis"><em>U</em></span> hipotézistérben keres, mint amire szükség van. Ennek oka, hogy számos olyan függvényt is számon tart a hipotézisek közt, amelyek sértik a Bellman-egyenletet. Emiatt az algoritmus gyakran csak nagyon lassan konvergál.</p></div><div class="section" title="Adaptív dinamikus programozás"><div class="titlepage"><div><div><h2 class="title"><a id="id752154"/>Adaptív dinamikus programozás</h2></div></div></div><p>Ahhoz, hogy használhassuk az állapotok közti kényszerekben rejlő információt, az ágensnek meg kell tanulnia, hogy az állapotok között milyen kapcsolatok állnak fenn. Egy <span class="strong"><strong>adaptív dinamikus programozás</strong></span> (<span class="strong"><strong>ADP</strong></span>) (<span class="strong"><strong>adaptive dynamic programming</strong></span>, <span class="strong"><strong>ADP</strong></span>) alapú ágens működésének lényege, hogy menet közben megtanulja a környezet állapotátmenet-modelljét, és dinamikus programozási módszerrel megoldja a hozzá kapcsolható Markov döntési folyamatot. Egy passzív tanuló ágens számára ez azt jelenti, hogy a megtanult <span class="emphasis"><em>T</em></span>(<span class="emphasis"><em>s</em></span>, <span xml:lang="hu" class="emphasis"><em>π</em></span>(<span class="emphasis"><em>s</em></span>), <span class="emphasis"><em>s'</em></span>) állapotátmenet-modellt és a megfigyelt <span class="emphasis"><em>R</em></span>(<span class="emphasis"><em>s</em></span>) jutalmakat behelyettesíti a (21.2) Bellman-egyenletbe, majd kiszámítja az állapotok hasznosságát. Mint a 17. fejezetben a stratégiaiterációnál rámutattunk, ezek az egyenletek lineárisak (nincs maximalizálási lépés), így bármely lineáris algebrai programcsomaggal megoldhatók. Egy másik alternatíva, ha a <span class="strong"><strong>módosított eljárásmód-iteráció</strong></span>t (<span class="strong"><strong>modified policy iteration</strong></span>) választjuk (lásd <a class="xref" href="ch17s03.md#ID_720_oldal">„Eljárásmód-iteráció”</a> részben), egy egyszerűsített értékiterációs eljárást használva arra, hogy a tanult modell minden változása után frissítsük a hasznosságbecslés eredményét. Mivel a modell legtöbbször csak keveset változik az egyes megfigyelések után, az értékiterációs algoritmus kiinduló értékként felhasználhatja az előző hasznosságbecslést, és egész gyorsan konvergálhat.</p><p>Magának a modellnek egyszerű a megtanulása, mivel a környezet teljesen megfigyelhető. Ennek megfelelően egy ellenőrzött tanítási feladatunk van, amelyben a bemenet az állapot-cselekvés pár, a kimenet pedig a következő állapot. A legegyszerűbb esetben az állapotátmenet-modellt valószínűségek táblázatával reprezentálhatjuk. Számon tartjuk, hogy a cselekvések egyes eredményei milyen gyakran lépnek fel, és abból a gyakoriságból becsüljük a<span class="emphasis"><em> T</em></span>(<span class="emphasis"><em>s</em></span>, <span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>s'</em></span>) állapotátmenet valószínűségét, hogy hányszor következett <span class="emphasis"><em>s'</em></span>, amikor az <span class="emphasis"><em>s</em></span> állapotban az <span class="emphasis"><em>a</em></span> cselekvést választottuk.<sup>[<a id="id752249" href="#ftn.id752249" class="footnote">212</a>]</sup><sup> </sup>Például ha a <a class="xref" href="ch21s02.md#ID_875_oldal">„Passzív megerősítéses tanulás”</a> rész három útvonalában a <span class="emphasis"><em>Jobbra</em></span> cselekvést háromszor választottuk az (1,3) állapotban, és a háromból kétszer a (2, 3) lett a következő állapot, akkor <span class="emphasis"><em>T</em></span>((1, 3), <span class="emphasis"><em>Jobbra</em></span>, (2, 3)) becsült értéke 2/3 lesz.</p><p>A passzív ADP-ágens teljes programja a 21.2. ábrán látható. A 21.3. ábra mutatja a 4 × 3-as világon elért teljesítményét. Az értékbecsléseinek javulási sebessége szempontjából az ADP-ágens a lehetőségekhez képest a legjobban működik, ahogy csak az állapotátmenet-modell tanulási képességének megfelelően haladni tud. Ebben az értelemben más megerősítéses tanulási algoritmusok mérésének standardjaként szolgálhat. Ugyanakkor nagy állapotterek esetén némiképp kezelhetetlenné válhat. Például az ostáblajátékban nagyjából 10<sup>50</sup> egyenletet kell megoldania 10<sup>50</sup> ismeretlenre nézve.</p><div class="figure"><a id="id752282"/><p class="title"><strong>21.2. ábra - Egy adaptív lineáris programozás alapon tanuló passzív megerősítéses ágens. A kód egyszerűsítése érdekében feltettük, hogy minden egyes észlelés szétbontható egy észlelt állapotra és egy jutalomjelre.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/21-02.png" alt="Egy adaptív lineáris programozás alapon tanuló passzív megerősítéses ágens. A kód egyszerűsítése érdekében feltettük, hogy minden egyes észlelés szétbontható egy észlelt állapotra és egy jutalomjelre."/></div></div></div><div class="figure"><a id="id752291"/><p class="title"><strong>21.3. ábra - A passzív ADP-tanuló tanulási görbéi a 4 × 3-as világ tanulása során, a 21.1. ábrán látható optimális stratégia használata esetén. (a) Az állapotok egy kiválasztott részhalmazára kiszámított hasznosságbecslések a kísérletek számának függvényében. Figyeljük meg a 78-adik kísérlet környékén a jelentős változásokat – ez az első eset, amikor az ágens a (4,2) végállapotba jut, melyben –1 jutalmat kap. (b) Az <span class="emphasis"><em>U</em></span>(1,1) hibájának RMS-értéke 20 – egyenként 100 kísérletből álló – futásra átlagolva.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/21-03.png" alt="A passzív ADP-tanuló tanulási görbéi a 4 × 3-as világ tanulása során, a 21.1. ábrán látható optimális stratégia használata esetén. (a) Az állapotok egy kiválasztott részhalmazára kiszámított hasznosságbecslések a kísérletek számának függvényében. Figyeljük meg a 78-adik kísérlet környékén a jelentős változásokat – ez az első eset, amikor az ágens a (4,2) végállapotba jut, melyben –1 jutalmat kap. (b) Az U(1,1) hibájának RMS-értéke 20 – egyenként 100 kísérletből álló – futásra átlagolva."/></div></div></div></div><div class="section" title="Az időbeli különbség tanulása"><div class="titlepage"><div><div><h2 class="title"><a id="id752304"/>Az időbeli különbség tanulása</h2></div></div></div><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Lehetőségünk van a két világ (szinte) legjobb vonásait egyesíteni: közelíthetjük az állapotok közti kényszereket leíró egyenleteket anélkül, hogy megoldanánk az összes lehetséges állapotra. <span class="emphasis"><em>Az alapötlet a következő: használjuk fel a megfigyelt állapotátmeneteket a megfigyelt állapotok olyan módosítására, hogy azok megfeleljenek a korlátozó egyenleteknek.</em></span> Vegyük például az (1,3)-ból (2,3)-ba való átmenetet a <a class="xref" href="ch21s02.md#ID_875_oldal">„Passzív megerősítéses tanulás”</a> második kísérletében. Tegyük fel, hogy az első kísérlet eredményeként a hasznosságuk becsült értéke <span class="emphasis"><em>U<sup>π</sup></em></span>(1,3) = 0,84 és <span class="emphasis"><em>U<sup>π</sup></em></span>(2,3) = 0,92. Ha ez az átmenet mindig előáll, akkor azt várjuk, hogy a hasznosságoknak meg kell felelniük a következő kényszernek:</p><p><code class="code"><em><span class="remark">U<sup>π </sup></span></em>(1,3) = −0,04 + <em><span class="remark">U<sup>π </sup></span></em>(2,3)</code></p><p>így <span class="emphasis"><em>U<sup>π</sup></em></span>(1,3) eszerint 0,88 lenne. Tehát a jelenlegi becsült érték (0,84) egy kicsit alacsonyabb a kívánatosnál, növelni kellene. Általánosabban megfogalmazva, ha egy <span class="emphasis"><em>s</em></span>-ből <span class="emphasis"><em>s'</em></span>-be történő átmenet lép fel, akkor a következő módon frissítjük az <span class="emphasis"><em>U<sup>π</sup></em></span> (<span class="emphasis"><em>s</em></span>) hasznosságot:</p><p><code class="code"><em><span class="remark">U<sup>π</sup></span></em>(<em><span class="remark">s</span></em>) ← <em><span class="remark">U<sup>π</sup></span></em>(<em><span class="remark">s</span></em>) + <em><span class="remark">α</span></em>(<em><span class="remark">R</span></em>(<em><span class="remark">s</span></em>) + <em><span class="remark">γUπ</span></em>(<em><span class="remark">s'</span></em>)<em><span class="remark"> </span></em>−<em><span class="remark"> Uπ</span></em>(<em><span class="remark">s</span></em>))				(21.3)</code></p><p>Ebben az egyenletben <span xml:lang="hu" class="emphasis"><em>α</em></span> az úgynevezett <span class="strong"><strong>bátorsági faktor</strong></span> (<span class="strong"><strong>learning rate</strong></span>). Mivel ez az egyenlet a hasznosságok egymást követő állapotokhoz tartozó értékeit használja, ezért gyakran <span class="strong"><strong>időbeli különbs</strong></span><span class="strong"><strong>ég</strong></span> vagy <span class="strong"><strong>IK</strong></span>- (<span class="strong"><strong>temporal-difference</strong></span>, <span class="strong"><strong>TD</strong></span>) egyenletnek nevezik.</p></div><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Az időbeli különbségen alapuló módszerek alapötlete az, hogy először határozzuk meg azokat a feltételeket, amelyek korrekt becslési értékek esetén lokálisan fennállnak, majd állítsunk fel egy olyan értékfrissítésre szolgáló egyenletet, amely ezen ideális „egyensúlyi” egyenlet irányába viszi a becsléseket. Passzív tanulás esetén az egyensúlyi egyenletet a (21.2) szolgáltatja. A (21.3) egyenlet ténylegesen a (21.2) által megadott egyensúlyi helyzetbe viszi az ágenst, bár van némi finom trükk a dologban. Először vegyük észre, hogy a frissítés csak a megfigyelt következő állapotot, <span class="emphasis"><em>s'</em></span> -t veszi figyelembe, míg a tényleges egyensúlyi feltétel az összes lehetséges következő állapotot. Azt hihetnénk, hogy ez túlzottan megváltoztatja <span class="emphasis"><em>U<sup>π</sup></em></span>(<span class="emphasis"><em>s</em></span>)-t, amikor egy ritka átmenet következik be. Valójában viszont ritka átmenetek ritkán következnek be, ezért az <span class="emphasis"><em>U<sup>π</sup></em></span>(<span class="emphasis"><em>s</em></span>) <span class="emphasis"><em>átlagos értéke</em></span> a helyes értékhez fog konvergálni. Továbbá, ha <span xml:lang="hu" class="emphasis"><em>α</em></span>-t megváltoztatjuk úgy, hogy ne egy rögzített paraméter legyen, hanem egy függvény, amely aszerint csökken, ahogy egy állapotnak a kísérletekben történő előfordulása nő, akkor maga az <span class="emphasis"><em>U</em></span>(<span class="emphasis"><em>s</em></span>) is a helyes értékhez fog tartani.<sup>[<a id="id752510" href="#ftn.id752510" class="footnote">213</a>]</sup> Ez a 21.4. ábrán látható ágensprogramhoz vezet. A 21.5. ábra bemutatja a passzív IK-ágens által a 4 × 3 világban mutatott teljesítményt. Ugyan nem tanul olyan gyorsan, mint az ADP-ágens, és nagyobb változékonyságot mutat, de sokkal kevesebb számítást igényel megfigyelésenként. Vegyük észre, hogy az IK <span class="emphasis"><em>nem igényel modellt a frissítés elvégzéséhez</em></span>. A környezet szolgáltatja a szomszédos állapotok közti kapcsolatokat, a megfigyelt átmenetek formájában.</p></div><div class="figure"><a id="id752559"/><p class="title"><strong>21.4. ábra - Egy időbeli különbség alapon hasznosságot tanuló passzív megerősítéses ágens</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/21-04.png" alt="Egy időbeli különbség alapon hasznosságot tanuló passzív megerősítéses ágens"/></div></div></div><div class="figure"><a id="id752568"/><p class="title"><strong>21.5. ábra - Az IK-tanulás 4 × 3-as világon mért tanulási görbéi. (a) Néhány kiválasztott állapot hasznosságbecslése a kísérletszám függvényében. (b) Az <span class="emphasis"><em>U</em></span>(1,1) becslésének RMS-hibája, amelyet 20 futás átlagából számoltunk, mindegyik futás 500 kísérletet tartalmazott. Csak az első 100 kísérletből származó eredményt rajzoltuk fel, hogy a görbe a 21.3. ábrával összehasonlítható legyen.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/21-05.png" alt="Az IK-tanulás 4 × 3-as világon mért tanulási görbéi. (a) Néhány kiválasztott állapot hasznosságbecslése a kísérletszám függvényében. (b) Az U(1,1) becslésének RMS-hibája, amelyet 20 futás átlagából számoltunk, mindegyik futás 500 kísérletet tartalmazott. Csak az első 100 kísérletből származó eredményt rajzoltuk fel, hogy a görbe a 21.3. ábrával összehasonlítható legyen."/></div></div></div><p>Az ADP- és IK-megközelítés valójában szoros rokonságban van. Mindkettő lokális változtatásokat végez a hasznosságbecslésben annak érdekében, hogy minden egyes állapot „megfeleljen” a rákövetkezőknek. Ugyanakkor van egy különbség. Az IK úgy módosítja az állapotot, hogy az megfeleljen a <span class="emphasis"><em>megfigyelt</em></span> rákövetkező állapotnak ((21.3) egyenlet). Az ADP viszont úgy módosítja az aktuális állapotot, hogy az <span class="emphasis"><em>összes</em></span> lehetséges következő állapotnak megfeleljen, de ezeket az állapotokat a valószínűségükkel súlyozza (21.2) egyenlet). Ez a különbség eltűnik, amikor az IK-módosítást nagyszámú átmenetre átlagoljuk, mivel az állapothalmaz következő állapotainak gyakorisága nagyjából arányos a valószínűségükkel. Egy, az előzőnél fontosabb különbség, hogy míg az IK minden megfigyelt állapotátmenet után egyetlen módosítást végez, addig az ADP annyit, amennyi csak szükséges ahhoz, hogy helyreállítsa az <span class="emphasis"><em>U</em></span> hasznosságbecslések és a <span class="emphasis"><em>T</em></span> környezetmodell közti konzisztenciát. Bár a megfigyelt állapotátmenet csak lokálisan változtatja meg <span class="emphasis"><em>T</em></span>-t, de szükség lehet arra, hogy ezt végigterjesszük <span class="emphasis"><em>U</em></span>-n. Ennek megfelelően úgy vehetjük, hogy az IK egy durva, de hatékony közelítése az ADP-módszernek.</p><p>Mindegyik, az ADP által elvégzett módosítás az IK szempontjából úgy is tekinthető, mint az aktuális környezeti modell szimulációja segítségével generált „pszeudokísérlet” eredménye. Kiterjeszthetjük az IK-megközelítést úgy, hogy a környezeti modell segítségével számos pszeudokísérletet generáljon – létrehozva olyan átmeneteket, amelyekről az aktuális környezet modellje alapján az IK-ágens el tudja képzelni, hogy <span class="emphasis"><em>megtörténhetnek</em></span>. Az IK-ágens akár nagyszámú képzelt átmenetet is generálhat minden megfigyelt átmenetre. Ennek eredményeképpen a hasznosság becslései egyre inkább az ADP becsléseihez közelítenek, természetesen a fizetendő ár a megnövekedett számítási idő lesz.</p><p>Hasonló módon az ADP-eljárás hatékonyabb változatait hozhatjuk létre közvetlenül approximálva az értékiteráció és stratégiaiteráció algoritmusait. Ne felejtsük, hogy a teljes értékiteráció kezelhetetlenné válhat, ha az állapottér nagy. Ugyanakkor a módosítási lépések közül számos csak nagyon parányi. Elfogadhatóan jó válaszok gyors generálásához vezető lehetséges megközelítés, ha megkötjük a megfigyelt átmenetek után elvégzett módosítások számát. Azt is megtehetjük, hogy valamilyen heurisztika segítségével rangsoroljuk a lehetséges módosításokat, és csak a legfontosabbakat végezzük el. A <span class="strong"><strong>prior</strong></span><span class="strong"><strong>itásos végigsöprés</strong></span> (<span class="strong"><strong>prioritized sweeping</strong></span>) heurisztikája preferálja azon állapotok módosítását, amelyek <span class="emphasis"><em>valószínű</em></span> követő állapotainak hasznosságában éppen most <span class="emphasis"><em>nagy</em></span> módosítás történt. Ehhez hasonló heurisztikák segítségével a közelítő ADP-algoritmusok rendszerint nagyjából ugyanolyan gyorsan képesek tanulni, mint a teljes ADP, ha pusztán a tanító szekvenciák számát nézzük. Viszont több nagyságrenddel hatékonyabbak lehetnek, ha a számítási igényt vizsgáljuk (lásd 21.3. feladat). Ezáltal lehetővé válik számukra olyan állapotterek vizsgálata is, amelyek messze túl nagyok a teljes ADP-nek. A közelítő ADP-algoritmusoknak további előnye, hogy egy új környezet tanulásának korai fázisában a <span class="emphasis"><em>T</em></span> környezeti modell gyakran olyan távol van a helyestől, hogy nincs sok értelme egy azzal pontosan konzisztens hasznosságfüggvényt kiszámítani. A közelítő algoritmus használhat olyan minimális méretű módosítást, amely csökken annak mentén, ahogy a környezeti modell egyre pontosabbá válik. Ez elkerülhetővé teszi a nagyon hosszú értékiterációkat, amelyek a tanulás korai szakaszában azért lépnek fel, mert nagy változások történnek a modellben.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.id752249" href="#id752249" class="para">212</a>] </sup> Ez – mint a 20. fejezetben is tárgyaltuk – a maximum-likelihood becslés. Egy Bayes-frissítés Dirichlet-priorral jobb is lehet.</p></div><div class="footnote"><p><sup>[<a id="ftn.id752510" href="#id752510" class="para">213</a>] </sup> Technikailag azt kívánjuk meg, hogy <span class="inlinemediaobject"><img src="math/mi-21-0030.gif" alt="Az időbeli különbség tanulása"/></span> és <span class="inlinemediaobject"><img src="math/mi-21-0031.gif" alt="Az időbeli különbség tanulása"/></span> fennálljon. Az <span xml:lang="hu" class="emphasis"><em>α</em></span> (<span class="emphasis"><em>n</em></span>) = 1/<span class="emphasis"><em>n</em></span> csökkenés megfelel ennek a feltételnek. A 21.5. ábrán <span xml:lang="hu" class="emphasis"><em>α</em></span>(<span class="emphasis"><em>n</em></span>) = 60/(59 + <span class="emphasis"><em>n</em></span>)-t használtuk.</p></div></div></div></body></html>
