<?xml version="1.0" encoding="UTF-8" standalone="no"?>

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="chapter" title="23. fejezet - Valószínűségi nyelv-feldolgozás"><div class="titlepage"><div><div><h1 class="title"><a id="id772386"/>23. fejezet - Valószínűségi nyelv-feldolgozás</h1></div></div></div><p><span class="emphasis"><em>Ebben a fejezetben meglátjuk, hogyan lehet egyszerű, statisztikailag tanított nyelvi modelleket szavak millióinak feldolgozására használni ahelyett, hogy csak egyes mondatok feldolgozására használnánk</em></span></p><p>A 22. fejezetben<span class="emphasis"><em> </em></span>láttuk, hogyan képes egy ágens kommunikálni egy másik (szoftver vagy emberi) ágenssel közös nyelvi megnyilatkozások segítségével. A megnyilatkozások teljes szintaktikai és szemantikai elemzése <span class="emphasis"><em>szükséges</em></span> a jelentésük teljes kinyeréséhez, ami azért <span class="emphasis"><em>lehetséges,</em></span> mert a megnyilatkozások rövidek és körülhatárolt tárgyterületre korlátozottak.</p><p>Ebben a fejezetben a nyelvmegértés <span class="strong"><strong>korpuszalapú</strong></span> (<span class="strong"><strong>corpus-based</strong></span>) megközelítését tárgyaljuk. A korpusz egy nagy szöveggyűjtemény, például az a több milliárd oldal, ami a világhálót építi fel. A szövegeket emberek írják embereknek, és a szoftverek feladata az, hogy megkönnyítsék az embereknek az információkeresést. Ez a megközelítés magában foglalja statisztikák és tanulás használatát a korpusz kihasználására, és jellemzően olyan valószínűségi nyelvi modelleket von maga után, amelyek az adatokból tanulhatók, és amelyek egyszerűbbek, mint a 22. fejezet kibővített DCG-i. A legtöbb feladat esetén az adatmennyiség kárpótol azért, hogy egyszerűbb nyelvi modellt használunk. Három specifikus feladatot fogunk áttekinteni: az <span class="strong"><strong>információkeresés</strong></span>t (<span class="strong"><strong>information retrieval</strong></span>) (23.2. alfejezet), az <span class="strong"><strong>információkinyerés</strong></span>t (<span class="strong"><strong>information extraction</strong></span>) (23.3. alfejezet) és a <span class="strong"><strong>gépi fordítás</strong></span>t (<span class="strong"><strong>machine translation</strong></span>) (23.4. alfejezet). Elsőként azonban a valószínűségi nyelvi modelleket (probabilistic language model) mutatjuk be röviden.</p><div class="section" title="Valószínűségi nyelvi modellek"><div class="titlepage"><div><div><h1 class="title"><a id="id772447"/>Valószínűségi nyelvi modellek</h1></div></div></div><p>A 22. fejezet a nyelv <span class="emphasis"><em>logikai</em></span> modelljét adta meg: CFG-ket és DCG-ket használtunk, hogy egy adott karakterfüzérről eldöntsük, eleme-e vagy sem egy nyelvnek. Ebben az alfejezetben számos <span class="emphasis"><em>valószínűségi</em></span> modellt vezetünk be. A valószínűségi modellek számos előnnyel rendelkeznek. Kényelmesen taníthatók adatok alapján: a tanulás mindössze az előfordulások megszámlálásából áll (némi tűréssel a kis mintaméret okozta hibák miatt). Továbbá robusztusabbak (mivel <span class="emphasis"><em>bármely</em></span> karakterfüzért elfogadnak, habár kis valószínűséggel), visszatükrözik azt a tényt, miszerint nem a beszélők 100%-a ért egyet abban, hogy mely mondatok részei a nyelvnek; valamint alkalmasak a többértelműség feloldására: a valószínűségre támaszkodva kiválasztható a legvalószínűbb értelmezés.</p><p>A <span class="strong"><strong>valószínűségi nyelvi modell</strong></span> (<span class="strong"><strong>probabilistic language model</strong></span>) valószínűségi eloszlást definiál egy (esetleg végtelen) karakterfüzér-halmaz felett. Példaként állhatnak a 15.6. alfejezetben beszédmegértésre használt bi- és trigram nyelvi modellek. Az unigram modell <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>w</em></span>) valószínűséget rendel a szókincs minden egyes szavához. A modell feltételezi, hogy a szavak függetlenül lettek kiválasztva, azaz a karakterfüzér valószínűsége egyszerűen a szavak valószínűségének szorzata:<span class="inlinemediaobject"><img src="math/mi-23-0001.gif" alt="Valószínűségi nyelvi modellek"/></span>. A következő húszszavas szekvenciát véletlen módon generáltuk, a könyv szavainak unigram modellje alapján:<sup>[<a id="id772493" href="#ftn.id772493" class="footnote">236</a>]</sup></p><p>logical are as confusing a may right tries agent goal the was diesel more object then information-gathering search is</p><a id="ID_952_oldal"/><p>A bigram modell egy P(<span class="emphasis"><em>w<sub>i</sub></em></span>|<span class="emphasis"><em>w<sub>i–</sub></em></span><sub>1</sub>) valószínűséget rendel minden egyes szóhoz, adott előző szó esetén. A 15.21. ábra néhány bigram-valószínűséget mutatott be. A könyv bigram modellje a következő véletlen szekvenciát generálja:</p><p>planning purely diagnostic expert system are very similar computational approach would be represented compactly using tic tac toe a predicate</p><p>Általánosan, egy <span class="emphasis"><em>n</em></span>-gram modell az előző <span class="emphasis"><em>n</em></span> – 1 szó alapján szabja meg a P(<span class="emphasis"><em>w<sub>i</sub></em></span>|<span class="emphasis"><em>w</em></span><sub><span class="emphasis"><em>i–(n–</em></span>1<span class="emphasis"><em>)</em></span></sub>…<span class="emphasis"><em>w</em></span><sub><span class="emphasis"><em>i–</em></span>1</sub>) valószínűséget. A könyv trigram modellje ezt a véletlen szekvenciát generálja:</p><p>planning and scheduling are integrated the success of naive bayes model is just a possible prior source by the time</p><p>Még a fenti kis példa alapján is láthatónak kell lennie, hogy a trigram modell jobb, mint a bigram (amely pedig jobb, mint az unigram), mind az angol nyelv, mind egy MI-tankönyv témájának közelítésében. A modellek maguk is egyetértenek ezzel: a trigram modell a véletlen módon generált sztringjéhez 10<sup>–10</sup> valószínűséget rendel, a bigram 10<sup>–29</sup>-et, az unigram pedig 10<sup>–59</sup>-et.</p><p>Ez a könyv félmillió szavával nem tartalmaz elég adatot ahhoz, hogy jó minőségű bigram modellt lehessen előállítani belőle, nem is beszélve a trigram modellről. A könyv szókincse körülbelül 15 ezer különböző szót tartalmaz, tehát a bigram modell 15 000<sup>2</sup> = 225 millió szópárt tartalmaz. Világos, hogy a szópárok legalább 99,8%-a 0 gyakoriságú, de nem akarjuk, hogy a modell azt állítsa, hogy ezek a szópárok lehetetlenek. Szükségünk van valamilyen <span class="strong"><strong>simítás</strong></span>ra (<span class="strong"><strong>smoothing</strong></span>) a nulla gyakoriságok felett. A legegyszerűbb megoldás az <span class="strong"><strong>adj-hozzá-egyet simítás</strong></span> (<span class="strong"><strong>add-one smoothing</strong></span>): minden lehetséges bigram gyakoriságához hozzáadunk egyet. Azaz amennyiben a korpuszban <span class="emphasis"><em>N</em></span> szó és <span class="emphasis"><em>B</em></span> lehetséges bigram található, akkor minden <span class="emphasis"><em>c</em></span> gyakoriságú bigramhoz egy (<span class="emphasis"><em>c + </em></span>1)/(<span class="emphasis"><em>N + B</em></span>) értékű valószínűség-becslőt rendelünk. Ez a módszer megszünteti a nulla valószínűségű <span class="emphasis"><em>n</em></span>-gramok problémáját, de az a követelmény, hogy minden gyakoriságot pontosan eggyel kell növelni, kétséges, és rossz becslésekhez vezethet.</p><p>A másik megközelítés a <span class="strong"><strong>lineáris interpoláción alapuló simítás</strong></span> (<span class="strong"><strong>linear interpolation smoothing</strong></span>), ami lineárisan kombinálja a trigam, bigram és unigram modelleket. A valószínűség-becslőt a következőképpen definiáljuk:</p><p><span class="inlinemediaobject"><img src="math/mi-23-0002.gif" alt="Valószínűségi nyelvi modellek"/></span></p><p>ahol <span class="emphasis"><em>c</em></span><sub>3 </sub>+  <span class="emphasis"><em>c</em></span><sub>2 </sub><span class="emphasis"><em>+ c</em></span><sub>1</sub> = 1. A <span class="emphasis"><em>c<sub>i</sub></em></span> paraméterek lehetnek rögzítettek vagy EM algoritmussal taníthatók. Az is lehetséges, hogy a <span class="emphasis"><em>c<sub>i</sub></em></span> értékeket az <span class="emphasis"><em>n</em></span>-gram gyakoriságoktól függőnek választjuk, azaz nagyobb súlyt adunk azoknak a valószínűségi becslőknek, amelyeket nagyobb gyakoriságokból számítunk.</p><p>A nyelvi modell egyik lehetséges <span class="emphasis"><em>kiértékelési</em></span> módja a következő: először válasszuk szét a korpuszt egy tanító és egy teszthalmazra. Határozzuk meg a modell paramétereit a tanító halmaz alapján. Ezután számítsuk ki a valószínűséget a teszthalmazra a modell alapján; minél nagyobb a valószínűség, annál jobb. A megközelítés egyik problémája az, hogy hosszú karakterfüzérek esetén a <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>szavak</em></span>) nagyon kicsi; a kis számok lebegőpontos alulcsordulást okozhatnak, vagy egyszerűen túl nehéz lenne elolvasni őket. Tehát a valószínűség helyett a modell <span class="strong"><strong>összetettség</strong></span>ét (<span class="strong"><strong>perplexity</strong></span>) számíthatjuk ki a teszt karakterfüzéren:</p><p><span class="inlinemediaobject"><img src="math/mi-23-0003.gif" alt="Valószínűségi nyelvi modellek"/></span></p><p>ahol <span class="emphasis"><em>N</em></span> a <span class="emphasis"><em>szavak</em></span> száma. Minél kisebb az összetettség, annál jobb a modell. Az az <span class="emphasis"><em>n</em></span>-gram modell, amely minden szóhoz 1/<span class="emphasis"><em>k</em></span> valószínűséget rendel, <span class="emphasis"><em>k</em></span> összetettségű; az összetettséget úgy is lehet értelmezni, mint átlagos elágazási tényezőt.</p><p>Példaként arra, hogy mire képesek az <span class="emphasis"><em>n</em></span>-gram modellek, vegyük a <span class="strong"><strong>szegmentáció</strong></span> (<span class="strong"><strong>segmentation</strong></span>) feladatát, ami szóhatárok megtalálása szóköz nélküli szövegben. Ez a feladat elengedhetetlen a japán és kínai nyelv esetén; ezek olyan nyelvek, amelyek nem raknak szóközt a szavak közé, feltételezzük azonban, hogy a legtöbb olvasónak az angol nyelv<sup>[<a id="id772747" href="#ftn.id772747" class="footnote">237</a>]</sup> jobban megfelel. Az </p><p>Itiseasytoreadwordswithoutspaces<sup>[<a id="id772763" href="#ftn.id772763" class="footnote">238</a>]</sup></p><p>mondatatot tényleg könnyű elolvasni. Az olvasó arra gondolhat, hogy ezt az teszi lehetővé, hogy teljesen ismerjük az angol szintaktikát, szemantikát és pragmatikát. Meg fogjuk mutatni, hogy a mondatot egy egyszerű unigram modellel is könnyen dekódolni lehet.</p><p>Korábban láthattuk, hogy a Viterbi algoritmus (15.9) hogyan használható a legvalószínűbb szekvencia megtalálására egy szó-valószínűségi hálóban. A 23.1. ábrán látható a Viterbi algoritmus olyan változata, amelyet specifikusan a szegmentációs probléma megoldására terveztünk. Bemenete a <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>szó</em></span>) unigram valószínűségi eloszlás és egy karakterfüzér. Ezután az algoritmus a karakterfüzér minden egyes <span class="emphasis"><em>i</em></span> pozíciójára a <span class="emphasis"><em>legjobb</em></span>[<span class="emphasis"><em>i</em></span>] elemben eltárolja a legvalószínűbb <span class="emphasis"><em>i</em></span>-ig tartó karakterfüzér valószínűségét. Emellett a <span class="emphasis"><em>szavak</em></span>[<span class="emphasis"><em>i</em></span>] elemben eltárolja azt az <span class="emphasis"><em>i</em></span>-edik pozícióban végződő szót, ami a legnagyobb valószínűséget adta. Miután felépítette a <span class="emphasis"><em>legjobb</em></span> és a <span class="emphasis"><em>szavak</em></span> tömböket dinamikus programozási módon, hátrafelé mozogva feldolgozza a <span class="emphasis"><em>szavak</em></span> tömböt, hogy megtalálja a legjobb utat. Ebben az esetben, a könyv unigram modellje alapján a legjobb szekvencia ténylegesen az „It is easy to read words without spaces”,<sup>[<a id="id772821" href="#ftn.id772821" class="footnote">239</a>]</sup> 10<sup>–25</sup> valószínűséggel. A szekvencia részeinek összehasonlítása során látható, hogy az „easy” unigram valószínűsége 2,6 × 10<sup>–4</sup>, miközben az alternatív „<span class="emphasis"><em>e</em></span> as <span class="emphasis"><em>y</em></span>”<sup>[<a id="id772846" href="#ftn.id772846" class="footnote">240</a>]</sup> valószínűsége sokkal kisebb, 9,8 × 10<sup>–12</sup> annak ellenére, hogy a könyv képleteiben viszonylag gyakran előfordul az „<span class="emphasis"><em>e</em></span>” és az „<span class="emphasis"><em>y</em></span>”. Hasonlóképpen:</p><p><code class="code"><em><span class="remark">P</span></em>(„without”) = 0,0004</code></p><p><code class="code"><em><span class="remark">P</span></em>(„with”) = 0,005; <em><span class="remark">P</span></em>(„out”) = 0,0008</code></p><p><code class="code"><em><span class="remark">P</span></em>(„with out”) = 0,005 × 0,0008 = 0,000004</code></p><p>Ennek következtében az unigram modell szerint a „without” százszor nagyobb valószínűségű, mint a „with out”.<sup>[<a id="id772897" href="#ftn.id772897" class="footnote">241</a>]</sup></p><div class="figure"><a id="id772917"/><p class="title"><strong>23.1. ábra - Viterbi-alapú szószegmentáló algoritmus. Egy szóközöket nem tartalmazó szófüzért feldolgozva megadja a legvalószínűbb szavakra történő szegmentációt.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/23-01.png" alt="Viterbi-alapú szószegmentáló algoritmus. Egy szóközöket nem tartalmazó szófüzért feldolgozva megadja a legvalószínűbb szavakra történő szegmentációt."/></div></div></div><p>Ebben a bekezdésben a szavak feletti <span class="emphasis"><em>n</em></span>-gram modellekről értekeztünk, azonban az <span class="emphasis"><em>n</em></span>-gram modelleket számos egyéb egység – például karakterek vagy <span class="strong"><strong>beszédrész</strong></span>ek (<span class="strong"><strong>part</strong></span>s<span class="strong"><strong> of speech</strong></span>) – felett is lehet értelmezni.</p><div class="section" title="Valószínűségi környezetfüggetlen nyelvtanok"><div class="titlepage"><div><div><h2 class="title"><a id="id772950"/>Valószínűségi környezetfüggetlen nyelvtanok</h2></div></div></div><p>Az <span class="emphasis"><em>n</em></span>-gram modellek a korpuszon belüli közös előfordulási statisztikát használják ki, azonban nincs semmilyen információjuk a nyelvtanról <span class="emphasis"><em>n</em></span>-nél nagyobb távolságra. A <span class="strong"><strong>való</strong></span><span class="strong"><strong>színűségi környezetfüggetlen nyelvtan</strong></span> – <span class="strong"><strong>PCFG</strong></span><sup>[<a id="id772977" href="#ftn.id772977" class="footnote">242</a>]</sup> (<span class="strong"><strong>probabilistic context-free grammar</strong></span>) – egy alternatív nyelvi modell, ami egy olyan CFG, melyben minden átírási szabályhoz valószínűséget rendelünk. Az azonos bal oldallal rendelkező szabályok valószínűségének összege 1. A 23.2. ábrán az ℰ<sub>0</sub> nyelvtan egy részletének PCFG-je látható.</p><div class="figure"><a id="id772990"/><p class="title"><strong>23.2. ábra - Az ℰ<sub>0</sub> nyelvtan egy részletének valószínűségi környezetfüggetlen nyelvtana (PCFG) és szókincse. A szögletes zárójelben levő szám jelzi a valószínűségét annak, hogy az adott bal oldali szimbólumot a megfelelő szabály szerint írjuk át</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/23-02.png" alt="Az ℰ0 nyelvtan egy részletének valószínűségi környezetfüggetlen nyelvtana (PCFG) és szókincse. A szögletes zárójelben levő szám jelzi a valószínűségét annak, hogy az adott bal oldali szimbólumot a megfelelő szabály szerint írjuk át"/></div></div></div><div class="figure"><a id="id773003"/><p class="title"><strong>23.3. ábra - A „Minden wumpus bűzlik” mondat elemzési fája, megadva minden egyes részfa valószínűségét. A teljes fa valószínűsége 1,0 × 0,5 × 0,05 × 0,15 × 0,60 × 0,10 = 0,000225. Mivel a mondatnak ez az egyetlen elemzése, ezért ennyi a mondat valószínűsége is.</strong></p><div class="figure-contents"><div class="mediaobject"><img src="kepek/23-03.png" alt="A „Minden wumpus bűzlik” mondat elemzési fája, megadva minden egyes részfa valószínűségét. A teljes fa valószínűsége 1,0 × 0,5 × 0,05 × 0,15 × 0,60 × 0,10 = 0,000225. Mivel a mondatnak ez az egyetlen elemzése, ezért ennyi a mondat valószínűsége is."/></div></div></div><p>A PCFG-modellben egy karakterfüzér valószínűsége – a <span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>szavak</em></span>) – egyszerűen az elemzési fái valószínűségeinek összege. Egy adott fa valószínűsége a fa csomópontjait felépítő szabályok valószínűségeinek szorzata. A 23.3. ábra bemutatja, hogyan lehet kiszámítani egy mondat valószínűségét. A valószínűség kiszámítható egy CFG diagramelemző alkalmazásával, amely megadja az összes lehetséges elemzést, majd egyszerűen össze kell adni a valószínűségeket. Azonban ha csak a legvalószínűbb elemzés érdekel minket, akkor a nem valószínű elemzések meghatározása pazarlás. A legvalószínűbb elemzés hatékony megtalálására használhatunk egy Viterbi algoritmus variációt vagy egy legjobbat-először keresési technikát (mint például az A<sup>*</sup>-ot).</p><p>A PCFG-vel az a probléma, hogy környezetfüggetlen. Ez azt jelenti, hogy a különbség a <span class="emphasis"><em>P</em></span>(„eat a banana”) és a <span class="emphasis"><em>P</em></span>(„eat a bandanna”)<sup>[<a id="id773035" href="#ftn.id773035" class="footnote">243</a>]</sup> között mindössze a <span class="emphasis"><em>P</em></span>(„banana”) és <span class="emphasis"><em>P</em></span>(„bandanna”) közti különbségtől függ, és nem az „enni” ige és a megfelelő objektumok közti kapcsolattól. Hogy megkapjuk ezt a kapcsolatot, szükség van valamilyen környezetfüggő modellre, mint például a <span class="strong"><strong>szókinccsel ellátott PCFG</strong></span>-re (<span class="strong"><strong>lexicalized PCFG</strong></span>), amelyben a kifejezés feje<sup>[<a id="id773063" href="#ftn.id773063" class="footnote">244</a>]</sup> szerepet játszhat a kifejezés valószínűségének meghatározásában. Elegendő tanító adat esetén a <span class="emphasis"><em>VP</em></span> → <span class="emphasis"><em>VP</em></span> <span class="emphasis"><em>NP</em></span> szabály kondicionálható a beágyazott <span class="emphasis"><em>VP</em></span> fejére („enni”) és az <span class="emphasis"><em>NP</em></span> fejére („banán”). Ezáltal a szókinccsel ellátott PCFG-k képesek az <span class="emphasis"><em>n</em></span>-gram modellek közös előfordulási megszorításai egy részének a megragadására, megtartva a CFG-modellek nyelvtani megszorításait.</p><p>A PCFG másik problémája az, hogy erősen preferálja a rövid mondatokat. Egy olyan korpuszban, mint a <span class="emphasis"><em>Wall Street Journal</em></span>, az átlagos mondathossz körülbelül 25 szó. Azonban egy PCFG általában úgy végez, hogy viszonylag magas valószínűséget rendel olyan szabályokhoz, mint az <span class="emphasis"><em>S</em></span> → <span class="emphasis"><em>NP</em></span> <span class="emphasis"><em>VP</em></span>, az <span class="emphasis"><em>NP</em></span> → <span class="emphasis"><em>Pronoun</em></span> és a <span class="emphasis"><em>VP</em></span> → <span class="emphasis"><em>Verb</em></span>. Ez azt jelenti, hogy a PCFG viszonylag nagy valószínűséget fog rendelni sok rövid mondathoz, mint pl. „ő aludt”, azonban a <span class="emphasis"><em>Journalban</em></span> sokkal valószínűbben találkozunk olyan mondatokkal, mint „Egy megbízható kormányzati forrás jelentése szerint az az állítás, hogy aludt, hihető”. Úgy tűnik, hogy a <span class="emphasis"><em>Journal</em></span> mondatai igazából nem környezetfüggetlenek, hanem az íróknak van elképzelésük az elvárt mondathosszról, amit fel is használnak lágy globális kényszerként a mondatok írásakor. Ezt nehéz visszatükrözni egy PCFG-ben.</p></div><div class="section" title="PCFG-valószínűségek tanulása"><div class="titlepage"><div><div><h2 class="title"><a id="id774069"/>PCFG-valószínűségek tanulása</h2></div></div></div><p>Egy PCFG-modell létrehozásához a CFG konstrukciójának összes nehézségével szembesülünk, ehhez hozzátevődik még az egyes szabályok valószínűséggel való ellátása. Ez azt sugallja, hogy a nyelvtan adatokból való <span class="strong"><strong>tanulás</strong></span>a (<span class="strong"><strong>learning</strong></span>) hasznosabb lehet, mint a tudásmérnöki megközelítés. Csakúgy, mint a beszédmegértés esetén is, kétféle adat áll rendelkezésre: elemzett és nem elemzett. A feladat sokkal egyszerűbb, ha az adatok elemzési fáját nyelvészek (vagy legalábbis képzett anyanyelvi beszélők) készítették el. Egy ilyen korpusz elkészítése óriási feladat, a legnagyobb korpuszok „mindössze” körülbelül egymillió szót tartalmaznak. Az elemzési fa korpusz alapján a PCFG-t egyszerűen számlálással (és simítással) készítjük el: minden egyes nem záró szimbólumra megnézzük az összes olyan csomópontot, amelynek ez a szimbólum a gyökere, és előállítjuk a csomópontok gyermekeinek összes különböző kombinációit leíró szabályokat. Például ha az <span class="emphasis"><em>NP</em></span> szimbólum 100 000-szer fordul elő, és ebből 20 000 esetben a gyermekek listája [<span class="emphasis"><em>NP</em></span>, <span class="emphasis"><em>PP</em></span>], akkor a következő szabályt állítjuk elő:</p><p><code class="code"><em><span class="remark">NP</span></em> → <em><span class="remark">NP</span></em> <em><span class="remark">PP</span></em> [0,20]</code></p><p>A feladat sokkal nehezebb, ha csak elemzetlen szöveggel rendelkezünk. Először is két problémával szembesülünk: a nyelvtani szabályok struktúrájának és az egyes szabályok valószínűségének megtanulásával. (Ugyanezt a megkülönböztetést tesszük neurális hálózatok, valamint Bayes-hálók tanulása esetén is.)</p><p>Pillanatnyilag feltételezzük, hogy a szabályok struktúrája adott, és csak a valószínűségeket próbáljuk megtanulni. Alkalmazhatunk egy várhatóérték-maximalizálás (<span class="strong"><strong>expectation– maximization</strong></span>, <span class="strong"><strong>EM</strong></span>) módszert, úgy, mint az RMM-ek tanulásánál. A paraméterek – amelyeket tanulni próbálunk – a szabály-valószínűségek. A rejtett változók az elemzési fák: nem tudjuk, hogy a <span class="emphasis"><em>w<sub>i</sub></em></span>…<span class="emphasis"><em>w<sub>j</sub></em></span> szavakból álló karakterfüzért ténylegesen az <span class="emphasis"><em>X</em></span> → <span xml:lang="hu" class="emphasis"><em>α</em></span> szabály generálja-e, vagy sem. Az E lépés megbecsüli az egyes részszekvenciák egyes szabályok által történő generálásának valószínűségét. Ezután az M lépés megbecsüli az egyes szabályok valószínűségét. Az egész számítást el lehet végezni dinamikus programozási módon, az ún. <span class="strong"><strong>belső–külső</strong></span> (<span class="strong"><strong>inside–outside</strong></span>) <span class="strong"><strong>algoritmus</strong></span>sal, ami a HMM tanulás előre–hátra algoritmusának analógiája.</p><p>A belső–külső algoritmus varázslatosnak tűnik, hiszen elemzetlen szövegekből állít elő nyelvtant. Azonban számos hátránnyal rendelkezik. Először is lassú: ahol <span class="emphasis"><em>n</em></span> a mondatbeli szavak, <span class="emphasis"><em>t</em></span> pedig a nem záró szimbólumok száma. Másodsorban, a valószínűségi hozzárendelések tere nagyon nagy, és a tapasztalatok alapján a lokális maximumokban való bennragadás súlyos probléma. Alternatív módszerek – például szimulált lehűtés – megpróbálhatók ugyan, de ezek még nagyobb számításigényűek. Harmadsorban, a kapott nyelvtanok által elvégzett elemzések gyakran nehezen érthetők, és nem elégítik ki a nyelvészeket. Ez megnehezíti a manuálisan előállított tudás kombinálását az automatikus indukcióval.</p></div><div class="section" title="PCFG-szabálystruktúrák tanulása"><div class="titlepage"><div><div><h2 class="title"><a id="id774167"/>PCFG-szabálystruktúrák tanulása</h2></div></div></div><p>Most pedig tételezzük fel, hogy a nyelvtani szabályok struktúrája nem ismert. Az első probléma, amivel szembesülünk, az, hogy a lehetséges szabályhalmazok tere végtelen, azaz nem tudjuk, hogy hány szabályt vegyünk figyelembe, és azt sem, hogy az egyes szabályok milyen hosszúak lehetnek. A probléma egyik lehetséges megkerülése az, hogy a nyelvtani szabályokat <span class="strong"><strong>Chomsky normál alak</strong></span>ban (<span class="strong"><strong>Chomsky normal form</strong></span>) tanuljuk, ami azt jelenti, hogy minden szabály a következő két alak egyike lehet:</p><p><code class="code"><em><span class="remark">X</span></em> → <em><span class="remark">Y</span></em> <em><span class="remark">Z</span></em></code></p><p><code class="code"><em><span class="remark">X</span></em> → <em><span class="remark">t</span></em></code></p><p>ahol <span class="emphasis"><em>X</em></span>, <span class="emphasis"><em>Y</em></span> és <span class="emphasis"><em>Z</em></span> nem záró, míg <span class="strong"><strong>t</strong></span> záró szimbólum. Minden környezetfüggetlen nyelvtant át lehet írni Chomsky normál alakra, amely pontosan ugyanazt a nyelvet fogadja el. Ezután önhatalmúlag <span class="emphasis"><em>n</em></span> nem záró szimbólumra szorítkozhatunk, ezáltal <span class="emphasis"><em>n</em></span><sup>3</sup> + <span class="emphasis"><em>nv</em></span> szabályt kapunk, ahol <span class="emphasis"><em>v</em></span> a záró szimbólumok száma. A gyakorlatban ez a módszer csak kis nyelvtanok esetén bizonyult hatékonynak. A <span class="strong"><strong>bayesi modellösszevonás</strong></span> (<span class="strong"><strong>Bayesian model merging</strong></span>) alternatív megközelítés hasonló a <code class="code">SEQUITUR</code> modellhez (lásd 22.8. alfejezet). A módszer mondatonkénti lokális modellek (nyelvtanok) építésével kezd, majd a minimális leíróhossz felhasználásával összevonja a modelleket.</p></div></div><div class="footnotes"><br/><hr/><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id772493" href="#id772493" class="para">236</a>] </sup> Mivel nem lett volna értelme lefordítani magyarra az egyes szavakat, ezért meghagytuk az eredeti angol nyelvű példát. (<span class="emphasis"><em>A ford.</em></span>)</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id772747" href="#id772747" class="para">237</a>] </sup> Mivel nem lehetett volna hasonló magyar példát valószínűségekkel együtt elkészíteni, ezért meghagytuk az eredeti angol nyelvű példát.<span class="emphasis"><em> </em></span>(<span class="emphasis"><em>A ford.</em></span>)</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id772763" href="#id772763" class="para">238</a>] </sup> Könnyűszavakatszóközöknélkülolvasni (<span class="emphasis"><em>A ford.</em></span>)</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id772821" href="#id772821" class="para">239</a>] </sup> Könnyű szavakat szóközök nélkül olvasni (<span class="emphasis"><em>A ford.</em></span>)</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id772846" href="#id772846" class="para">240</a>] </sup> <span class="emphasis"><em>easy</em></span> – könnyű; <span class="emphasis"><em>as</em></span> – mint (<span class="emphasis"><em>A ford.</em></span>)</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id772897" href="#id772897" class="para">241</a>] </sup> <span class="emphasis"><em>without</em></span> – nélkül; <span class="emphasis"><em>with</em></span> – vele; <span class="emphasis"><em>out</em></span> – kint (<span class="emphasis"><em>A ford.</em></span>)</p></div><div class="footnote"><p><sup>[<a id="ftn.id772977" href="#id772977" class="para">242</a>] </sup> A PCFG-k másik megnevezése a sztochasztikus környezetfüggetlen nyelvtan (stochastic context-free grammar), avagy SCFG.</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id773035" href="#id773035" class="para">243</a>] </sup> „egy banánt eszik” – „egy selyemkendőt eszik” (<span class="emphasis"><em>A ford.</em></span>)</p></div><div class="footnote"><p class="footnote text"><sup>[<a id="ftn.id773063" href="#id773063" class="para">244</a>] </sup> A kifejezés <span class="emphasis"><em>feje</em></span> a legfontosabb szó, például a főnév a főnévi szerkezetben.</p></div></div></div></body></html>
