<?xml version="1.0" encoding="UTF-8" standalone="no"?>

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="DocBook XSL Stylesheets V1.76.1"/></head><body><div class="chapter" title="21. fejezet - Megerősítéses tanulás"><div class="titlepage"><div><div><h1 class="title"><a id="id751617"/>21. fejezet - Megerősítéses tanulás</h1></div></div></div><p><span class="emphasis"><em>Ebben a fejezetben azt vizsgáljuk, hogy egy ágens hogyan képes tanulni a sikereiből és a kudarcaiból, illetve a jutalomból és a büntetésből.</em></span></p><div class="section" title="Bevezetés"><div class="titlepage"><div><div><h1 class="title"><a id="id751624"/>Bevezetés</h1></div></div></div><p>A 18. és 20. fejezetben olyan tanuló módszereket mutattunk be, amelyek példák alapján függvényeket vagy valószínűségi modelleket sajátítanak el. Ebben a fejezetben azzal foglalkozunk, hogy az ágens mi módon tanulhatja meg azt, hogy <span class="emphasis"><em>mit tegyen,</em></span> különösen akkor, amikor nincs tanár, aki minden előforduló körülmény esetére elárulná a helyes cselekvést.</p><div class="important" title="Fontos" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Fontos</h3><p>Vegyük példaként a sakkot. Tudjuk, hogy az ágens képes megtanulni sakkozni felügyelt tanulással, vagyis ha példaként sakkállásokat mutatunk neki, megnevezve az adott állásban a legjobb lépést. De mit tud csinálni az ágens akkor, ha nincs kéznél egy barátságos tanító, aki példákat mutatna? Véletlen lépések kipróbálásával végül is az ágens felépítheti környezetének prediktív modelljét: milyen lesz az állás, ha meglép egy adott lépést, esetleg még azt is modellezheti, hogy az ellenfél várhatóan mit fog válaszul lépni. A következő azonban a probléma: <span class="emphasis"><em>visszacsatolás nélkül</em></span> <span class="emphasis"><em>– tehát anélkül, hogy tudná, mi a jó és mi a rossz – az ágensnek semmilyen alapja sem lesz eldönteni azt, hogy melyik lépést húzza meg</em></span>. Az ágensnek tudnia kell, hogy valami jó történt, amikor nyert, illetve valami rossz történt, amikor vesztett. Az ilyen típusú visszacsatolást nevezzük <span class="strong"><strong>jutalom</strong></span>nak (<span class="strong"><strong>reward</strong></span>) vagy <span class="strong"><strong>megerősítés</strong></span>nek (<span class="strong"><strong>reinforcement</strong></span>). A sakkhoz hasonló játékokban a megerősítés csak a játék végén jelenik meg. Más környezetben a jutalom gyakrabban jön, például a pingpongban mindegyik pont jutalomnak tekinthető éppúgy, mint a mászás tanulásakor bármely előrefelé haladás. Az általunk használt tárgyalásmódban az ágens a jutalmat is az észlelés <span class="emphasis"><em>részeként</em></span> kezeli, de valahogy bele kell lennie „huzalozva”, hogy ez a rész jutalom, és nem csupán egy észlelésfajta. Úgy tűnik, hogy az állatokba be van építve, hogy a fájdalom és az éhség negatív „jutalom”, míg a gyönyör és az élelem pozitív jutalom. A megerősítéses tanulást az állatok pszichológiáját kutatók 60 éve intenzíven tanulmányozzák.</p></div><p>A jutalom fogalmát a 17. fejezetben vezettük be, ahol a <span class="strong"><strong>Markov döntési folyamat</strong></span>ok (<span class="strong"><strong>Markov decision process</strong></span>, <span class="strong"><strong>MDP</strong></span>) optimalitásának definiálására szolgált. Optimális stratégiának neveztük azt a stratégiát, amely maximálja a várható összjutalmat. A <span class="strong"><strong>megerősítéses tanulás</strong></span> (<span class="strong"><strong>reinforcement learning</strong></span>) feladata az, hogy optimális (vagy közel optimális) stratégiát tanuljunk az adott környezethez. Míg a 17. fejezetben az ágensnek a teljes környezeti modell és a jutalomfüggvény a rendelkezésére állt, addig a jelen fejezetben egyikről sem feltételezünk semmilyen előzetes tudást. Képzelje el, hogy egy új, ismeretlen játékot játszik, amelynek nem ismeri a szabályait. Nagyjából száz lépés után az ellenfele bejelenti: ön vesztett. Dióhéjban ez jellemzi a megerősítéses tanulást.</p><p>Számos bonyolult problématerületen a megerősítéses tanulás az egyetlen lehetséges út arra, hogy egy programnak magas szintű működést tanítsunk. Például a játékok terén nagyon nehéz lenne a szakember dolga: a lehetséges nagyszámú állás pontos és következetes értékelését kellene elvégeznie annak érdekében, hogy a példák alapján közvetlenül megtanítson egy kiértékelő függvényt. Ehelyett a programnak megadható az az információ, hogy nyert vagy vesztett, és ezt felhasználhatja egy olyan kiértékelő függvény tanulására, amely elfogadható pontossággal képes megbecsülni bármely adott állásban a nyerés valószínűségét. Hasonlóképpen rendkívül nehéz megtanítani egy ágenst helikopter vezetésére, de ha megfelelő negatív jutalmat kap minden balesetnél, bukdácsoló mozgásnál vagy a beállított útvonaltól való eltérésnél, akkor az ágens önállóan megtanulhat repülni.</p><p>Bizonyos értelemben a megerősítéses tanulás az egész MI-problémát átfogja. Egy ágenst belehelyezünk valamilyen környezetbe, és meg kell tanulnia ott sikeresen viselkedni. Ahhoz, hogy ezt a fejezetet áttekinthető bonyolultsági szinten tartsuk, csak egyszerű helyzetekre és egyszerű ágensekre fogunk koncentrálni. A fejezet legnagyobb részében teljesen megfigyelhető környezetet feltételezünk, tehát a megfigyelések informálnak az aktuális állapotról. Másrészt viszont azt feltételezzük, hogy az ágens sem azt nem tudja, hogyan működik a környezet, sem azt, hogy cselekedeteinek mi a hatása, továbbá megengedjük a cselekedetek valószínűségi (nemdeterminisztikusan meghatározott) kimenetelét. Három olyan ágensfelépítést fogunk áttekinteni, amelyeket először a 2. fejezetben vezettünk be:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>A <span class="strong"><strong>hasznosságalapú ágens</strong></span> (<span class="strong"><strong>utility-based agent</strong></span>) az állapotokra alapozott hasznosságfüggvényt tanul, és ennek alapján választja ki azokat a cselekvéseit, amelyekkel maximálja az elérhető hasznosság várható értékét.</p></li><li class="listitem"><p>A <span class="bold"><strong><span class="emphasis"><em>Q</em></span>-tanuló</strong></span> (<span class="bold"><strong><span class="emphasis"><em>Q</em></span>-learning</strong></span>) ágens egy <span class="strong"><strong>cselekvésérték</strong></span> (<span class="strong"><strong>action-value</strong></span>) függvényt – vagy más néven <span class="emphasis"><em>Q</em></span>-függvényt – tanul, valamilyen várható hasznot tulajdonítva egy adott helyzetben egy adott cselekvésnek.</p></li><li class="listitem"><p>Egy <span class="strong"><strong>reflexszerű ágens</strong></span> (<span class="strong"><strong>reflex agent</strong></span>) olyan stratégiát tanul, amely közvetlenül képezi le az állapotokat cselekvésekre.</p></li></ul></div><p>Egy hasznosságalapú ágensnek a környezet valamilyen modelljével is rendelkeznie kell ahhoz, hogy tudja, cselekvése milyen állapotba vezet. Például ahhoz, hogy az ostáblajáték hasznosságfüggvényének hasznát vegyük, az ostáblajátékot játszó programnak tudnia kell, hogy melyek a lehetséges szabályos lépések, és ezek <span class="emphasis"><em>hogyan hatnak az állásra</em></span>. Csak ilyen módon van arra lehetősége, hogy a lépése eredményeképpen kapott állásra alkalmazza a hasznosságfüggvényt. Egy <span class="emphasis"><em>Q</em></span>-tanuló ágens viszont össze tudja hasonlítani a lehetséges választásait anélkül, hogy tudná, mire vezetnek, így nincs szüksége a környezet modelljére. Másrészt viszont, mivel nem tudják, hogy cselekvéseik hova vezetnek, a <span class="emphasis"><em>Q</em></span>-tanuló ágensek nem képesek előre nézni. Ez viszont, mint látni is fogjuk, súlyosan korlátozhatja tanulási képességeiket.</p><p>A 21.2. alfejezetben a <span class="strong"><strong>passzív tanulás</strong></span>sal (<span class="strong"><strong>passive learning</strong></span>) kezdünk foglalkozni, amelyben az ágensnek rögzített stratégiája van, és az állapotok hasznosságának (vagy az állapot-cselekvés párok hasznosságának) megtanulása a feladat. Ez magában foglalhatja a környezet modelljének megtanulását is. A 21.3. alfejezet az <span class="strong"><strong>aktív tanulás</strong></span>sal (<span class="strong"><strong>active learning</strong></span>) foglalkozik, amikor is az ágensnek azt is meg kell tanulnia, hogy mit tegyen. Az alapvető elv a <span class="strong"><strong>felfedezés</strong></span> (<span class="strong"><strong>exploration</strong></span>), az ágensnek a lehető legtöbbet meg kell tapasztalnia környezetéről ahhoz, hogy megtanulja, mi a célszerű viselkedés benne. A 21.4. alfejezet bemutatja, hogy az ágens hogyan tudja felhasználni az induktív tanulást arra, hogy sokkal gyorsabban tanuljon a tapasztalataiból. A 21.5. alfejezet a közvetlen stratégiareprezentáció tanulásának módszereivel foglalkozik reflexszerű ágenseknél. A Markov döntési folyamatok (lásd 17. fejezet) megértése döntő fontosságú ennek a fejezetnek a tárgyalása szempontjából.</p></div></div></body></html>
